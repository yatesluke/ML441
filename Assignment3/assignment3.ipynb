{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f956bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_class_simple = pd.read_csv('breast_cancer.csv')\n",
    "df_class_med = pd.read_csv('heart_disease.csv')\n",
    "df_class_complex = pd.read_csv('diabetes.csv')\n",
    "\n",
    "df_reg_simple = pd.read_csv('housing.csv')\n",
    "df_reg_med = pd.read_csv('real_estate_valuation.csv')\n",
    "df_reg_complex = pd.read_csv('Housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf3cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure matplotlib for Jupyter notebooks\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10210a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, precision_score, f1_score, recall_score,confusion_matrix, classification_report, r2_score\n",
    "\n",
    "\n",
    "\n",
    "class ClassificationNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ClassificationNeuralNetwork, self).__init__()\n",
    "        self.task_type = 'classification'\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Hidden layer + activation\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden_act = nn.LeakyReLU()\n",
    "\n",
    "        # Output layer\n",
    "        if output_size == 2:\n",
    "            # Binary classification -> single sigmoid output\n",
    "            self.output = nn.Linear(hidden_size, 1)\n",
    "            self.final_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Hidden layer transformation\n",
    "        x = self.hidden_act(self.hidden(x))\n",
    "        # Output layer + final activation\n",
    "        x = self.final_act(self.output(x))\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "class RegressionNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RegressionNeuralNetwork, self).__init__()\n",
    "        self.task_type = 'regression'\n",
    "        \n",
    "        self.hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, 1)  # Single output for regression\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)      # Input to hidden layer\n",
    "        x = self.lrelu(x)       # Apply Leaky ReLU activation\n",
    "        x = self.output(x)      # Linear output (no activation)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def train_passive_sgd(model, X_train, y_train, X_val, y_val, \n",
    "                     epochs=100, learning_rate=0.01, weight_decay=0.001, verbose=True):\n",
    "    # Make COPIES to avoid modifying original tensors\n",
    "    y_train_work = y_train.clone()\n",
    "    y_val_work = y_val.clone()\n",
    "    \n",
    "    # Choose loss function based on task type and output size\n",
    "    if model.task_type == 'classification':\n",
    "        if model.output.out_features == 1:\n",
    "            criterion = nn.BCELoss()  # Binary classification\n",
    "            # Convert targets to float for BCE (use working copies)\n",
    "            y_train_work = y_train_work.float().unsqueeze(1)\n",
    "            y_val_work = y_val_work.float().unsqueeze(1)\n",
    "        else:\n",
    "            criterion = nn.CrossEntropyLoss()  # Multi-class classification\n",
    "    else:\n",
    "        criterion = nn.MSELoss()  \n",
    "\n",
    "    # SGD optimizer with weight decay (L2 regularization)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_train)\n",
    "        train_loss = criterion(outputs, y_train_work)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val_work)\n",
    "        \n",
    "        train_losses.append(train_loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "        \n",
    "        # Print progress every 20 epochs (only if verbose=True)\n",
    "        if verbose and (epoch + 1) % 20 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "def prepare_data_for_torch(df, target_column, task_type='classification', test_size=0.2, val_size=0.1):\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_column]).values\n",
    "    y = df[target_column].values\n",
    "    \n",
    "    # Handle different target types\n",
    "    if task_type == 'classification':\n",
    "        # Encode categorical targets to integers\n",
    "        label_encoder = LabelEncoder()\n",
    "        y = label_encoder.fit_transform(y)\n",
    "        num_classes = len(np.unique(y))\n",
    "        print(f\"Classification task: {num_classes} classes\")\n",
    "    else:\n",
    "        print(\"Regression task\")\n",
    "        num_classes = 1\n",
    "\n",
    "    # Split data into train, validation, and test sets  \n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
    "    \n",
    "    # Scale features to [0, 1] range\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    X_train = torch.FloatTensor(X_train)\n",
    "    X_val = torch.FloatTensor(X_val)\n",
    "    X_test = torch.FloatTensor(X_test)\n",
    "    \n",
    "    if task_type == 'classification':\n",
    "        y_train = torch.LongTensor(y_train)\n",
    "        y_val = torch.LongTensor(y_val)\n",
    "        y_test = torch.LongTensor(y_test)\n",
    "    else:\n",
    "        y_train = torch.FloatTensor(y_train).view(-1, 1)\n",
    "        y_val = torch.FloatTensor(y_val).view(-1, 1)\n",
    "        y_test = torch.FloatTensor(y_test).view(-1, 1)\n",
    "    \n",
    "    print(f\"Data splits - Train: {X_train.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}\")\n",
    "    \n",
    "    return (X_train, X_val, X_test, y_train, y_val, y_test), scaler, num_classes\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        \n",
    "        if model.task_type == 'classification':\n",
    "            # Binary classification with BCELoss\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            y_test_np = y_test.cpu().numpy()\n",
    "            predicted_np = predicted.cpu().numpy().flatten()\n",
    "            \n",
    "            accuracy = accuracy_score(y_test_np, predicted_np)\n",
    "            precision = precision_score(y_test_np, predicted_np, average='weighted', zero_division=0)\n",
    "            f1 = f1_score(y_test_np, predicted_np, average='weighted', zero_division=0)\n",
    "            recall = recall_score(y_test_np, predicted_np, average='weighted', zero_division=0)\n",
    "            confusion = confusion_matrix(y_test_np, predicted_np)\n",
    "            class_report = classification_report(y_test_np, predicted_np)\n",
    "\n",
    "            print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "            print(f\"Test Precision: {precision:.4f}\")\n",
    "            print(f\"Test F1 Score: {f1:.4f}\")\n",
    "            print(f\"Test Recall: {recall:.4f}\")\n",
    "            print(f\"Confusion Matrix:\\n{confusion}\")\n",
    "            print(f\"Classification Report:\\n{class_report}\")\n",
    "            return accuracy\n",
    "        else:\n",
    "            # Calculate MSE for regression\n",
    "            mse = mean_squared_error(y_test.cpu().numpy(), outputs.cpu().numpy())\n",
    "            rmse = np.sqrt(mse)\n",
    "            r2 = r2_score(y_test.cpu().numpy(), outputs.cpu().numpy())\n",
    "            print(f\"Test R^2: {r2:.4f}\")\n",
    "            # Calculate Pearson correlation coefficient\n",
    "            correlation_matrix = np.corrcoef(y_test.cpu().numpy().flatten(), outputs.cpu().numpy().flatten())\n",
    "            pearson_correlation = correlation_matrix[0, 1]\n",
    "            print(f\"Test RMSE: {rmse:.4f}\")\n",
    "            print(f\"Test Pearson Correlation: {pearson_correlation:.4f}\")\n",
    "            print(f\"Test MSE: {mse:.4f}\")\n",
    "            return mse\n",
    "\n",
    "def visualise_losses(train_losses, val_losses, title=\"Loss over Epochs\"):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b904d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare simple classification data - Breast Cancer Wisconsin dataset\n",
    "df_class_simple_processed = df_class_simple.copy()\n",
    "\n",
    "# Check for missing values\n",
    "missing_count = df_class_simple_processed.isnull().sum().sum()\n",
    "if missing_count > 0:\n",
    "    print(f\"Handling {missing_count} missing values\")\n",
    "    # Fill missing values with median for numeric columns\n",
    "    for col in df_class_simple_processed.columns:\n",
    "        if df_class_simple_processed[col].isnull().any():\n",
    "            if df_class_simple_processed[col].dtype in ['float64', 'int64']:\n",
    "                df_class_simple_processed[col].fillna(df_class_simple_processed[col].median(), inplace=True)\n",
    "else:\n",
    "    print(\"No missing values found\")\n",
    "\n",
    "# The target column is 'Diagnosis' (M = Malignant, B = Benign)\n",
    "target_column = 'Diagnosis'\n",
    "\n",
    "# Convert target to binary (0 for Benign, 1 for Malignant)\n",
    "df_class_simple_processed[target_column] = (df_class_simple_processed[target_column] == 'M').astype(int)\n",
    "\n",
    "# All other columns are numeric features (no categorical encoding needed)\n",
    "target_dist = df_class_simple_processed[target_column].value_counts().to_dict()\n",
    "print(f\"Breast cancer dataset prepared: {df_class_simple_processed.shape[0]} samples, {df_class_simple_processed.shape[1]-1} features\")\n",
    "print(f\"Target distribution: {target_dist} (0=Benign, 1=Malignant)\")\n",
    "\n",
    "# Prepare data for PyTorch\n",
    "target_col = 'Diagnosis'\n",
    "splits, scaler, num_classes = prepare_data_for_torch(\n",
    "    df_class_simple_processed, \n",
    "    target_col, \n",
    "    task_type='classification'\n",
    ")\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = splits\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 32\n",
    "output_size = num_classes\n",
    "print(f\"Input size: {input_size}, Hidden size: {hidden_size}, Output size: {output_size}\")\n",
    "\n",
    "# Create and train classification model\n",
    "baseline_model = ClassificationNeuralNetwork(input_size, hidden_size, output_size)\n",
    "print(f\"Model created with {sum(p.numel() for p in baseline_model.parameters())} parameters\")\n",
    "\n",
    "print(\"\\n--- Training Baseline Model ---\")\n",
    "train_losses, val_losses = train_passive_sgd(\n",
    "    baseline_model, X_train, y_train, X_val, y_val,\n",
    "    epochs=1500,\n",
    "    learning_rate=0.01,\n",
    "    weight_decay=0.0001\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n--- Final Evaluation ---\")\n",
    "test_accuracy = evaluate_model(baseline_model, X_test, y_test)\n",
    "visualise_losses(train_losses, val_losses, title=\"Breast Cancer Classification Loss over Epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca251bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Handle missing values more thoroughly\n",
    "df_reg_simple = df_reg_simple.copy()\n",
    "df_reg_simple['total_bedrooms'].fillna(df_reg_simple['total_bedrooms'].median(), inplace=True)\n",
    "\n",
    "# Fill any other missing values\n",
    "for col in df_reg_simple.columns:\n",
    "    if df_reg_simple[col].isnull().any():\n",
    "        if df_reg_simple[col].dtype in ['float64', 'int64']:\n",
    "            df_reg_simple[col].fillna(df_reg_simple[col].median(), inplace=True)\n",
    "        else:\n",
    "            df_reg_simple[col].fillna(df_reg_simple[col].mode()[0], inplace=True)\n",
    "\n",
    "reg_target = 'median_house_value'\n",
    "# Prepare regression dataset\n",
    "df_reg_processed = df_reg_simple.copy()\n",
    "\n",
    "# Handle any categorical columns in housing data\n",
    "for col in df_reg_processed.columns:\n",
    "    if df_reg_processed[col].dtype == 'object' and col != reg_target:\n",
    "        print(f\"Encoding categorical column: {col}\")\n",
    "        df_reg_processed[col] = LabelEncoder().fit_transform(df_reg_processed[col].astype(str))\n",
    "\n",
    "# Scale the target values to prevent large gradients\n",
    "target_scaler = StandardScaler()\n",
    "df_reg_processed[reg_target] = target_scaler.fit_transform(df_reg_processed[[reg_target]])\n",
    "print(f\"Dataset prepared: {df_reg_processed.shape[0]} samples, target scaled\")\n",
    "\n",
    "reg_splits, reg_scaler, _ = prepare_data_for_torch(\n",
    "    df_reg_processed, \n",
    "    reg_target, \n",
    "    task_type='regression'\n",
    ")\n",
    "\n",
    "X_train_reg, X_val_reg, X_test_reg, y_train_reg, y_val_reg, y_test_reg = reg_splits\n",
    "\n",
    "print(f\"\\n--- Regression Model Configuration ---\")\n",
    "input_size_reg = X_train_reg.shape[1]\n",
    "hidden_size_reg = 64  # Overestimate for regularization\n",
    "output_size_reg = 1   # Single output for regression\n",
    "print(f\"Input features: {input_size_reg}\")\n",
    "\n",
    "# Create regression model\n",
    "reg_model = RegressionNeuralNetwork(input_size_reg, hidden_size_reg)\n",
    "print(f\"Regression model created with {sum(p.numel() for p in reg_model.parameters())} parameters\")\n",
    "\n",
    "print(\"\\n--- Training Regression Baseline Model ---\")\n",
    "\n",
    "# Train the regression model\n",
    "reg_train_losses, reg_val_losses = train_passive_sgd(\n",
    "    reg_model, X_train_reg, y_train_reg, X_val_reg, y_val_reg,\n",
    "    epochs=1500,\n",
    "    learning_rate=0.01,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n--- Regression Final Evaluation ---\")\n",
    "test_mse = evaluate_model(reg_model, X_test_reg, y_test_reg)\n",
    "\n",
    "visualise_losses(reg_train_losses, reg_val_losses, title=\"Regression Loss over Epochs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ade94de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "# Prepare classification data - Heart Disease dataset\n",
    "df_class_med_processed = df_class_med.copy()\n",
    "\n",
    "# Check for missing values and handle them\n",
    "missing_count = df_class_med_processed.isnull().sum().sum()\n",
    "if missing_count > 0:\n",
    "    print(f\"Handling {missing_count} missing values\")\n",
    "    # Fill missing values appropriately \n",
    "    for col in df_class_med_processed.columns:\n",
    "        if df_class_med_processed[col].isnull().any():\n",
    "            if df_class_med_processed[col].dtype in ['float64', 'int64']:\n",
    "                df_class_med_processed[col].fillna(df_class_med_processed[col].median(), inplace=True)\n",
    "            else:\n",
    "                df_class_med_processed[col].fillna(df_class_med_processed[col].mode()[0], inplace=True)\n",
    "else:\n",
    "    print(\"No missing values found\")\n",
    "\n",
    "# The target variable is 'num' (0 = no heart disease, >0 = heart disease)\n",
    "target_column = 'num'\n",
    "\n",
    "# Convert multi-class target to binary classification (0 vs >0)\n",
    "df_class_med_processed[target_column] = (df_class_med_processed[target_column] > 0).astype(int)\n",
    "\n",
    "# All features are already numeric (no categorical encoding needed)\n",
    "target_dist = df_class_med_processed[target_column].value_counts().to_dict()\n",
    "print(f\"Heart disease dataset prepared: {df_class_med_processed.shape[0]} samples, {df_class_med_processed.shape[1]-1} features\")\n",
    "print(f\"Target distribution: {target_dist} (0=No Disease, 1=Disease)\")\n",
    "\n",
    "# Prepare data for PyTorch\n",
    "target_col = 'num'\n",
    "splits, scaler, num_classes = prepare_data_for_torch(\n",
    "    df_class_med_processed, \n",
    "    target_col, \n",
    "    task_type='classification'\n",
    ")\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = splits\n",
    "\n",
    "print(f\"\\n--- Model Configuration ---\")\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 256\n",
    "output_size = num_classes\n",
    "print(f\"Input size: {input_size}, Hidden size: {hidden_size}, Output size: {output_size}\")\n",
    "\n",
    "# Create and train classification model\n",
    "baseline_model = ClassificationNeuralNetwork(input_size, hidden_size, output_size)\n",
    "print(f\"Model created with {sum(p.numel() for p in baseline_model.parameters())} parameters\")\n",
    "\n",
    "print(\"\\n--- Training Baseline Model ---\")\n",
    "train_losses, val_losses = train_passive_sgd(\n",
    "    baseline_model, X_train, y_train, X_val, y_val,\n",
    "    epochs=1000,\n",
    "    learning_rate=0.01,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n--- Final Evaluation ---\")\n",
    "test_accuracy = evaluate_model(baseline_model, X_test, y_test)\n",
    "visualise_losses(train_losses, val_losses, title=\"Heart Disease Classification Loss over Epochs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63983387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Prepare regression data - real estate valuation dataset\n",
    "df_reg_med_processed = df_reg_med.copy()\n",
    "\n",
    "# Check for missing values (should be none in this dataset)\n",
    "if df_reg_med_processed.isnull().sum().sum() > 0:\n",
    "    print(\"Missing values found - handling them\")\n",
    "    print(df_reg_med_processed.isnull().sum())\n",
    "else:\n",
    "    print(\"No missing values found\")\n",
    "\n",
    "# No missing values in this dataset, but let's handle any that might exist\n",
    "for col in df_reg_med_processed.columns:\n",
    "    if df_reg_med_processed[col].isnull().any():\n",
    "        if df_reg_med_processed[col].dtype in ['float64', 'int64']:\n",
    "            df_reg_med_processed[col].fillna(df_reg_med_processed[col].median(), inplace=True)\n",
    "        else:\n",
    "            df_reg_med_processed[col].fillna(df_reg_med_processed[col].mode()[0], inplace=True)\n",
    "\n",
    "# The target column for this real estate dataset\n",
    "reg_target = 'Y house price of unit area'\n",
    "# Prepare regression dataset - all features are already numeric\n",
    "df_reg_med_processed = df_reg_med_processed.copy()\n",
    "\n",
    "# Check for any categorical columns \n",
    "categorical_columns = df_reg_med_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "if categorical_columns:\n",
    "    print(f\"Found categorical columns: {categorical_columns}\")\n",
    "    for col in categorical_columns:\n",
    "        if col != reg_target:\n",
    "            df_reg_med_processed[col] = LabelEncoder().fit_transform(df_reg_med_processed[col].astype(str))\n",
    "\n",
    "# Scale the target values to prevent large gradients and improve training stability\n",
    "target_scaler = StandardScaler()\n",
    "df_reg_med_processed[reg_target] = target_scaler.fit_transform(df_reg_med_processed[[reg_target]])\n",
    "print(f\"Dataset prepared: {df_reg_med_processed.shape[0]} samples, {df_reg_med_processed.shape[1]-1} features\")\n",
    "\n",
    "reg_splits, reg_scaler, _ = prepare_data_for_torch(\n",
    "    df_reg_med_processed, \n",
    "    reg_target, \n",
    "    task_type='regression'\n",
    ")\n",
    "\n",
    "X_train_reg, X_val_reg, X_test_reg, y_train_reg, y_val_reg, y_test_reg = reg_splits\n",
    "\n",
    "print(f\"\\n--- Regression Model Configuration ---\")\n",
    "input_size_reg = X_train_reg.shape[1]\n",
    "hidden_size_reg = 64  # Overestimate for regularization\n",
    "output_size_reg = 1   # Single output for regression\n",
    "print(f\"Input features: {input_size_reg}\")\n",
    "# Create regression model\n",
    "reg_model = RegressionNeuralNetwork(input_size_reg, hidden_size_reg)\n",
    "print(f\"Regression model created with {sum(p.numel() for p in reg_model.parameters())} parameters\")\n",
    "\n",
    "print(\"\\n--- Training Regression Baseline Model ---\")\n",
    "\n",
    "# Train the regression model\n",
    "reg_train_losses, reg_val_losses = train_passive_sgd(\n",
    "    reg_model, X_train_reg, y_train_reg, X_val_reg, y_val_reg,\n",
    "    epochs=1500,\n",
    "    learning_rate=0.01,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--- Regression Final Evaluation ---\")\n",
    "test_mse = evaluate_model(reg_model, X_test_reg, y_test_reg)\n",
    "\n",
    "visualise_losses(reg_train_losses, reg_val_losses, title=\"Regression Loss over Epochs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b7b1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare complex classification data - Diabetes dataset\n",
    "df_class_complex_processed = df_class_complex.copy()\n",
    "\n",
    "# Columns where 0 is not medically possible and indicates missing data\n",
    "zero_not_acceptable = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "\n",
    "# Replace 0s with NaN for these columns, then impute with median\n",
    "for col in zero_not_acceptable:\n",
    "    if col in df_class_complex_processed.columns:\n",
    "        # Replace 0 with NaN (except for Insulin where 0 might be acceptable)\n",
    "        if col != 'Insulin':\n",
    "            df_class_complex_processed[col] = df_class_complex_processed[col].replace(0, np.nan)\n",
    "        else:\n",
    "            # For Insulin, only replace very low values that seem unrealistic\n",
    "            df_class_complex_processed[col] = df_class_complex_processed[col].replace(0, np.nan)\n",
    "        \n",
    "        # Impute missing values with median\n",
    "        df_class_complex_processed[col].fillna(df_class_complex_processed[col].median(), inplace=True)\n",
    "\n",
    "# Check final missing values\n",
    "missing_count = df_class_complex_processed.isnull().sum().sum()\n",
    "if missing_count > 0:\n",
    "    print(f\"Remaining {missing_count} missing values after preprocessing\")\n",
    "    df_class_complex_processed = df_class_complex_processed.dropna()\n",
    "else:\n",
    "    print(\"All missing values handled successfully\")\n",
    "\n",
    "# Target variable is 'Outcome' (0 = no diabetes, 1 = diabetes)\n",
    "target_column = 'Outcome'\n",
    "target_dist = df_class_complex_processed[target_column].value_counts().to_dict()\n",
    "print(f\"Diabetes dataset prepared: {df_class_complex_processed.shape[0]} samples, {df_class_complex_processed.shape[1]-1} features\")\n",
    "print(f\"Target distribution: {target_dist} (0=No Diabetes, 1=Diabetes)\")\n",
    "print(f\"Dataset imbalance: {target_dist[0]/(target_dist[0]+target_dist[1]):.1%} no diabetes, {target_dist[1]/(target_dist[0]+target_dist[1]):.1%} diabetes\")\n",
    "\n",
    "# Prepare data for PyTorch\n",
    "target_col = 'Outcome'\n",
    "splits, scaler, num_classes = prepare_data_for_torch(\n",
    "    df_class_complex_processed, \n",
    "    target_col, \n",
    "    task_type='classification'\n",
    ")\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = splits\n",
    "\n",
    "print(f\"\\n--- Training Complex Classification Model ---\")\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 128  # Larger hidden layer for complex dataset\n",
    "output_size = num_classes\n",
    "print(f\"Input size: {input_size}, Hidden size: {hidden_size}, Output size: {output_size}\")\n",
    "\n",
    "# Create and train classification model with larger capacity for complex problem\n",
    "complex_model = ClassificationNeuralNetwork(input_size, hidden_size, output_size)\n",
    "print(f\"Complex model: {sum(p.numel() for p in complex_model.parameters())} parameters\")\n",
    "\n",
    "# Train with careful hyperparameters for medical data\n",
    "train_losses, val_losses = train_passive_sgd(\n",
    "    complex_model, X_train, y_train, X_val, y_val,\n",
    "    epochs=1000,\n",
    "    learning_rate=0.005,  # Lower learning rate for stability\n",
    "    weight_decay=0.0001   # Moderate regularization\n",
    ")\n",
    "\n",
    "print(\"\\n--- Final Evaluation ---\")\n",
    "test_accuracy = evaluate_model(complex_model, X_test, y_test)\n",
    "visualise_losses(train_losses, val_losses, title=\"Diabetes Classification Loss over Epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0808ecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg_complex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b416819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare complex regression data - comprehensive housing features\n",
    "df_reg_complex_processed = df_reg_complex.copy()\n",
    "\n",
    "# Check for missing values\n",
    "missing_count = df_reg_complex_processed.isnull().sum().sum()\n",
    "if missing_count > 0:\n",
    "    print(f\"Handling {missing_count} missing values\")\n",
    "    # Handle missing values appropriately\n",
    "    for col in df_reg_complex_processed.columns:\n",
    "        if df_reg_complex_processed[col].isnull().any():\n",
    "            if df_reg_complex_processed[col].dtype in ['float64', 'int64']:\n",
    "                df_reg_complex_processed[col].fillna(df_reg_complex_processed[col].median(), inplace=True)\n",
    "            else:\n",
    "                df_reg_complex_processed[col].fillna(df_reg_complex_processed[col].mode()[0], inplace=True)\n",
    "else:\n",
    "    print(\"No missing values found\")\n",
    "\n",
    "# Handle categorical variables - convert yes/no to binary and encode furnishing status\n",
    "binary_cols = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
    "for col in binary_cols:\n",
    "    if col in df_reg_complex_processed.columns:\n",
    "        df_reg_complex_processed[col] = (df_reg_complex_processed[col] == 'yes').astype(int)\n",
    "\n",
    "# Handle furnishingstatus with label encoding (ordinal: unfurnished < semi-furnished < furnished)\n",
    "if 'furnishingstatus' in df_reg_complex_processed.columns:\n",
    "    furnishing_map = {'unfurnished': 0, 'semi-furnished': 1, 'furnished': 2}\n",
    "    df_reg_complex_processed['furnishingstatus'] = df_reg_complex_processed['furnishingstatus'].map(furnishing_map)\n",
    "\n",
    "# Target variable is price\n",
    "target_column = 'price'\n",
    "target_stats = df_reg_complex_processed[target_column].describe()\n",
    "print(f\"Complex dataset prepared: {df_reg_complex_processed.shape[0]} samples, {df_reg_complex_processed.shape[1]-1} features\")\n",
    "print(f\"Target range: {target_stats['min']:.0f} to {target_stats['max']:.0f} (mean: {target_stats['mean']:.0f})\")\n",
    "\n",
    "# Scale the target values to prevent large gradients and improve training stability\n",
    "target_scaler = StandardScaler()\n",
    "df_reg_complex_processed[target_column] = target_scaler.fit_transform(df_reg_complex_processed[[target_column]])\n",
    "\n",
    "# Prepare data for PyTorch\n",
    "target_col = 'price'\n",
    "splits, scaler, _ = prepare_data_for_torch(\n",
    "    df_reg_complex_processed, \n",
    "    target_col, \n",
    "    task_type='regression'\n",
    ")\n",
    "\n",
    "X_train_reg, X_val_reg, X_test_reg, y_train_reg, y_val_reg, y_test_reg = splits\n",
    "\n",
    "print(f\"\\n--- Training Complex Regression Model ---\")\n",
    "input_size_reg = X_train_reg.shape[1]\n",
    "hidden_size_reg = 64  # Larger hidden layer for complex dataset\n",
    "output_size_reg = 1   # Single output for regression\n",
    "print(f\"Input features: {input_size_reg}\")\n",
    "\n",
    "\n",
    "# Create regression model with larger capacity for complex problem\n",
    "complex_reg_model = RegressionNeuralNetwork(input_size_reg, hidden_size_reg)\n",
    "print(f\"Complex regression model: {sum(p.numel() for p in complex_reg_model.parameters())} parameters, training for 1500 epochs\")\n",
    "\n",
    "# Train with more epochs and careful learning rate for complex problem\n",
    "reg_train_losses, reg_val_losses = train_passive_sgd(\n",
    "    complex_reg_model, X_train_reg, y_train_reg, X_val_reg, y_val_reg,\n",
    "    epochs=1500,\n",
    "    learning_rate=0.05,  # Careful learning rate for stability\n",
    "    weight_decay=0.001   # Moderate regularization\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n--- Final Evaluation ---\")\n",
    "test_mse = evaluate_model(complex_reg_model, X_test_reg, y_test_reg)\n",
    "visualise_losses(reg_train_losses, reg_val_losses, title=\"Complex Regression Loss over Epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea35c8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6a5a342",
   "metadata": {},
   "source": [
    "# Extensive Grid Search for All Datasets\n",
    "\n",
    "This section performs comprehensive hyperparameter tuning for all classification and regression datasets using grid search to find optimal configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca5e70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, r2_score\n",
    "\n",
    "def create_grid_search_params():\n",
    "    # Classification grid - IMPROVED with wider ranges and longer training\n",
    "    classification_grid = {\n",
    "        'hidden_size': [32, 64, 128, 256],  # More sizes\n",
    "        'learning_rate': [0.001, 0.005, 0.01, 0.05, 0.1],  # Wider range\n",
    "        'weight_decay': [0.0001, 0.001, 0.01],  # More options\n",
    "        'epochs': [200, 500, 1000]  # Longer training to show differences\n",
    "    }\n",
    "    \n",
    "    # Regression grid - similar improvements\n",
    "    regression_grid = {\n",
    "        'hidden_size': [32, 64, 128, 256],\n",
    "        'learning_rate': [0.0001, 0.001, 0.01, 0.05],\n",
    "        'weight_decay': [0.0, 0.0001, 0.001, 0.01], \n",
    "        'epochs': [500, 1000, 1500]  # Longer for regression\n",
    "    }\n",
    "    \n",
    "    return classification_grid, regression_grid\n",
    "\n",
    "def create_reduced_grid_search_params():\n",
    "    \n",
    "    # Reduced classification grid - for testing\n",
    "    classification_grid = {\n",
    "        'hidden_size': [64, 256],  \n",
    "        'learning_rate': [0.001, 0.05],  # Extreme values to show difference\n",
    "        'weight_decay': [0.0, 0.01],  # Extreme values\n",
    "        'epochs': [200, 500]  # Reasonable epochs\n",
    "    }\n",
    "    \n",
    "    # Reduced regression grid \n",
    "    regression_grid = {\n",
    "        'hidden_size': [64, 256],\n",
    "        'learning_rate': [0.001, 0.05],\n",
    "        'weight_decay': [0.0, 0.01], \n",
    "        'epochs': [500, 1000]\n",
    "    }\n",
    "    \n",
    "    return classification_grid, regression_grid\n",
    "\n",
    "\n",
    "def save_results(results, filename, dataset_info):\n",
    "    output = {\n",
    "        'dataset_info': dataset_info,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'total_experiments': len(results),\n",
    "        'results': results\n",
    "    }\n",
    "    \n",
    "    os.makedirs('results/passive', exist_ok=True)\n",
    "    filepath = f'results/passive/{filename}'\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(output, f, indent=2)\n",
    "    \n",
    "    print(f\"✅ Results saved to {filepath}\")\n",
    "\n",
    "def print_best_results(results, task_type, top_k=3):\n",
    "    if task_type == 'classification':\n",
    "        sorted_results = sorted(results, key=lambda x: x.get('test_f1', -float('inf')), reverse=True)\n",
    "        metric = 'test_f1'\n",
    "    else:\n",
    "        sorted_results = sorted(results, key=lambda x: x.get('test_r2', -float('inf')), reverse=True)\n",
    "        metric = 'test_r2'\n",
    "    \n",
    "    print(f\"\\n🏆 Top {top_k} Results (by {metric}):\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, result in enumerate(sorted_results[:top_k]):\n",
    "        print(f\"\\nRank {i+1}:\")\n",
    "        print(f\"  Hidden Size: {result['hidden_size']}\")\n",
    "        print(f\"  Learning Rate: {result['learning_rate']}\")\n",
    "        print(f\"  Weight Decay: {result['weight_decay']}\")\n",
    "        print(f\"  Epochs: {result['epochs']}\")\n",
    "        val_metric = result.get(metric, None)\n",
    "        if val_metric is not None:\n",
    "            print(f\"  {metric.replace('_', ' ').title()}: {val_metric:.4f}\")\n",
    "        print(f\"  Final Val Loss: {result['final_val_loss']:.4f}\")\n",
    "        print(f\"  Training Time: {result.get('training_time', 0):.1f}s\")\n",
    "\n",
    "print(\"✅ Improved grid search utilities loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8e424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search(X_train, X_val, X_test, y_train, y_val, y_test, \n",
    "                   task_type, num_classes=None, dataset_name=\"dataset\"):\n",
    "    classification_grid, regression_grid = create_grid_search_params()\n",
    "    \n",
    "    if task_type == 'classification':\n",
    "        param_grid = classification_grid\n",
    "    else:\n",
    "        param_grid = regression_grid\n",
    "    \n",
    "    # Generate all parameter combinations\n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "    param_combinations = list(product(*param_values))\n",
    "    \n",
    "    print(f\"\\n🔍 Starting Grid Search for {dataset_name}\")\n",
    "    print(f\"Task: {task_type.title()}\")\n",
    "    print(f\"Total combinations to test: {len(param_combinations)}\")\n",
    "    print(f\"Parameters: {param_names}\")\n",
    "    \n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Store ORIGINAL data copies to avoid in-place modifications\n",
    "    X_train_orig = X_train.clone()\n",
    "    X_val_orig = X_val.clone() \n",
    "    X_test_orig = X_test.clone()\n",
    "    y_train_orig = y_train.clone()\n",
    "    y_val_orig = y_val.clone()\n",
    "    y_test_orig = y_test.clone()\n",
    "    \n",
    "    for i, combination in enumerate(param_combinations):\n",
    "        params = dict(zip(param_names, combination))\n",
    "        \n",
    "        print(f\"\\n[{i+1}/{len(param_combinations)}] Testing: {params}\")\n",
    "        \n",
    "        try:\n",
    "            # SET RANDOM SEED for reproducible results\n",
    "            torch.manual_seed(42 + i)  # Different seed for each experiment\n",
    "            np.random.seed(42 + i)\n",
    "            \n",
    "            # Create fresh copies of data for each experiment\n",
    "            X_train_exp = X_train_orig.clone()\n",
    "            X_val_exp = X_val_orig.clone()\n",
    "            X_test_exp = X_test_orig.clone()\n",
    "            y_train_exp = y_train_orig.clone()\n",
    "            y_val_exp = y_val_orig.clone()\n",
    "            y_test_exp = y_test_orig.clone()\n",
    "            \n",
    "            # Create NEW model for each experiment (fresh initialization)\n",
    "            input_size = X_train_exp.shape[1]\n",
    "            \n",
    "            if task_type == 'classification':\n",
    "                model = ClassificationNeuralNetwork(input_size, params['hidden_size'], num_classes)\n",
    "            else:\n",
    "                model = RegressionNeuralNetwork(input_size, params['hidden_size'])\n",
    "            \n",
    "            # Train model with current parameters using COPIES of the data (suppress epoch prints)\n",
    "            train_losses, val_losses = train_passive_sgd(\n",
    "                model, X_train_exp, y_train_exp, X_val_exp, y_val_exp,\n",
    "                epochs=params['epochs'],\n",
    "                learning_rate=params['learning_rate'],\n",
    "                weight_decay=params['weight_decay'],\n",
    "                verbose=False  # Suppress epoch prints during grid search\n",
    "            )\n",
    "            \n",
    "            # Evaluate model using ORIGINAL unmodified targets\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Validation and test predictions\n",
    "                val_outputs = model(X_val_orig)\n",
    "                test_outputs = model(X_test_orig)\n",
    "                \n",
    "                if task_type == 'classification':\n",
    "                    if model.output.out_features == 1:\n",
    "                        # Binary classification\n",
    "                        val_pred = (val_outputs > 0.5).float().cpu().numpy().flatten()\n",
    "                        test_pred = (test_outputs > 0.5).float().cpu().numpy().flatten()\n",
    "                    else:\n",
    "                        # Multi-class classification\n",
    "                        _, val_pred = torch.max(val_outputs, 1)\n",
    "                        _, test_pred = torch.max(test_outputs, 1)\n",
    "                        val_pred = val_pred.cpu().numpy()\n",
    "                        test_pred = test_pred.cpu().numpy()\n",
    "                    \n",
    "                    # Use ORIGINAL targets for evaluation\n",
    "                    y_val_np = y_val_orig.cpu().numpy()\n",
    "                    y_test_np = y_test_orig.cpu().numpy()\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    val_f1 = f1_score(y_val_np, val_pred, average='weighted', zero_division=0)\n",
    "                    test_f1 = f1_score(y_test_np, test_pred, average='weighted', zero_division=0)\n",
    "                    val_accuracy = accuracy_score(y_val_np, val_pred)\n",
    "                    test_accuracy = accuracy_score(y_test_np, test_pred)\n",
    "                    \n",
    "                else:\n",
    "                    # Regression\n",
    "                    y_val_np = y_val_orig.cpu().numpy()\n",
    "                    y_test_np = y_test_orig.cpu().numpy()\n",
    "                    val_pred = val_outputs.cpu().numpy()\n",
    "                    test_pred = test_outputs.cpu().numpy()\n",
    "                    \n",
    "                    # Calculate R² score\n",
    "                    val_r2 = r2_score(y_val_np, val_pred)\n",
    "                    test_r2 = r2_score(y_test_np, test_pred)\n",
    "                    val_mse = mean_squared_error(y_val_np, val_pred)\n",
    "                    test_mse = mean_squared_error(y_test_np, test_pred)\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                'hidden_size': params['hidden_size'],\n",
    "                'learning_rate': params['learning_rate'],\n",
    "                'weight_decay': params['weight_decay'],\n",
    "                'epochs': params['epochs'],\n",
    "                'final_train_loss': train_losses[-1],\n",
    "                'final_val_loss': val_losses[-1],\n",
    "                'training_time': time.time() - start_time\n",
    "            }\n",
    "            \n",
    "            if task_type == 'classification':\n",
    "                result.update({\n",
    "                    'val_f1': val_f1,\n",
    "                    'test_f1': test_f1,\n",
    "                    'val_accuracy': val_accuracy,\n",
    "                    'test_accuracy': test_accuracy\n",
    "                })\n",
    "                print(f\"  Val F1: {val_f1:.4f}, Test F1: {test_f1:.4f}\")\n",
    "            else:\n",
    "                result.update({\n",
    "                    'val_r2': val_r2,\n",
    "                    'test_r2': test_r2,\n",
    "                    'val_mse': val_mse,\n",
    "                    'test_mse': test_mse\n",
    "                })\n",
    "                print(f\"  Val R²: {val_r2:.4f}, Test R²: {test_r2:.4f}\")\n",
    "                \n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error with params {params}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nGrid search completed in {total_time:.1f} seconds\")\n",
    "    print(f\"Successfully tested {len(results)}/{len(param_combinations)} combinations\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2373e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple classification splits (breast_cancer.csv) - rename existing variables for consistency\n",
    "print(\"\\n--- Simple Classification Data Preparation ---\")\n",
    "sim_splits, sim_scaler, sim_num_classes = prepare_data_for_torch(\n",
    "    df_class_simple_processed, \n",
    "    'Diagnosis',  # Updated target column for breast cancer\n",
    "    task_type='classification'\n",
    ")\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = sim_splits\n",
    "\n",
    "# Medium classification splits (heart_disease.csv)\n",
    "print(\"\\n--- Medium Classification Data Preparation ---\")\n",
    "med_splits, med_scaler, med_num_classes = prepare_data_for_torch(\n",
    "    df_class_med_processed, \n",
    "    'num',  # Updated target column for heart disease\n",
    "    task_type='classification'\n",
    ")\n",
    "X_train_med, X_val_med, X_test_med, y_train_med, y_val_med, y_test_med = med_splits\n",
    "\n",
    "# Complex classification splits (diabetes.csv) \n",
    "print(\"\\n--- Complex Classification Data Preparation ---\")\n",
    "complex_splits, complex_scaler, complex_num_classes = prepare_data_for_torch(\n",
    "    df_class_complex_processed,\n",
    "    'Outcome',  # Updated target column for diabetes\n",
    "    task_type='classification'\n",
    ")\n",
    "X_train_complex, X_val_complex, X_test_complex, y_train_complex, y_val_complex, y_test_complex = complex_splits\n",
    "\n",
    "# CORRECT - Create fresh splits from the PROCESSED simple regression dataset\n",
    "print(\"\\n--- Simple Regression Data Preparation ---\")\n",
    "simple_reg_splits, simple_reg_scaler, _ = prepare_data_for_torch(\n",
    "    df_reg_processed,  # Use the PROCESSED df_reg_processed (already handled categorical columns)\n",
    "    'median_house_value',  # California housing target\n",
    "    task_type='regression'\n",
    ")\n",
    "X_train_simple_reg, X_val_simple_reg, X_test_simple_reg, y_train_simple_reg, y_val_simple_reg, y_test_simple_reg = simple_reg_splits\n",
    "\n",
    "# Medium regression splits (real_estate_valuation.csv)\n",
    "print(\"\\n--- Medium Regression Data Preparation ---\") \n",
    "med_reg_splits, med_reg_scaler, _ = prepare_data_for_torch(\n",
    "    df_reg_med_processed,\n",
    "    'Y house price of unit area',\n",
    "    task_type='regression'\n",
    ")\n",
    "X_train_med_reg, X_val_med_reg, X_test_med_reg, y_train_med_reg, y_val_med_reg, y_test_med_reg = med_reg_splits\n",
    "\n",
    "# Complex regression splits (Housing.csv)\n",
    "print(\"\\n--- Complex Regression Data Preparation ---\")\n",
    "complex_reg_splits, complex_reg_scaler, _ = prepare_data_for_torch(\n",
    "    df_reg_complex_processed,\n",
    "    'price', \n",
    "    task_type='regression'\n",
    ")\n",
    "X_train_complex_reg, X_val_complex_reg, X_test_complex_reg, y_train_complex_reg, y_val_complex_reg, y_test_complex_reg = complex_reg_splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e836bc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Use the simple classification data splits (breast cancer)\n",
    "simple_class_results = run_grid_search(\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "    task_type='classification',\n",
    "    num_classes=sim_num_classes,\n",
    "    dataset_name=\"Simple Classification (Breast Cancer)\"\n",
    ")\n",
    "\n",
    "dataset_info = {\n",
    "    'name': 'Simple Classification (Breast Cancer)', \n",
    "    'samples': X_train.shape[0] + X_val.shape[0] + X_test.shape[0],\n",
    "    'features': X_train.shape[1],\n",
    "    'classes': sim_num_classes\n",
    "}\n",
    "\n",
    "save_results(simple_class_results, 'simple_classification_grid_search.json', dataset_info)\n",
    "print_best_results(simple_class_results, task_type='classification', top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1fbbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Use the medium complexity data splits\n",
    "medium_class_results = run_grid_search(\n",
    "    X_train_med, X_val_med, X_test_med, y_train_med, y_val_med, y_test_med,\n",
    "    task_type='classification',\n",
    "    num_classes=med_num_classes,\n",
    "    dataset_name=\"Medium Classification (Heart Disease)\"\n",
    ")\n",
    "\n",
    "dataset_info = {\n",
    "    'name': 'Medium Classification (Heart Disease)', \n",
    "    'samples': X_train_med.shape[0] + X_val_med.shape[0] + X_test_med.shape[0],\n",
    "    'features': X_train_med.shape[1],\n",
    "    'classes': med_num_classes\n",
    "}\n",
    "\n",
    "save_results(medium_class_results, 'medium_classification_grid_search.json', dataset_info)\n",
    "\n",
    "# === COMPLEX CLASSIFICATION (Diabetes) ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPLEX CLASSIFICATION - Diabetes Dataset\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "complex_class_results = run_grid_search(\n",
    "    X_train_complex, X_val_complex, X_test_complex, y_train_complex, y_val_complex, y_test_complex,\n",
    "    task_type='classification',\n",
    "    num_classes=complex_num_classes,\n",
    "    dataset_name=\"Complex Classification (Diabetes)\"\n",
    ")\n",
    "\n",
    "dataset_info = {\n",
    "    'name': 'Complex Classification (Diabetes)',\n",
    "    'samples': X_train_complex.shape[0] + X_val_complex.shape[0] + X_test_complex.shape[0],\n",
    "    'features': X_train_complex.shape[1],\n",
    "    'classes': complex_num_classes\n",
    "}\n",
    "\n",
    "save_results(complex_class_results, 'complex_classification_grid_search.json', dataset_info)\n",
    "\n",
    "print_best_results(medium_class_results, 'classification', top_k=5)\n",
    "print_best_results(complex_class_results, 'classification', top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368bd62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "simple_reg_results = run_grid_search(\n",
    "    X_train_simple_reg, X_val_simple_reg, X_test_simple_reg, \n",
    "    y_train_simple_reg, y_val_simple_reg, y_test_simple_reg,\n",
    "    task_type='regression',\n",
    "    dataset_name=\"Simple Regression (California Housing)\"\n",
    ")\n",
    "\n",
    "dataset_info = {\n",
    "    'name': 'Simple Regression (California Housing)',\n",
    "    'samples': X_train_simple_reg.shape[0] + X_val_simple_reg.shape[0] + X_test_simple_reg.shape[0],\n",
    "    'features': X_train_simple_reg.shape[1]\n",
    "}\n",
    "\n",
    "save_results(simple_reg_results, 'simple_regression_grid_search.json', dataset_info)\n",
    "\n",
    "# === MEDIUM REGRESSION (Real Estate Valuation) ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MEDIUM REGRESSION - Real Estate Valuation Dataset\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "medium_reg_results = run_grid_search(\n",
    "    X_train_med_reg, X_val_med_reg, X_test_med_reg,\n",
    "    y_train_med_reg, y_val_med_reg, y_test_med_reg, \n",
    "    task_type='regression',\n",
    "    dataset_name=\"Medium Regression (Real Estate Valuation)\"\n",
    ")\n",
    "\n",
    "dataset_info = {\n",
    "    'name': 'Medium Regression (Real Estate Valuation)',\n",
    "    'samples': X_train_med_reg.shape[0] + X_val_med_reg.shape[0] + X_test_med_reg.shape[0],\n",
    "    'features': X_train_med_reg.shape[1]\n",
    "}\n",
    "\n",
    "save_results(medium_reg_results, 'medium_regression_grid_search.json', dataset_info)\n",
    "\n",
    "# === COMPLEX REGRESSION (Comprehensive Housing) ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPLEX REGRESSION - Comprehensive Housing Dataset\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "complex_reg_results = run_grid_search(\n",
    "    X_train_complex_reg, X_val_complex_reg, X_test_complex_reg,\n",
    "    y_train_complex_reg, y_val_complex_reg, y_test_complex_reg,\n",
    "    task_type='regression', \n",
    "    dataset_name=\"Complex Regression (Comprehensive Housing)\"\n",
    ")\n",
    "\n",
    "dataset_info = {\n",
    "    'name': 'Complex Regression (Comprehensive Housing)',\n",
    "    'samples': X_train_complex_reg.shape[0] + X_val_complex_reg.shape[0] + X_test_complex_reg.shape[0],\n",
    "    'features': X_train_complex_reg.shape[1]\n",
    "}\n",
    "\n",
    "save_results(complex_reg_results, 'complex_regression_grid_search.json', dataset_info)\n",
    "\n",
    "print_best_results(simple_reg_results, 'regression', top_k=5)\n",
    "print_best_results(medium_reg_results, 'regression', top_k=5)\n",
    "print_best_results(complex_reg_results, 'regression', top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84a70ed",
   "metadata": {},
   "source": [
    "# Multiple Runs for Statistical Analysis\n",
    "\n",
    "This section runs each model configuration 10 times with different random seeds to collect statistical data for hypothesis testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef404587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs('results/passive_multiple_runs', exist_ok=True)\n",
    "\n",
    "def run_multiple_experiments(model_configs, n_runs=10):\n",
    "    all_results = {}\n",
    "    \n",
    "    for config_name, config in model_configs.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Running {config_name} - {n_runs} times\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for run in range(n_runs):\n",
    "            print(f\"\\nRun {run + 1}/{n_runs} for {config_name}\")\n",
    "            \n",
    "            # Set different random seed for each run\n",
    "            torch.manual_seed(42 + run)\n",
    "            np.random.seed(42 + run)\n",
    "            \n",
    "            try:\n",
    "                # Get data splits\n",
    "                X_train, X_val, X_test = config['data']['X_train'], config['data']['X_val'], config['data']['X_test']\n",
    "                y_train, y_val, y_test = config['data']['y_train'], config['data']['y_val'], config['data']['y_test']\n",
    "                \n",
    "                # Create fresh model\n",
    "                if config['task_type'] == 'classification':\n",
    "                    model = ClassificationNeuralNetwork(\n",
    "                        config['input_size'], \n",
    "                        config['hidden_size'], \n",
    "                        config['num_classes']\n",
    "                    )\n",
    "                else:\n",
    "                    model = RegressionNeuralNetwork(\n",
    "                        config['input_size'], \n",
    "                        config['hidden_size']\n",
    "                    )\n",
    "                \n",
    "                # Train model\n",
    "                start_time = time.time()\n",
    "                train_losses, val_losses = train_passive_sgd(\n",
    "                    model, X_train, y_train, X_val, y_val,\n",
    "                    epochs=config['epochs'],\n",
    "                    learning_rate=config['learning_rate'],\n",
    "                    weight_decay=config['weight_decay'],\n",
    "                    verbose=False\n",
    "                )\n",
    "                training_time = time.time() - start_time\n",
    "                \n",
    "                # Evaluate model\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    test_outputs = model(X_test)\n",
    "                    \n",
    "                    if config['task_type'] == 'classification':\n",
    "                        if model.output.out_features == 1:\n",
    "                            predicted = (test_outputs > 0.5).float().cpu().numpy().flatten()\n",
    "                        else:\n",
    "                            _, predicted = torch.max(test_outputs, 1)\n",
    "                            predicted = predicted.cpu().numpy()\n",
    "                        \n",
    "                        y_test_np = y_test.cpu().numpy()\n",
    "                        \n",
    "                        accuracy = accuracy_score(y_test_np, predicted)\n",
    "                        precision = precision_score(y_test_np, predicted, average='weighted', zero_division=0)\n",
    "                        f1 = f1_score(y_test_np, predicted, average='weighted', zero_division=0)\n",
    "                        recall = recall_score(y_test_np, predicted, average='weighted', zero_division=0)\n",
    "                        \n",
    "                        metrics = {\n",
    "                            'accuracy': accuracy,\n",
    "                            'precision': precision,\n",
    "                            'f1_score': f1,\n",
    "                            'recall': recall\n",
    "                        }\n",
    "                        \n",
    "                    else:  # regression\n",
    "                        y_test_np = y_test.cpu().numpy()\n",
    "                        predictions = test_outputs.cpu().numpy()\n",
    "                        \n",
    "                        mse = mean_squared_error(y_test_np, predictions)\n",
    "                        rmse = np.sqrt(mse)\n",
    "                        r2 = r2_score(y_test_np, predictions)\n",
    "                        \n",
    "                        # Calculate correlation coefficient\n",
    "                        correlation_matrix = np.corrcoef(y_test_np.flatten(), predictions.flatten())\n",
    "                        correlation = correlation_matrix[0, 1] if not np.isnan(correlation_matrix[0, 1]) else 0\n",
    "                        \n",
    "                        metrics = {\n",
    "                            'mse': mse,\n",
    "                            'rmse': rmse,\n",
    "                            'r2_score': r2,\n",
    "                            'correlation': correlation\n",
    "                        }\n",
    "                \n",
    "                # Store results\n",
    "                result = {\n",
    "                    'run_id': run + 1,\n",
    "                    'seed': 42 + run,\n",
    "                    'training_time': training_time,\n",
    "                    'final_train_loss': train_losses[-1],\n",
    "                    'final_val_loss': val_losses[-1],\n",
    "                    'metrics': metrics,\n",
    "                    'hyperparameters': {\n",
    "                        'hidden_size': config['hidden_size'],\n",
    "                        'learning_rate': config['learning_rate'],\n",
    "                        'weight_decay': config['weight_decay'],\n",
    "                        'epochs': config['epochs']\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                results.append(result)\n",
    "                \n",
    "                # Print progress\n",
    "                if config['task_type'] == 'classification':\n",
    "                    print(f\"  Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
    "                else:\n",
    "                    print(f\"  R²: {r2:.4f}, RMSE: {rmse:.4f}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in run {run + 1}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        all_results[config_name] = {\n",
    "            'algorithm': 'passive_learning',\n",
    "            'dataset': config['dataset_name'],\n",
    "            'task_type': config['task_type'],\n",
    "            'results': results,\n",
    "            'summary': calculate_summary_stats(results, config['task_type'])\n",
    "        }\n",
    "        \n",
    "        # Save individual results\n",
    "        filename = f\"passive_{config_name.lower().replace(' ', '_')}_multiple_runs.json\"\n",
    "        filepath = f'results/passive_multiple_runs/{filename}'\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(all_results[config_name], f, indent=2)\n",
    "        \n",
    "        print(f\"Results saved to {filepath}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def calculate_summary_stats(results, task_type):\n",
    "    if not results:\n",
    "        return {}\n",
    "    \n",
    "    if task_type == 'classification':\n",
    "        accuracies = [r['metrics']['accuracy'] for r in results]\n",
    "        f1_scores = [r['metrics']['f1_score'] for r in results]\n",
    "        precisions = [r['metrics']['precision'] for r in results]\n",
    "        recalls = [r['metrics']['recall'] for r in results]\n",
    "        \n",
    "        return {\n",
    "            'accuracy': {\n",
    "                'mean': np.mean(accuracies),\n",
    "                'std': np.std(accuracies),\n",
    "                'min': np.min(accuracies),\n",
    "                'max': np.max(accuracies)\n",
    "            },\n",
    "            'f1_score': {\n",
    "                'mean': np.mean(f1_scores),\n",
    "                'std': np.std(f1_scores),\n",
    "                'min': np.min(f1_scores),\n",
    "                'max': np.max(f1_scores)\n",
    "            },\n",
    "            'precision': {\n",
    "                'mean': np.mean(precisions),\n",
    "                'std': np.std(precisions),\n",
    "                'min': np.min(precisions),\n",
    "                'max': np.max(precisions)\n",
    "            },\n",
    "            'recall': {\n",
    "                'mean': np.mean(recalls),\n",
    "                'std': np.std(recalls),\n",
    "                'min': np.min(recalls),\n",
    "                'max': np.max(recalls)\n",
    "            },\n",
    "            'training_time': {\n",
    "                'mean': np.mean([r['training_time'] for r in results]),\n",
    "                'std': np.std([r['training_time'] for r in results])\n",
    "            }\n",
    "        }\n",
    "    else:  # regression\n",
    "        r2_scores = [r['metrics']['r2_score'] for r in results]\n",
    "        mse_scores = [r['metrics']['mse'] for r in results]\n",
    "        rmse_scores = [r['metrics']['rmse'] for r in results]\n",
    "        correlations = [r['metrics']['correlation'] for r in results]\n",
    "        \n",
    "        return {\n",
    "            'r2_score': {\n",
    "                'mean': np.mean(r2_scores),\n",
    "                'std': np.std(r2_scores),\n",
    "                'min': np.min(r2_scores),\n",
    "                'max': np.max(r2_scores)\n",
    "            },\n",
    "            'mse': {\n",
    "                'mean': np.mean(mse_scores),\n",
    "                'std': np.std(mse_scores),\n",
    "                'min': np.min(mse_scores),\n",
    "                'max': np.max(mse_scores)\n",
    "            },\n",
    "            'rmse': {\n",
    "                'mean': np.mean(rmse_scores),\n",
    "                'std': np.std(rmse_scores),\n",
    "                'min': np.min(rmse_scores),\n",
    "                'max': np.max(rmse_scores)\n",
    "            },\n",
    "            'correlation': {\n",
    "                'mean': np.mean(correlations),\n",
    "                'std': np.std(correlations),\n",
    "                'min': np.min(correlations),\n",
    "                'max': np.max(correlations)\n",
    "            },\n",
    "            'training_time': {\n",
    "                'mean': np.mean([r['training_time'] for r in results]),\n",
    "                'std': np.std([r['training_time'] for r in results])\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fe0497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure all model parameters based on current notebook settings\n",
    "model_configurations = {\n",
    "    'Simple_Classification': {\n",
    "        'dataset_name': 'Breast Cancer (Simple)',\n",
    "        'task_type': 'classification',\n",
    "        'input_size': X_train.shape[1],\n",
    "        'hidden_size': 32,\n",
    "        'num_classes': sim_num_classes,\n",
    "        'epochs': 1500,\n",
    "        'learning_rate': 0.01,\n",
    "        'weight_decay': 0.0001,\n",
    "        'data': {\n",
    "            'X_train': X_train, 'X_val': X_val, 'X_test': X_test,\n",
    "            'y_train': y_train, 'y_val': y_val, 'y_test': y_test\n",
    "        }\n",
    "    },\n",
    "    'Medium_Classification': {\n",
    "        'dataset_name': 'Heart Disease (Medium)',\n",
    "        'task_type': 'classification',\n",
    "        'input_size': X_train_med.shape[1],\n",
    "        'hidden_size': 256,\n",
    "        'num_classes': med_num_classes,\n",
    "        'epochs': 1000,\n",
    "        'learning_rate': 0.01,\n",
    "        'weight_decay': 0.01,\n",
    "        'data': {\n",
    "            'X_train': X_train_med, 'X_val': X_val_med, 'X_test': X_test_med,\n",
    "            'y_train': y_train_med, 'y_val': y_val_med, 'y_test': y_test_med\n",
    "        }\n",
    "    },\n",
    "    'Complex_Classification': {\n",
    "        'dataset_name': 'Diabetes (Complex)',\n",
    "        'task_type': 'classification',\n",
    "        'input_size': X_train_complex.shape[1],\n",
    "        'hidden_size': 128,\n",
    "        'num_classes': complex_num_classes,\n",
    "        'epochs': 1000,\n",
    "        'learning_rate': 0.005,\n",
    "        'weight_decay': 0.0001,\n",
    "        'data': {\n",
    "            'X_train': X_train_complex, 'X_val': X_val_complex, 'X_test': X_test_complex,\n",
    "            'y_train': y_train_complex, 'y_val': y_val_complex, 'y_test': y_test_complex\n",
    "        }\n",
    "    },\n",
    "    'Simple_Regression': {\n",
    "        'dataset_name': 'California Housing (Simple)',\n",
    "        'task_type': 'regression',\n",
    "        'input_size': X_train_simple_reg.shape[1],\n",
    "        'hidden_size': 64,\n",
    "        'num_classes': None,\n",
    "        'epochs': 1500,\n",
    "        'learning_rate': 0.01,\n",
    "        'weight_decay': 0.01,\n",
    "        'data': {\n",
    "            'X_train': X_train_simple_reg, 'X_val': X_val_simple_reg, 'X_test': X_test_simple_reg,\n",
    "            'y_train': y_train_simple_reg, 'y_val': y_val_simple_reg, 'y_test': y_test_simple_reg\n",
    "        }\n",
    "    },\n",
    "    'Medium_Regression': {\n",
    "        'dataset_name': 'Real Estate (Medium)',\n",
    "        'task_type': 'regression',\n",
    "        'input_size': X_train_med_reg.shape[1],\n",
    "        'hidden_size': 64,\n",
    "        'num_classes': None,\n",
    "        'epochs': 1500,\n",
    "        'learning_rate': 0.01,\n",
    "        'weight_decay': 0.01,\n",
    "        'data': {\n",
    "            'X_train': X_train_med_reg, 'X_val': X_val_med_reg, 'X_test': X_test_med_reg,\n",
    "            'y_train': y_train_med_reg, 'y_val': y_val_med_reg, 'y_test': y_test_med_reg\n",
    "        }\n",
    "    },\n",
    "    'Complex_Regression': {\n",
    "        'dataset_name': 'Comprehensive Housing (Complex)',\n",
    "        'task_type': 'regression',\n",
    "        'input_size': X_train_complex_reg.shape[1],\n",
    "        'hidden_size': 64,\n",
    "        'num_classes': None,\n",
    "        'epochs': 1500,\n",
    "        'learning_rate': 0.05,\n",
    "        'weight_decay': 0.001,\n",
    "        'data': {\n",
    "            'X_train': X_train_complex_reg, 'X_val': X_val_complex_reg, 'X_test': X_test_complex_reg,\n",
    "            'y_train': y_train_complex_reg, 'y_val': y_val_complex_reg, 'y_test': y_test_complex_reg\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "for name, config in model_configurations.items():\n",
    "    print(f\"  {name}: {config['dataset_name']} - {config['task_type']}\")\n",
    "    print(f\"    Hidden: {config['hidden_size']}, LR: {config['learning_rate']}, Epochs: {config['epochs']}\")\n",
    "\n",
    "passive_results = run_multiple_experiments(model_configurations, n_runs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8b450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive results\n",
    "comprehensive_results = {\n",
    "    'experiment_type': 'passive_learning_multiple_runs',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'total_configurations': len(model_configurations),\n",
    "    'runs_per_configuration': 10,\n",
    "    'results': passive_results\n",
    "}\n",
    "\n",
    "# Save master results file\n",
    "master_filepath = 'results/passive_multiple_runs/comprehensive_passive_results.json'\n",
    "with open(master_filepath, 'w') as f:\n",
    "    json.dump(comprehensive_results, f, indent=2)\n",
    "\n",
    "\n",
    "for config_name, results in passive_results.items():\n",
    "    print(f\"\\n{config_name} ({results['dataset']})\")\n",
    "    print(f\"{'-'*50}\")\n",
    "    \n",
    "    summary = results['summary']\n",
    "    \n",
    "    if results['task_type'] == 'classification':\n",
    "        acc = summary['accuracy']\n",
    "        f1 = summary['f1_score']\n",
    "        print(f\"  Accuracy:  {acc['mean']:.4f} ± {acc['std']:.4f} (range: {acc['min']:.4f}-{acc['max']:.4f})\")\n",
    "        print(f\"  F1 Score:  {f1['mean']:.4f} ± {f1['std']:.4f} (range: {f1['min']:.4f}-{f1['max']:.4f})\")\n",
    "    else:\n",
    "        r2 = summary['r2_score']\n",
    "        rmse = summary['rmse']\n",
    "        print(f\"  R² Score:  {r2['mean']:.4f} ± {r2['std']:.4f} (range: {r2['min']:.4f}-{r2['max']:.4f})\")\n",
    "        print(f\"  RMSE:      {rmse['mean']:.4f} ± {rmse['std']:.4f} (range: {rmse['min']:.4f}-{rmse['max']:.4f})\")\n",
    "    \n",
    "    time_stats = summary['training_time']\n",
    "    print(f\"  Avg Time:  {time_stats['mean']:.2f} ± {time_stats['std']:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
