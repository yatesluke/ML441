{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a5f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Updated to use medical classification datasets\n",
    "df_class_simple = pd.read_csv('breast_cancer.csv')\n",
    "df_class_med = pd.read_csv('heart_disease.csv')\n",
    "df_class_complex = pd.read_csv('diabetes.csv')\n",
    "\n",
    "df_reg_simple = pd.read_csv('housing.csv')\n",
    "df_reg_med = pd.read_csv('real_estate_valuation.csv')\n",
    "df_reg_complex = pd.read_csv('Housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8e40ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430ba987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing for different classification datasets\n",
    "def preprocess_classification_data(df, dataset_type='auto'):\n",
    "    # Make a copy to avoid modifying the original\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Auto-detect dataset type based on columns\n",
    "    if dataset_type == 'auto':\n",
    "        if 'Diagnosis' in data.columns and 'radius1' in data.columns:\n",
    "            dataset_type = 'simple'  # Breast cancer dataset\n",
    "        elif 'num' in data.columns and 'age' in data.columns and 'sex' in data.columns:\n",
    "            dataset_type = 'medium'  # Heart disease dataset\n",
    "        elif 'Outcome' in data.columns and 'Pregnancies' in data.columns:\n",
    "            dataset_type = 'complex'  # Diabetes dataset\n",
    "        else:\n",
    "            raise ValueError(\"Cannot auto-detect dataset type. Please specify manually.\")\n",
    "    \n",
    "    label_encoders = {}\n",
    "    \n",
    "    \n",
    "    if dataset_type == 'simple':\n",
    "        # Simple dataset - Breast Cancer Wisconsin dataset\n",
    "        target_column = 'Diagnosis'\n",
    "        \n",
    "        # Convert target to binary (0 for Benign, 1 for Malignant)\n",
    "        data[target_column] = (data[target_column] == 'M').astype(int)\n",
    "        \n",
    "        # All other columns are numeric features (no categorical encoding needed)\n",
    "        print(f\"Breast cancer dataset: {data.shape[1]-1} numeric features\")\n",
    "    \n",
    "    elif dataset_type == 'medium':\n",
    "        # Medium dataset - Heart Disease dataset\n",
    "        target_column = 'num'\n",
    "        \n",
    "        # Convert multi-class target to binary classification (0 vs >0)\n",
    "        data[target_column] = (data[target_column] > 0).astype(int)\n",
    "        \n",
    "        # All features are already numeric (no categorical encoding needed)\n",
    "        print(f\"Heart disease dataset: {data.shape[1]-1} numeric features\")\n",
    "    \n",
    "    elif dataset_type == 'complex':\n",
    "        # Complex dataset - Diabetes dataset\n",
    "        target_column = 'Outcome'\n",
    "        \n",
    "        # Handle missing values encoded as 0s in medical measurements\n",
    "        print(\"Handling missing values (represented as 0s in medical measurements)\")\n",
    "        \n",
    "        # Columns where 0 is not medically possible and indicates missing data\n",
    "        zero_not_acceptable = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "        \n",
    "        # Replace 0s with NaN for these columns, then impute with median\n",
    "        for col in zero_not_acceptable:\n",
    "            if col in data.columns:\n",
    "                # Replace 0 with NaN (except for Insulin where 0 might be acceptable)\n",
    "                if col != 'Insulin':\n",
    "                    data[col] = data[col].replace(0, np.nan)\n",
    "                else:\n",
    "                    # For Insulin, only replace very low values that seem unrealistic\n",
    "                    data[col] = data[col].replace(0, np.nan)\n",
    "                \n",
    "                # Impute missing values with median\n",
    "                data[col].fillna(data[col].median(), inplace=True)\n",
    "        \n",
    "        print(f\"Diabetes dataset: {data.shape[1]-1} numeric features\")\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset type: {dataset_type}\")\n",
    "    \n",
    "    # Final check for missing values\n",
    "    missing_count = data.isnull().sum().sum()\n",
    "    if missing_count > 0:\n",
    "        print(f\"Warning: Found {missing_count} remaining missing values. Removing rows with missing data.\")\n",
    "        data = data.dropna()\n",
    "    \n",
    "    return data, label_encoders, target_column\n",
    "\n",
    "# Function to select and preprocess classification dataset\n",
    "def select_classification_dataset(choice='simple'):\n",
    "    if choice == 'simple':\n",
    "        df = df_class_simple\n",
    "        dataset_name = \"Breast Cancer Wisconsin Dataset\"\n",
    "    elif choice == 'medium':\n",
    "        df = df_class_med\n",
    "        dataset_name = \"Heart Disease Dataset\"\n",
    "    elif choice == 'complex':\n",
    "        df = df_class_complex\n",
    "        dataset_name = \"Pima Indian Diabetes Dataset\"\n",
    "    else:\n",
    "        raise ValueError(\"Choice must be 'simple', 'medium', or 'complex'\")\n",
    "    \n",
    "    processed_data, label_encoders, target_column = preprocess_classification_data(df, choice)\n",
    "    \n",
    "    print(f\"\\n=== {dataset_name} ===\")\n",
    "    print(f\"Original shape: {df.shape}\")\n",
    "    print(f\"Processed shape: {processed_data.shape}\")\n",
    "    print(f\"Target column: {target_column}\")\n",
    "    print(f\"Target distribution: {processed_data[target_column].value_counts().to_dict()}\")\n",
    "    if choice == 'simple':\n",
    "        print(f\"Target meaning: 0=Benign, 1=Malignant\")\n",
    "    elif choice == 'medium':\n",
    "        print(f\"Target meaning: 0=No Heart Disease, 1=Heart Disease\")\n",
    "    elif choice == 'complex':\n",
    "        print(f\"Target meaning: 0=No Diabetes, 1=Diabetes\")\n",
    "    print(f\"Number of features: {len([col for col in processed_data.columns if col != target_column])}\")\n",
    "    \n",
    "    return processed_data, label_encoders, target_column, dataset_name\n",
    "\n",
    "DATASET_CHOICE = 'medium'  # Change this to 'simple' (Breast Cancer), 'medium' (Heart Disease), or 'complex' (Diabetes)\n",
    "\n",
    "processed_data, label_encoders, target_column, dataset_name = select_classification_dataset(DATASET_CHOICE)\n",
    "print(f\"\\nSelected dataset: {dataset_name}\")\n",
    "print(f\"Data types: {processed_data.dtypes.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeeaa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Data preprocessing for different regression datasets\n",
    "def preprocess_regression_data(df, dataset_type='auto'):\n",
    "    # Make a copy to avoid modifying the original\n",
    "    data = df.copy()\n",
    "\n",
    "    # Auto-detect dataset type based on columns\n",
    "    if dataset_type == 'auto':\n",
    "        if 'median_house_value' in data.columns:\n",
    "            dataset_type = 'simple'\n",
    "        elif 'Y house price of unit area' in data.columns:\n",
    "            dataset_type = 'medium'\n",
    "        elif 'price' in data.columns and 'furnishingstatus' in data.columns:\n",
    "            dataset_type = 'complex'\n",
    "        else:\n",
    "            raise ValueError(\"Cannot auto-detect dataset type. Please specify manually.\")\n",
    "\n",
    "    scalers = {}\n",
    "\n",
    "    print(f\"Processing {dataset_type} regression dataset...\")\n",
    "\n",
    "    if dataset_type == 'simple':\n",
    "        # Simple dataset (California Housing - housing.csv)\n",
    "        target_column = 'median_house_value'\n",
    "        \n",
    "        # Handle categorical column: ocean_proximity\n",
    "        categorical_cols = ['ocean_proximity']\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            if col in data.columns:\n",
    "                le = LabelEncoder()\n",
    "                data[col] = le.fit_transform(data[col].astype(str))\n",
    "                scalers[f'{col}_encoder'] = le\n",
    "        \n",
    "        print(f\"Encoded categorical columns: {categorical_cols}\")\n",
    "\n",
    "    elif dataset_type == 'medium':\n",
    "        # Medium dataset (Real Estate Valuation - real_estate_valuation.csv)\n",
    "        target_column = 'Y house price of unit area'\n",
    "        \n",
    "        # This dataset has no categorical variables, all numeric\n",
    "        # Rename columns for better readability (optional)\n",
    "        column_mapping = {\n",
    "            'X1 transaction date': 'transaction_date',\n",
    "            'X2 house age': 'house_age',\n",
    "            'X3 distance to the nearest MRT station': 'distance_to_MRT',\n",
    "            'X4 number of convenience stores': 'convenience_stores',\n",
    "            'X5 latitude': 'latitude',\n",
    "            'X6 longitude': 'longitude',\n",
    "            'Y house price of unit area': 'price_per_unit_area'\n",
    "        }\n",
    "        \n",
    "        # Only rename columns that exist\n",
    "        rename_dict = {k: v for k, v in column_mapping.items() if k in data.columns}\n",
    "        data.rename(columns=rename_dict, inplace=True)\n",
    "        target_column = 'price_per_unit_area' if 'Y house price of unit area' in df.columns else target_column\n",
    "        \n",
    "\n",
    "    elif dataset_type == 'complex':\n",
    "        # Complex dataset (Housing Sales - Housing.csv)\n",
    "        target_column = 'price'\n",
    "        \n",
    "        # Handle categorical variables\n",
    "        categorical_cols = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', \n",
    "                          'airconditioning', 'prefarea', 'furnishingstatus']\n",
    "        \n",
    "        # Binary encode yes/no columns\n",
    "        binary_cols = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', \n",
    "                      'airconditioning', 'prefarea']\n",
    "        \n",
    "        for col in binary_cols:\n",
    "            if col in data.columns:\n",
    "                data[col] = data[col].map({'yes': 1, 'no': 0})\n",
    "                print(f\"Binary encoded: {col}\")\n",
    "        \n",
    "        # Label encode furnishingstatus\n",
    "        if 'furnishingstatus' in data.columns:\n",
    "            le = LabelEncoder()\n",
    "            data['furnishingstatus'] = le.fit_transform(data['furnishingstatus'].astype(str))\n",
    "            scalers['furnishingstatus_encoder'] = le\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset type: {dataset_type}\")\n",
    "\n",
    "    # Check for missing values\n",
    "    missing_count = data.isnull().sum().sum()\n",
    "    if missing_count > 0:\n",
    "        print(f\"\\nWarning: Found {missing_count} missing values. Filling appropriately...\")\n",
    "        for col in data.columns:\n",
    "            if data[col].isnull().sum() > 0:\n",
    "                if data[col].dtype in ['float64', 'int64']:\n",
    "                    data[col].fillna(data[col].mean(), inplace=True)\n",
    "                    print(f\"  - Filled {col} with mean\")\n",
    "                else:\n",
    "                    mode_val = data[col].mode()[0] if not data[col].mode().empty else 'Unknown'\n",
    "                    data[col].fillna(mode_val, inplace=True)\n",
    "                    print(f\"  - Filled {col} with mode\")\n",
    "\n",
    "\n",
    "    return data, scalers, target_column\n",
    "\n",
    "\n",
    "# Function to select and preprocess regression dataset\n",
    "def select_regression_dataset(choice='simple', df_reg_simple=None, df_reg_med=None, df_reg_complex=None):\n",
    "    if choice == 'simple':\n",
    "        df = df_reg_simple\n",
    "        dataset_name = \"Simple California Housing Dataset\"\n",
    "    elif choice == 'medium':\n",
    "        df = df_reg_med\n",
    "        dataset_name = \"Medium Real Estate Valuation Dataset\"\n",
    "    elif choice == 'complex':\n",
    "        df = df_reg_complex\n",
    "        dataset_name = \"Complex Housing Sales Dataset\"\n",
    "    else:\n",
    "        raise ValueError(\"Choice must be 'simple', 'medium', or 'complex'\")\n",
    "    \n",
    "    if df is None:\n",
    "        raise ValueError(f\"Dataset for choice '{choice}' is None. Please provide the dataframe.\")\n",
    "\n",
    "    processed_data, scalers, target_column = preprocess_regression_data(df, choice)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{dataset_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Original shape: {df.shape}\")\n",
    "    print(f\"Processed shape: {processed_data.shape}\")\n",
    "    print(f\"Target column: {target_column}\")\n",
    "    print(f\"Target statistics:\")\n",
    "    print(f\"  - Mean: {processed_data[target_column].mean():.2f}\")\n",
    "    print(f\"  - Std: {processed_data[target_column].std():.2f}\")\n",
    "    print(f\"  - Min: {processed_data[target_column].min():.2f}\")\n",
    "    print(f\"  - Max: {processed_data[target_column].max():.2f}\")\n",
    "    print(f\"\\nFeatures ({len([col for col in processed_data.columns if col != target_column])}):\")\n",
    "    features = [col for col in processed_data.columns if col != target_column]\n",
    "    for i, feat in enumerate(features, 1):\n",
    "        print(f\"  {i}. {feat}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    return processed_data, scalers, target_column, dataset_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd23d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size=1, task_type='classification'):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.task_type = task_type\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Hidden layer\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden_act = nn.LeakyReLU()\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # Final activations\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden_act(self.hidden(x))\n",
    "        x = self.output(x)\n",
    "        if self.task_type == 'classification':\n",
    "            x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            out = self.forward(x)\n",
    "            if self.output_size == 1:\n",
    "                probs = out\n",
    "                return torch.cat([1 - probs, probs], dim=1)\n",
    "            else:\n",
    "                return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50584249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, mean_squared_error, r2_score\n",
    "import math\n",
    "\n",
    "class ActiveLearner:\n",
    "    def __init__(self, model, optimizer, criterion, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        n_batches = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.to(self.device)\n",
    "            batch_y = batch_y.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(batch_x)\n",
    "            \n",
    "            # Fix tensor shape issues - ensure proper dimensions\n",
    "            if outputs.dim() > 1:\n",
    "                outputs = outputs.squeeze(1)  # Remove only dimension 1, not all\n",
    "            if batch_y.dim() == 0:\n",
    "                batch_y = batch_y.unsqueeze(0)  # Add batch dimension if missing\n",
    "            \n",
    "            # Ensure both tensors have the same shape\n",
    "            outputs = outputs.view(-1)\n",
    "            batch_y = batch_y.view(-1).float()\n",
    "            \n",
    "            loss = self.criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        return total_loss / max(1, n_batches)\n",
    "\n",
    "    def evaluate_classification(self, val_loader):\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_true = []\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x = batch_x.to(self.device)\n",
    "                batch_y = batch_y.to(self.device)\n",
    "                outputs = self.model(batch_x)\n",
    "                \n",
    "                # Fix tensor shape issues\n",
    "                if outputs.dim() > 1:\n",
    "                    outputs = outputs.squeeze(1)\n",
    "                if batch_y.dim() == 0:\n",
    "                    batch_y = batch_y.unsqueeze(0)\n",
    "                \n",
    "                outputs = outputs.view(-1)\n",
    "                batch_y = batch_y.view(-1).float()\n",
    "                \n",
    "                val_loss += self.criterion(outputs, batch_y).item()\n",
    "                \n",
    "                # Convert predictions properly\n",
    "                preds = (outputs > 0.5).long().cpu().numpy()\n",
    "                all_preds.extend(preds)\n",
    "                all_true.extend(batch_y.cpu().numpy().astype(int))\n",
    "        \n",
    "        # Calculate metrics using sklearn\n",
    "        accuracy = accuracy_score(all_true, all_preds)\n",
    "        precision = precision_score(all_true, all_preds, average='weighted', zero_division=0)\n",
    "        recall = recall_score(all_true, all_preds, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(all_true, all_preds, average='weighted', zero_division=0)\n",
    "        \n",
    "        avg_loss = val_loss / len(val_loader)\n",
    "        return accuracy, avg_loss, precision, recall, f1\n",
    "    \n",
    "\n",
    "    def evaluate_regression(self, val_loader):\n",
    "        self.model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_true = []\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x = batch_x.to(self.device)\n",
    "                batch_y = batch_y.to(self.device)\n",
    "                outputs = self.model(batch_x)\n",
    "                \n",
    "                # Fix tensor shape issues for regression\n",
    "                if outputs.dim() > 1:\n",
    "                    outputs = outputs.squeeze(1)\n",
    "                if batch_y.dim() == 0:\n",
    "                    batch_y = batch_y.unsqueeze(0)\n",
    "                \n",
    "                outputs = outputs.view(-1)\n",
    "                batch_y = batch_y.view(-1).float()\n",
    "                \n",
    "                val_loss += self.criterion(outputs, batch_y).item()\n",
    "                all_preds.extend(outputs.cpu().numpy().tolist())\n",
    "                all_true.extend(batch_y.cpu().numpy().tolist())\n",
    "\n",
    "        avg_loss = val_loss / max(1, len(val_loader))\n",
    "        mse = mean_squared_error(all_true, all_preds)\n",
    "        rmse = math.sqrt(mse)\n",
    "        r2 = r2_score(all_true, all_preds)\n",
    "        return mse, rmse, r2, avg_loss\n",
    "\n",
    "    def uncertainty_sampling_least_confidence(self, pool_x, n_samples):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            probs = self.model.predict_proba(pool_x.to(self.device))\n",
    "            confidence, _ = torch.max(probs, dim=1)\n",
    "            # Handle case where n_samples > available samples\n",
    "            actual_n_samples = min(n_samples, len(pool_x))\n",
    "            _, idx = torch.topk(-confidence, actual_n_samples)\n",
    "            return idx.cpu().numpy()\n",
    "        \n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bea74e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def run_active_learning_experiment(uncertainty_method='least_confidence', initial_size=1000, query_size=500, \n",
    "                                 n_iterations=10, epochs_per_iteration=10, learning_rate=0.001, \n",
    "                                 weight_decay=1e-5, hidden_size=64, type='classification'):\n",
    "    # Validate parameters against dataset size\n",
    "    pool_size = len(X_train_pool_tensor)\n",
    "    if initial_size > pool_size:\n",
    "        raise ValueError(f\"initial_size ({initial_size}) cannot be larger than training pool size ({pool_size}). \"\n",
    "                        f\"Recommended initial_size: {pool_size // 2}\")\n",
    "    \n",
    "    # Initialize with a small random labeled set\n",
    "    np.random.seed(42)\n",
    "    initial_indices = np.random.choice(len(X_train_pool_tensor), initial_size, replace=False)\n",
    "\n",
    "\n",
    "    # Split into labeled and unlabeled pools\n",
    "    labeled_indices = set(initial_indices)\n",
    "    unlabeled_indices = set(range(len(X_train_pool_tensor))) - labeled_indices\n",
    "    \n",
    "    # Convert to lists for easier manipulation\n",
    "    labeled_indices = list(labeled_indices)\n",
    "    unlabeled_indices = list(unlabeled_indices)\n",
    "    \n",
    "    input_size = X_train_pool_tensor.shape[1]\n",
    "    model = SimpleNN(input_size, hidden_size=64, output_size=1, task_type=type)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=0.9)\n",
    "    if type == 'classification':\n",
    "        criterion = nn.BCELoss()\n",
    "    else:\n",
    "        criterion = nn.MSELoss()\n",
    "    \n",
    "    learner = ActiveLearner(model, optimizer, criterion)\n",
    "    \n",
    "    # Track performance\n",
    "    history = []\n",
    "    \n",
    "    print(f\"Starting Active Learning with {uncertainty_method} uncertainty sampling\")\n",
    "    print(f\"Hyperparameters:\")\n",
    "    print(f\"  Learning Rate: {learning_rate}\")\n",
    "    print(f\"  Weight Decay: {weight_decay}\")\n",
    "    print(f\"  Hidden Size: {hidden_size}\")\n",
    "    print(f\"Initial labeled set size: {len(labeled_indices)}\")\n",
    "    print(f\"Initial unlabeled pool size: {len(unlabeled_indices)}\")\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        print(f\"\\n--- Iteration {iteration + 1}/{n_iterations} ---\")\n",
    "        \n",
    "        # Create current training set\n",
    "        labeled_X = X_train_pool_tensor[labeled_indices]\n",
    "        labeled_y = y_train_pool_tensor[labeled_indices]\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_dataset = TensorDataset(labeled_X, labeled_y)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "        epoch_losses = []\n",
    "        \n",
    "        for epoch in range(epochs_per_iteration):\n",
    "            train_loss = learner.train_epoch(train_loader)\n",
    "            epoch_losses.append(train_loss)\n",
    "            \n",
    "            print(f\"  Epoch {epoch+1}/{epochs_per_iteration}, Loss: {train_loss:.6f}\")\n",
    "            \n",
    "\n",
    "        # Average training loss across epochs\n",
    "        avg_train_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        print(f\"Avg Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        if type == 'regression':\n",
    "            val_mse, val_rmse, val_r2, val_loss = learner.evaluate_regression(val_loader)\n",
    "            print(f\"Val MSE: {val_mse:.4f}, Val RMSE: {val_rmse:.4f}, Val R2: {val_r2:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            # For regression, store MSE instead of accuracy\n",
    "            val_accuracy = -val_mse  # Negative MSE so higher is better\n",
    "            print(f\"Labeled samples: {len(labeled_indices)}, Val Performance (-MSE): {val_accuracy:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        else:\n",
    "            val_accuracy, val_loss, val_precision, val_recall, val_f1 = learner.evaluate_classification(val_loader)\n",
    "            print(f\"Labeled samples: {len(labeled_indices)}, Val Accuracy: {val_accuracy:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"Val Precision: {val_precision:.4f}, Val Recall: {val_recall:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        # Store results\n",
    "        history.append({\n",
    "            'iteration': iteration + 1,\n",
    "            'labeled_size': len(labeled_indices),\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'val_loss': val_loss,\n",
    "            'train_loss': epoch_losses,  \n",
    "        })\n",
    "        \n",
    "        if iteration < n_iterations - 1 and unlabeled_indices:\n",
    "            unlabeled_X = X_train_pool_tensor[unlabeled_indices]\n",
    "            unlabeled_y = y_train_pool_tensor[unlabeled_indices] \n",
    "            \n",
    "            # Adjust query_size if there aren't enough unlabeled samples\n",
    "            actual_query_size = min(query_size, len(unlabeled_indices))\n",
    "            if actual_query_size < query_size:\n",
    "                print(f\"  Warning: Only {len(unlabeled_indices)} unlabeled samples remaining, querying {actual_query_size} instead of {query_size}\")\n",
    "            \n",
    "            if type == 'classification':\n",
    "                query_indices = learner.uncertainty_sampling_least_confidence(unlabeled_X, actual_query_size)\n",
    "              \n",
    "           \n",
    "            \n",
    "            # Convert indices back to global\n",
    "            \n",
    "            if isinstance(query_indices, torch.Tensor):\n",
    "                query_indices = query_indices.cpu().numpy()\n",
    "            elif not isinstance(query_indices, np.ndarray):\n",
    "                query_indices = np.array(query_indices)\n",
    "\n",
    "            selected_global_indices = [unlabeled_indices[i] for i in query_indices]\n",
    "\n",
    "            \n",
    "            labeled_indices.extend(selected_global_indices)\n",
    "            unlabeled_indices = list(set(unlabeled_indices) - set(selected_global_indices))\n",
    "            \n",
    "            print(f\"Queried {len(selected_global_indices)} new samples\")\n",
    "            print(f\"Remaining unlabeled samples: {len(unlabeled_indices)}\")\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    train_eval_loss = 0.0\n",
    "    batches = 0\n",
    "    with torch.no_grad():\n",
    "        for bx, by in train_loader:\n",
    "            out = model(bx)\n",
    "            train_eval_loss += criterion(out.squeeze(), by.float()).item()\n",
    "            batches += 1\n",
    "    train_eval_loss = train_eval_loss / max(1, batches)\n",
    "    print('Train loss (eval-mode):', train_eval_loss)\n",
    "    print('Val loss (from your evaluate()):', val_loss) \n",
    "\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1df119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working ensemble regression active learning\n",
    "def ensemble_regression_active_learning(initial_size=600, query_size=120, n_iterations=3, \n",
    "                                        epochs_per_iteration=50, learning_rate=0.001):\n",
    "    print(\"Starting ensemble regression active learning...\")\n",
    "    \n",
    "    # Initialize\n",
    "    np.random.seed(42)\n",
    "    initial_indices = np.random.choice(len(X_train_pool_tensor), initial_size, replace=False)\n",
    "    \n",
    "    labeled_indices = list(initial_indices)\n",
    "    unlabeled_indices = list(set(range(len(X_train_pool_tensor))) - set(labeled_indices))\n",
    "    \n",
    "    # Create main model\n",
    "    input_size = X_train_pool_tensor.shape[1]\n",
    "    model = SimpleNN(input_size, hidden_size=64, output_size=1, task_type='regression')\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.001, momentum=0.9)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    history = []\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        print(f\"\\n--- Iteration {iteration + 1}/{n_iterations} ---\")\n",
    "        \n",
    "        # Create training data\n",
    "        labeled_X = X_train_pool_tensor[labeled_indices]\n",
    "        labeled_y = y_train_pool_tensor[labeled_indices]\n",
    "        \n",
    "        train_dataset = TensorDataset(labeled_X, labeled_y)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "        # Training main model\n",
    "        model.train()\n",
    "        epoch_losses = []  # Track epoch losses for history\n",
    "        for epoch in range(epochs_per_iteration):\n",
    "            epoch_loss = 0.0\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs.squeeze(), batch_y)\n",
    "                \n",
    "                # Check for NaN in loss\n",
    "                if torch.isnan(loss):\n",
    "                    print(f\"Warning: NaN loss detected at epoch {epoch}\")\n",
    "                    break\n",
    "                    \n",
    "                loss.backward()\n",
    "                \n",
    "                # Clip gradients to prevent explosion\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "            epoch_losses.append(avg_epoch_loss)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"  Epoch {epoch}, Loss: {avg_epoch_loss:.6f}\")\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_true = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(X_val_tensor), 32):\n",
    "                batch_x = X_val_tensor[i:i+32]\n",
    "                batch_y = y_val_tensor[i:i+32]\n",
    "                outputs = model(batch_x)\n",
    "                val_loss += criterion(outputs.squeeze(), batch_y).item()\n",
    "                all_preds.extend(outputs.squeeze().numpy())\n",
    "                all_true.extend(batch_y.numpy())\n",
    "        \n",
    "        val_mse = val_loss / (len(X_val_tensor) // 32 + 1)\n",
    "        val_r2 = r2_score(all_true, all_preds)\n",
    "        \n",
    "        print(f\"Labeled: {len(labeled_indices)}, Val MSE: {val_mse:.4f}, Val R²: {val_r2:.4f}\")\n",
    "        \n",
    "        history.append({\n",
    "            'iteration': iteration + 1,\n",
    "            'labeled_size': len(labeled_indices),\n",
    "            'val_mse': val_mse,\n",
    "            'val_r2': val_r2,\n",
    "            'train_loss': epoch_losses  # Add epoch losses to match expected format\n",
    "        })\n",
    "        \n",
    "        # Ensemble-based uncertainty sampling\n",
    "        if iteration < n_iterations - 1 and len(unlabeled_indices) >= query_size:\n",
    "            print(\"  Computing ensemble predictions for uncertainty sampling...\")\n",
    "            \n",
    "            unlabeled_X = X_train_pool_tensor[unlabeled_indices]\n",
    "            n_models = 3  # Smaller ensemble to avoid issues\n",
    "            ensemble_preds = []\n",
    "            \n",
    "            for m in range(n_models):\n",
    "                # Create a new model with different initialization\n",
    "                ensemble_model = SimpleNN(input_size, hidden_size=64, output_size=1, task_type='regression')\n",
    "                ensemble_optimizer = optim.SGD(ensemble_model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "                \n",
    "                # Quick training (fewer epochs for ensemble diversity)\n",
    "                ensemble_model.train()\n",
    "                for e in range(20):  # Just 20 epochs for ensemble models\n",
    "                    for batch_x, batch_y in train_loader:\n",
    "                        ensemble_optimizer.zero_grad()\n",
    "                        outputs = ensemble_model(batch_x)\n",
    "                        loss = criterion(outputs.squeeze(), batch_y)\n",
    "                        \n",
    "                        if not torch.isnan(loss):\n",
    "                            loss.backward()\n",
    "                            torch.nn.utils.clip_grad_norm_(ensemble_model.parameters(), max_norm=1.0)\n",
    "                            ensemble_optimizer.step()\n",
    "                \n",
    "                # Get predictions for unlabeled data\n",
    "                ensemble_model.eval()\n",
    "                with torch.no_grad():\n",
    "                    preds = ensemble_model(unlabeled_X).squeeze().numpy()\n",
    "                    \n",
    "                    # Check for NaN predictions\n",
    "                    if not np.isnan(preds).any():\n",
    "                        ensemble_preds.append(preds)\n",
    "                    else:\n",
    "                        print(f\"    Warning: Model {m} produced NaN, skipping\")\n",
    "                \n",
    "                # Clean up model to prevent memory leaks\n",
    "                del ensemble_model\n",
    "                del ensemble_optimizer\n",
    "            \n",
    "            if len(ensemble_preds) >= 2:  # Need at least 2 valid models\n",
    "                ensemble_preds = np.array(ensemble_preds)\n",
    "                pred_variance = np.var(ensemble_preds, axis=0)\n",
    "                \n",
    "                # Select samples with highest prediction variance\n",
    "                selected_idx = np.argsort(-pred_variance)[:query_size]\n",
    "                selected_global_idx = [unlabeled_indices[i] for i in selected_idx]\n",
    "                \n",
    "                labeled_indices.extend(selected_global_idx)\n",
    "                unlabeled_indices = list(set(unlabeled_indices) - set(selected_global_idx))\n",
    "                print(f\"  Added {query_size} samples using ensemble uncertainty\")\n",
    "            else:\n",
    "                # Fallback to random sampling if ensemble failed\n",
    "                selected_indices = np.random.choice(unlabeled_indices, query_size, replace=False)\n",
    "                labeled_indices.extend(selected_indices)\n",
    "                unlabeled_indices = list(set(unlabeled_indices) - set(selected_indices))\n",
    "                print(f\"  Fallback: Added {query_size} random samples\")\n",
    "    \n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cc6548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Active Learning with Uncertainty Sampling - Complete Implementation\n",
    "def run_active_learning_with_visualization(uncertainty_method='least_confidence', initial_size=500, \n",
    "                                         query_size=200, n_iterations=6, epochs_per_iteration=5,\n",
    "                                         learning_rate=0.001, weight_decay=1e-5, hidden_size=64, classification=True):\n",
    "    \n",
    "    # Run active learning experiment with specified hyperparameters\n",
    "    print(f\"Running Active Learning with {uncertainty_method} uncertainty sampling...\")\n",
    "    \n",
    "    if classification:\n",
    "        # Use general active learning for classification\n",
    "        model, history = run_active_learning_experiment(\n",
    "            uncertainty_method=uncertainty_method,\n",
    "            initial_size=initial_size,\n",
    "            query_size=query_size,\n",
    "            n_iterations=n_iterations,\n",
    "            epochs_per_iteration=epochs_per_iteration,\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            hidden_size=hidden_size,\n",
    "            type='classification',\n",
    "        )\n",
    "    else:\n",
    "        # Use ensemble regression active learning for regression\n",
    "        model, history = ensemble_regression_active_learning(\n",
    "            initial_size=initial_size,\n",
    "            query_size=query_size,\n",
    "            n_iterations=n_iterations,\n",
    "            epochs_per_iteration=epochs_per_iteration,\n",
    "            learning_rate=learning_rate\n",
    "        )\n",
    "        # Convert history format to match expected format\n",
    "        for item in history:\n",
    "            item['val_accuracy'] = item['val_r2'] # Use R² as accuracy\n",
    "            item['val_loss'] = item['val_mse']  # Use MSE as loss\n",
    "            # Keep original train_loss from regression training, don't overwrite with val_mse\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=0.9)\n",
    "    \n",
    "    if classification:\n",
    "        learner = ActiveLearner(model, optimizer, nn.BCELoss())\n",
    "        test_accuracy, test_loss, test_precision, test_recall, test_f1 = learner.evaluate_classification(test_loader)\n",
    "        \n",
    "        # Get predictions for classification report\n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        all_true_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                outputs = model(batch_x)\n",
    "                predicted = (outputs.squeeze() > 0.5).long()\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_true_labels.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        # Calculate test metrics\n",
    "        test_metrics = {\n",
    "            'accuracy': test_accuracy,\n",
    "            'precision': test_precision,\n",
    "            'recall': test_recall,\n",
    "            'f1': test_f1\n",
    "        }\n",
    "    else:\n",
    "        # Regression\n",
    "        learner = ActiveLearner(model, optimizer, nn.MSELoss())\n",
    "        test_mse, test_rmse, test_r2, test_loss = learner.evaluate_regression(test_loader)\n",
    "        \n",
    "        # Get predictions for regression\n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        all_true_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                outputs = model(batch_x)\n",
    "                all_predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "                all_true_labels.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        test_metrics = {\n",
    "            'mse': test_mse,\n",
    "            'rmse': test_rmse,\n",
    "            'r2': test_r2\n",
    "        }\n",
    "\n",
    "    # Create visualizations\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    labeled_sizes = [item['iteration'] for item in history]\n",
    "    metric_values = [item['val_accuracy'] for item in history]\n",
    "    plt.plot(labeled_sizes, metric_values, 'b-o', linewidth=3, markersize=8)\n",
    "    plt.xlabel('Iteration')\n",
    "    if classification:\n",
    "        plt.ylabel('Validation Accuracy')\n",
    "        plt.title('Active Learning Progress')\n",
    "    else:\n",
    "        plt.ylabel('Validation Performance (R²)')\n",
    "        plt.title('Active Learning Progress (Higher = Better)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot 2: Validation loss trend across iterations\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    iterations = [item['iteration'] for item in history]\n",
    "    val_losses = [item['val_loss'] for item in history]\n",
    "    plt.plot(iterations, val_losses, 'r-o', linewidth=2, markersize=6)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.title('Validation Loss (per iteration)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot 3: Per-iteration epoch losses placed in [i, i+1] intervals\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    total_iters = len(history)\n",
    "    cmap = plt.get_cmap('tab10')\n",
    "\n",
    "    for idx, item in enumerate(history):\n",
    "        epoch_losses = item['train_loss']\n",
    "        m = len(epoch_losses)\n",
    "        if m > 1:\n",
    "            # map epochs into interval [idx, idx+1) so iteration 0 -> [0,1), iteration 1 -> [1,2), etc.\n",
    "            xs = np.linspace(idx, idx + 1, m, endpoint=False)\n",
    "        else:\n",
    "            xs = np.array([idx + 0.5])\n",
    "        plt.plot(xs, epoch_losses, color=cmap(idx % 10), alpha=0.8)\n",
    "        # Mark validation loss at the right boundary of the interval (idx+1)\n",
    "        plt.scatter([idx + 1], [item['val_loss']], color='k', marker='x', s=50)\n",
    "\n",
    "    # Draw faint vertical separators to indicate iteration boundaries\n",
    "    for k in range(total_iters + 1):\n",
    "        plt.axvline(k, color='gray', alpha=0.2, linestyle='--')\n",
    "\n",
    "    plt.xlim(0, max(1, total_iters))\n",
    "    plt.xlabel('Iteration (each integer interval contains that iteration\\'s epochs)')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.title('Per-iteration Epoch Training Losses (0-1,1-2,...) with Validation markers')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Legend: custom elements\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = [Line2D([0], [0], color='gray', lw=2, label='Training (per-iteration lines)'),\n",
    "                       Line2D([0], [0], marker='x', color='k', label='Validation (interval boundary)', linestyle='None')]\n",
    "    plt.legend(handles=legend_elements)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nFinal Results:\")\n",
    "    if classification:\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "        print(f\"Test Precision: {test_metrics['precision']:.4f}\")\n",
    "        print(f\"Test Recall: {test_metrics['recall']:.4f}\")  \n",
    "        print(f\"Test F1-Score: {test_metrics['f1']:.4f}\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}\")\n",
    "        \n",
    "        # Dynamic target names based on dataset\n",
    "        if target_column == 'Default':\n",
    "            target_names = ['No Default', 'Default']\n",
    "        elif target_column == 'loan_status':\n",
    "            target_names = ['Approved', 'Rejected']\n",
    "        elif target_column == 'LoanApproved':\n",
    "            target_names = ['Rejected', 'Approved']\n",
    "        else:\n",
    "            target_names = ['Class 0', 'Class 1']\n",
    "        \n",
    "        print(f\"\\nClassification Report for {target_column}:\")\n",
    "        print(classification_report(all_true_labels, all_predictions, target_names=target_names))\n",
    "    else:\n",
    "        print(f\"Test MSE: {test_metrics['mse']:.4f}\")\n",
    "        print(f\"Test RMSE: {test_metrics['rmse']:.4f}\")\n",
    "        print(f\"Test R²: {test_metrics['r2']:.4f}\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}\")\n",
    "        \n",
    "        print(f\"\\nRegression Results for {target_column}:\")\n",
    "        print(f\"Mean Absolute Error: {np.mean(np.abs(np.array(all_predictions) - np.array(all_true_labels))):.4f}\")\n",
    "        print(f\"Target Range: {np.min(all_true_labels):.2f} to {np.max(all_true_labels):.2f}\")\n",
    "        print(f\"Prediction Range: {np.min(all_predictions):.2f} to {np.max(all_predictions):.2f}\")\n",
    "    \n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed578b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_stopping_points(history, task_type='classification'):\n",
    "    \"\"\"\n",
    "    Analyze training history to suggest optimal stopping points\n",
    "    \"\"\"\n",
    "    iterations = [h['iteration'] for h in history]\n",
    "    \n",
    "    if task_type == 'classification':\n",
    "        val_metric = [h['val_accuracy'] for h in history]\n",
    "        val_losses = [h['val_loss'] for h in history]\n",
    "        metric_name = 'Validation Accuracy'\n",
    "    else:\n",
    "        val_metric = [h['val_accuracy'] for h in history]  # This is R² for regression\n",
    "        val_losses = [h['val_loss'] for h in history]  # This is MSE for regression\n",
    "        metric_name = 'Validation R²'\n",
    "    \n",
    "    # Find best performance\n",
    "    if task_type == 'classification':\n",
    "        best_idx = np.argmax(val_metric)\n",
    "    else:\n",
    "        best_idx = np.argmax(val_metric)  # Higher R² is better\n",
    "    \n",
    "    best_iteration = iterations[best_idx]\n",
    "    best_metric = val_metric[best_idx]\n",
    "    \n",
    "    # Check for plateau (less than 1% improvement in last 3 iterations)\n",
    "    if len(val_metric) >= 3:\n",
    "        recent_improvement = (val_metric[-1] - val_metric[-3]) / abs(val_metric[-3])\n",
    "        plateau_detected = abs(recent_improvement) < 0.01\n",
    "    else:\n",
    "        plateau_detected = False\n",
    "    \n",
    "    print(f\"\\n=== Stopping Point Analysis ===\")\n",
    "    print(f\"Best {metric_name}: {best_metric:.4f} at iteration {best_iteration}\")\n",
    "    print(f\"Current {metric_name}: {val_metric[-1]:.4f} at iteration {iterations[-1]}\")\n",
    "    print(f\"Plateau detected: {plateau_detected}\")\n",
    "    \n",
    "    if plateau_detected:\n",
    "        print(\"RECOMMENDATION: Consider stopping - performance has plateaued\")\n",
    "    elif best_iteration == iterations[-1]:\n",
    "        print(\"RECOMMENDATION: Continue training - still improving\")\n",
    "    else:\n",
    "        print(f\"RECOMMENDATION: Consider stopping - best performance was at iteration {best_iteration}\")\n",
    "    \n",
    "    return best_iteration, best_metric, plateau_detected\n",
    "\n",
    "# best_iter, best_perf, plateau = analyze_stopping_points(history, 'classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e823bf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "DATASET_CHOICE = 'simple'  # 'simple', 'medium', or 'complex'\n",
    "# Get the selected dataset\n",
    "processed_data, label_encoders, target_column, dataset_name = select_classification_dataset(DATASET_CHOICE)\n",
    "\n",
    "# Prepare features and target\n",
    "X = processed_data.drop(target_column, axis=1).values\n",
    "y = processed_data[target_column].values\n",
    "\n",
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train_pool, X_val, y_train_pool, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Replace StandardScaler with MinMaxScaler\n",
    "scaler = MinMaxScaler()  # Scales to [0,1] range\n",
    "X_train_pool_scaled = scaler.fit_transform(X_train_pool)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors - Use FloatTensor for BCELoss compatibility\n",
    "import torch\n",
    "X_train_pool_tensor = torch.FloatTensor(X_train_pool_scaled)\n",
    "y_train_pool_tensor = torch.FloatTensor(y_train_pool)  # Changed to FloatTensor for BCELoss\n",
    "X_val_tensor = torch.FloatTensor(X_val_scaled)\n",
    "y_val_tensor = torch.FloatTensor(y_val)  # Changed to FloatTensor for BCELoss\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.FloatTensor(y_test)  # Changed to FloatTensor for BCELoss\n",
    "\n",
    "\n",
    "# Store metadata for later use\n",
    "dataset_info = {\n",
    "    'name': dataset_name,\n",
    "    'choice': DATASET_CHOICE,\n",
    "    'target_column': target_column,\n",
    "    'features': X.shape[1],\n",
    "    'samples': X.shape[0],\n",
    "    'classes': len(np.unique(y))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455de3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate optimal parameters for 80% training pool utilization\n",
    "training_pool_size = len(X_train_pool_tensor)\n",
    "target_samples = int(0.8 * training_pool_size)  \n",
    "initial_size = int(0.05 * training_pool_size)  \n",
    "query_size = 5\n",
    "n_iterations =  (target_samples - initial_size) // query_size + 1 \n",
    "epochs_per_iteration = 1500 // n_iterations\n",
    "print(f\"Training pool size: {training_pool_size}\")\n",
    "print(f\"Target samples (80%): {target_samples}\")\n",
    "print(f\"Initial size (5%): {initial_size}\")\n",
    "print(f\"Query size: {query_size}\")\n",
    "print(f\"Number of iterations: {n_iterations}\")\n",
    "print(f\"Final labeled size: {initial_size + (n_iterations-1) * query_size}\")\n",
    "\n",
    "model, history = run_active_learning_with_visualization(\n",
    "    uncertainty_method='least_confidence', \n",
    "    initial_size=initial_size,   \n",
    "    query_size=query_size,       \n",
    "    n_iterations=n_iterations,    \n",
    "    epochs_per_iteration=epochs_per_iteration,               \n",
    "    learning_rate=0.01,                   \n",
    "    weight_decay=0.0001,                    \n",
    "    hidden_size=32,               \n",
    "    classification=True\n",
    ")\n",
    "\n",
    "best_iter, best_perf, plateau = analyze_stopping_points(history, 'classification')\n",
    "print(f\"Best iteration: {best_iter}, Best performance: {best_perf:.4f}, Plateau: {plateau}\")\n",
    "\n",
    "model, history = run_active_learning_with_visualization(\n",
    "    uncertainty_method='least_confidence', \n",
    "    initial_size=initial_size,   \n",
    "    query_size=query_size,       \n",
    "    n_iterations=best_iter,    \n",
    "    epochs_per_iteration=epochs_per_iteration,               \n",
    "    learning_rate=0.01,                   \n",
    "    weight_decay=0.0001,                    \n",
    "    hidden_size=32,               \n",
    "    classification=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e09e59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "DATASET_CHOICE = 'medium'  # 'simple', 'medium', or 'complex'\n",
    "# Get the selected dataset\n",
    "processed_data, label_encoders, target_column, dataset_name = select_classification_dataset(DATASET_CHOICE)\n",
    "\n",
    "# Prepare features and target\n",
    "X = processed_data.drop(target_column, axis=1).values\n",
    "y = processed_data[target_column].values\n",
    "\n",
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initial split: 60% pool, 20% validation, 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train_pool, X_val, y_train_pool, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Replace StandardScaler with MinMaxScaler\n",
    "scaler = MinMaxScaler()  # Scales to [0,1] range\n",
    "X_train_pool_scaled = scaler.fit_transform(X_train_pool)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors - Use FloatTensor for BCELoss compatibility\n",
    "import torch\n",
    "X_train_pool_tensor = torch.FloatTensor(X_train_pool_scaled)\n",
    "y_train_pool_tensor = torch.FloatTensor(y_train_pool)  # Changed to FloatTensor for BCELoss\n",
    "X_val_tensor = torch.FloatTensor(X_val_scaled)\n",
    "y_val_tensor = torch.FloatTensor(y_val)  # Changed to FloatTensor for BCELoss\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.FloatTensor(y_test)  # Changed to FloatTensor for BCELoss\n",
    "\n",
    "\n",
    "# Store metadata for later use\n",
    "dataset_info = {\n",
    "    'name': dataset_name,\n",
    "    'choice': DATASET_CHOICE,\n",
    "    'target_column': target_column,\n",
    "    'features': X.shape[1],\n",
    "    'samples': X.shape[0],\n",
    "    'classes': len(np.unique(y))\n",
    "}\n",
    "\n",
    "training_pool_size = len(X_train_pool_tensor)\n",
    "target_samples = int(0.8 * training_pool_size)  \n",
    "initial_size = int(0.05 * training_pool_size)  \n",
    "query_size = 5\n",
    "n_iterations =  (target_samples - initial_size) // query_size + 1 \n",
    "epochs_per_iteration = 1000 // n_iterations\n",
    "print(f\"Training pool size: {training_pool_size}\")\n",
    "print(f\"Target samples (80%): {target_samples}\")\n",
    "print(f\"Initial size (5%): {initial_size}\")\n",
    "print(f\"Query size: {query_size}\")\n",
    "print(f\"Number of iterations: {n_iterations}\")\n",
    "print(f\"Final labeled size: {initial_size + (n_iterations-1) * query_size}\")\n",
    "\n",
    "\n",
    "\n",
    "model, history = run_active_learning_with_visualization(\n",
    "    uncertainty_method='least_confidence', \n",
    "    initial_size=initial_size,   \n",
    "    query_size=query_size,       \n",
    "    n_iterations=n_iterations,    \n",
    "    epochs_per_iteration=epochs_per_iteration,               \n",
    "    learning_rate=0.01,                   \n",
    "    weight_decay=0.01,                    \n",
    "    hidden_size=256,               \n",
    "    classification=True\n",
    ")\n",
    "\n",
    "\n",
    "best_iter, best_perf, plateau = analyze_stopping_points(history, 'classification')\n",
    "print(f\"Best iteration: {best_iter}, Best performance: {best_perf:.4f}, Plateau: {plateau}\")\n",
    "\n",
    "model, history = run_active_learning_with_visualization(\n",
    "    uncertainty_method='least_confidence', \n",
    "    initial_size=initial_size,   \n",
    "    query_size=query_size,       \n",
    "    n_iterations=best_iter,    \n",
    "    epochs_per_iteration=epochs_per_iteration,               \n",
    "    learning_rate=0.01,                   \n",
    "    weight_decay=0.01,                    \n",
    "    hidden_size=256,               \n",
    "    classification=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f4a958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "DATASET_CHOICE = 'complex'  # 'simple', 'medium', or 'complex'\n",
    "# Get the selected dataset\n",
    "processed_data, label_encoders, target_column, dataset_name = select_classification_dataset(DATASET_CHOICE)\n",
    "\n",
    "# Prepare features and target\n",
    "X = processed_data.drop(target_column, axis=1).values\n",
    "y = processed_data[target_column].values\n",
    "\n",
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initial split: 60% pool, 20% validation, 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train_pool, X_val, y_train_pool, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Replace StandardScaler with MinMaxScaler\n",
    "scaler = MinMaxScaler()  # Scales to [0,1] range\n",
    "X_train_pool_scaled = scaler.fit_transform(X_train_pool)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors - Use FloatTensor for BCELoss compatibility\n",
    "import torch\n",
    "X_train_pool_tensor = torch.FloatTensor(X_train_pool_scaled)\n",
    "y_train_pool_tensor = torch.FloatTensor(y_train_pool)  # Changed to FloatTensor for BCELoss\n",
    "X_val_tensor = torch.FloatTensor(X_val_scaled)\n",
    "y_val_tensor = torch.FloatTensor(y_val)  # Changed to FloatTensor for BCELoss\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.FloatTensor(y_test)  # Changed to FloatTensor for BCELoss\n",
    "\n",
    "\n",
    "# Store metadata for later use\n",
    "dataset_info = {\n",
    "    'name': dataset_name,\n",
    "    'choice': DATASET_CHOICE,\n",
    "    'target_column': target_column,\n",
    "    'features': X.shape[1],\n",
    "    'samples': X.shape[0],\n",
    "    'classes': len(np.unique(y))\n",
    "}\n",
    "\n",
    "training_pool_size = len(X_train_pool_tensor)\n",
    "target_samples = int(0.8 * training_pool_size)  \n",
    "initial_size = int(0.05 * training_pool_size)  \n",
    "query_size = 5\n",
    "n_iterations =  (target_samples - initial_size) // query_size + 1 \n",
    "epochs_per_iteration = 1000 // n_iterations\n",
    "print(f\"Training pool size: {training_pool_size}\")\n",
    "print(f\"Target samples (80%): {target_samples}\")\n",
    "print(f\"Initial size (5%): {initial_size}\")\n",
    "print(f\"Query size: {query_size}\")\n",
    "print(f\"Number of iterations: {n_iterations}\")\n",
    "print(f\"Final labeled size: {initial_size + (n_iterations-1) * query_size}\")\n",
    "\n",
    "\n",
    "\n",
    "model, history = run_active_learning_with_visualization(\n",
    "    uncertainty_method='least_confidence', \n",
    "    initial_size=initial_size,   \n",
    "    query_size=query_size,       \n",
    "    n_iterations=n_iterations,    \n",
    "    epochs_per_iteration=epochs_per_iteration,               \n",
    "    learning_rate=0.005,                   \n",
    "    weight_decay=0.0001,                    \n",
    "    hidden_size=128,               \n",
    "    classification=True\n",
    ")\n",
    "\n",
    "best_iter, best_perf, plateau = analyze_stopping_points(history, 'classification')\n",
    "print(f\"Best iteration: {best_iter}, Best performance: {best_perf:.4f}, Plateau: {plateau}\")\n",
    "\n",
    "model, history = run_active_learning_with_visualization(\n",
    "    uncertainty_method='least_confidence', \n",
    "    initial_size=initial_size,   \n",
    "    query_size=query_size,       \n",
    "    n_iterations=best_iter,    \n",
    "    epochs_per_iteration=epochs_per_iteration,               \n",
    "    learning_rate=0.005,                   \n",
    "    weight_decay=0.0001,                    \n",
    "    hidden_size=128,               \n",
    "    classification=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fb8386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "DATASET_CHOICE = 'simple'  \n",
    "# Get the selected dataset\n",
    "processed_data, label_encoders, target_column, dataset_name = select_regression_dataset(choice='simple', df_reg_simple=df_reg_simple, df_reg_med=df_reg_med, df_reg_complex=df_reg_complex)\n",
    "\n",
    "# Prepare features and target\n",
    "X = processed_data.drop(target_column, axis=1).values\n",
    "y = processed_data[target_column].values\n",
    "\n",
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initial split: 60% pool, 20% validation, 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_pool, X_val, y_train_pool, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "feature_scaler = MinMaxScaler()  # Scales to [0,1] range\n",
    "X_train_pool_scaled = feature_scaler.fit_transform(X_train_pool)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "# Scale targets to [0,1] range\n",
    "target_scaler = MinMaxScaler()\n",
    "y_train_pool_scaled = target_scaler.fit_transform(y_train_pool.reshape(-1, 1)).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(f\"Original target range: {y_train_pool.min():.2f} to {y_train_pool.max():.2f}\")\n",
    "print(f\"Scaled target range: {y_train_pool_scaled.min():.3f} to {y_train_pool_scaled.max():.3f}\")\n",
    "\n",
    "# Convert to PyTorch tensors - USE FLOAT TENSORS FOR REGRESSION!\n",
    "import torch\n",
    "X_train_pool_tensor = torch.FloatTensor(X_train_pool_scaled)\n",
    "y_train_pool_tensor = torch.FloatTensor(y_train_pool_scaled)  # Use scaled targets\n",
    "X_val_tensor = torch.FloatTensor(X_val_scaled)\n",
    "y_val_tensor = torch.FloatTensor(y_val_scaled)  # Use scaled targets\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.FloatTensor(y_test_scaled)  # Use scaled targets\n",
    "\n",
    "print(f\"Data shapes:\")\n",
    "print(f\"X_train_pool: {X_train_pool_tensor.shape}\")\n",
    "print(f\"y_train_pool: {y_train_pool_tensor.shape} (dtype: {y_train_pool_tensor.dtype})\")\n",
    "print(f\"Target values range: {y_train_pool_tensor.min():.3f} to {y_train_pool_tensor.max():.3f}\")\n",
    "\n",
    "# Store metadata for later use (including scalers for inverse transform)\n",
    "dataset_info = {\n",
    "    'name': dataset_name,\n",
    "    'choice': DATASET_CHOICE,\n",
    "    'target_column': target_column,\n",
    "    'features': X.shape[1],\n",
    "    'samples': X.shape[0],\n",
    "    'target_range': f\"{y.min():.2f} - {y.max():.2f}\",\n",
    "    'feature_scaler': feature_scaler,\n",
    "    'target_scaler': target_scaler\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38acb6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pool_size = len(X_train_pool_tensor)\n",
    "target_samples = int(0.8 * training_pool_size)  \n",
    "initial_size = int(0.10 * training_pool_size)  \n",
    "query_size = int(0.01 * training_pool_size)  \n",
    "n_iterations =  (target_samples - initial_size) // query_size + 1 \n",
    "epochs_per_iteration = 1500 // n_iterations\n",
    "print(f\"Training pool size: {training_pool_size}\")\n",
    "print(f\"Target samples (80%): {target_samples}\")\n",
    "print(f\"Initial size (10%): {initial_size}\")\n",
    "print(f\"Query size: {query_size}\")\n",
    "print(f\"Number of iterations: {n_iterations}\")\n",
    "print(f\"Final labeled size: {initial_size + (n_iterations-1) * query_size}\")\n",
    "\n",
    "\n",
    "\n",
    "model, history = run_active_learning_with_visualization(\n",
    "    uncertainty_method='ensemble_regression',  \n",
    "    initial_size=initial_size,                       \n",
    "    query_size=query_size,                         \n",
    "    n_iterations=n_iterations,                         \n",
    "    epochs_per_iteration=epochs_per_iteration,  # Reduced from 1000              \n",
    "    learning_rate=0.01,       # Much smaller learning rate               \n",
    "    weight_decay=0.01,        # Add some regularization                    \n",
    "    hidden_size=64,                          \n",
    "    classification=False\n",
    ")\n",
    "\n",
    "best_iter, best_perf, plateau = analyze_stopping_points(history, 'regression')\n",
    "print(f\"Best iteration: {best_iter}, Best performance: {best_perf:.4f}, Plateau: {plateau}\")\n",
    "\n",
    "\n",
    "model, history = run_active_learning_with_visualization(\n",
    "    uncertainty_method='ensemble_regression',  \n",
    "    initial_size=initial_size,                       \n",
    "    query_size=query_size,                         \n",
    "    n_iterations=best_iter,                         \n",
    "    epochs_per_iteration=epochs_per_iteration,  # Reduced from 1000              \n",
    "    learning_rate=0.01,       # Much smaller learning rate               \n",
    "    weight_decay=0.01,        # Add some regularization                    \n",
    "    hidden_size=64,                          \n",
    "    classification=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0c5fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "DATASET_CHOICE = 'medium'  \n",
    "# Get the selected dataset\n",
    "processed_data, label_encoders, target_column, dataset_name = select_regression_dataset(choice='medium', df_reg_simple=df_reg_simple, df_reg_med=df_reg_med, df_reg_complex=df_reg_complex)\n",
    "\n",
    "# Prepare features and target\n",
    "X = processed_data.drop(target_column, axis=1).values\n",
    "y = processed_data[target_column].values\n",
    "\n",
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initial split: 60% pool, 20% validation, 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_pool, X_val, y_train_pool, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "feature_scaler = MinMaxScaler()  # Scales to [0,1] range\n",
    "X_train_pool_scaled = feature_scaler.fit_transform(X_train_pool)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "# Scale targets to [0,1] range\n",
    "target_scaler = MinMaxScaler()\n",
    "y_train_pool_scaled = target_scaler.fit_transform(y_train_pool.reshape(-1, 1)).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(f\"Original target range: {y_train_pool.min():.2f} to {y_train_pool.max():.2f}\")\n",
    "print(f\"Scaled target range: {y_train_pool_scaled.min():.3f} to {y_train_pool_scaled.max():.3f}\")\n",
    "\n",
    "# Convert to PyTorch tensors - USE FLOAT TENSORS FOR REGRESSION!\n",
    "import torch\n",
    "X_train_pool_tensor = torch.FloatTensor(X_train_pool_scaled)\n",
    "y_train_pool_tensor = torch.FloatTensor(y_train_pool_scaled)  # Use scaled targets\n",
    "X_val_tensor = torch.FloatTensor(X_val_scaled)\n",
    "y_val_tensor = torch.FloatTensor(y_val_scaled)  # Use scaled targets\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.FloatTensor(y_test_scaled)  # Use scaled targets\n",
    "\n",
    "print(f\"Data shapes:\")\n",
    "print(f\"X_train_pool: {X_train_pool_tensor.shape}\")\n",
    "print(f\"y_train_pool: {y_train_pool_tensor.shape} (dtype: {y_train_pool_tensor.dtype})\")\n",
    "print(f\"Target values range: {y_train_pool_tensor.min():.3f} to {y_train_pool_tensor.max():.3f}\")\n",
    "\n",
    "# Store metadata for later use (including scalers for inverse transform)\n",
    "dataset_info = {\n",
    "    'name': dataset_name,\n",
    "    'choice': DATASET_CHOICE,\n",
    "    'target_column': target_column,\n",
    "    'features': X.shape[1],\n",
    "    'samples': X.shape[0],\n",
    "    'target_range': f\"{y.min():.2f} - {y.max():.2f}\",\n",
    "    'feature_scaler': feature_scaler,\n",
    "    'target_scaler': target_scaler\n",
    "}\n",
    "\n",
    "training_pool_size = len(X_train_pool_tensor)\n",
    "target_samples = int(0.8 * training_pool_size)  \n",
    "initial_size = int(0.05 * training_pool_size)  \n",
    "query_size = 5  \n",
    "n_iterations =  (target_samples - initial_size) // query_size + 1 \n",
    "epochs_per_iteration = 1500 // n_iterations\n",
    "print(f\"Training pool size: {training_pool_size}\")\n",
    "print(f\"Target samples (80%): {target_samples}\")\n",
    "print(f\"Initial size (5%): {initial_size}\")\n",
    "print(f\"Query size: {query_size}\")\n",
    "print(f\"Number of iterations: {n_iterations}\")\n",
    "print(f\"Final labeled size: {initial_size + (n_iterations-1) * query_size}\")\n",
    "\n",
    "\n",
    "\n",
    "model, history = run_active_learning_with_visualization(\n",
    "    uncertainty_method='ensemble_regression',  \n",
    "    initial_size=initial_size,                       \n",
    "    query_size=query_size,                         \n",
    "    n_iterations=n_iterations,                         \n",
    "    epochs_per_iteration=epochs_per_iteration,  # Reduced from 1000              \n",
    "    learning_rate=0.01,       # Much smaller learning rate               \n",
    "    weight_decay=0.01,        # Add some regularization                    \n",
    "    hidden_size=64,                          \n",
    "    classification=False\n",
    ")\n",
    "\n",
    "best_iter, best_perf, plateau = analyze_stopping_points(history, 'regression')\n",
    "print(f\"Best iteration: {best_iter}, Best performance: {best_perf:.4f}, Plateau: {plateau}\")\n",
    "\n",
    "model, history = run_active_learning_with_visualization(\n",
    "    uncertainty_method='ensemble_regression',  \n",
    "    initial_size=initial_size,                       \n",
    "    query_size=query_size,                         \n",
    "    n_iterations=best_iter,                         \n",
    "    epochs_per_iteration=epochs_per_iteration,  # Reduced from 1000              \n",
    "    learning_rate=0.01,       # Much smaller learning rate               \n",
    "    weight_decay=0.01,        # Add some regularization                    \n",
    "    hidden_size=64,                          \n",
    "    classification=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19806c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "DATASET_CHOICE = 'complex'  \n",
    "# Get the selected dataset\n",
    "processed_data, label_encoders, target_column, dataset_name = select_regression_dataset(choice='complex', df_reg_simple=df_reg_simple, df_reg_med=df_reg_med, df_reg_complex=df_reg_complex)\n",
    "\n",
    "# Prepare features and target\n",
    "X = processed_data.drop(target_column, axis=1).values\n",
    "y = processed_data[target_column].values\n",
    "\n",
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initial split: 60% pool, 20% validation, 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_pool, X_val, y_train_pool, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "feature_scaler = MinMaxScaler()  # Scales to [0,1] range\n",
    "X_train_pool_scaled = feature_scaler.fit_transform(X_train_pool)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "# Scale targets to [0,1] range\n",
    "target_scaler = MinMaxScaler()\n",
    "y_train_pool_scaled = target_scaler.fit_transform(y_train_pool.reshape(-1, 1)).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(f\"Original target range: {y_train_pool.min():.2f} to {y_train_pool.max():.2f}\")\n",
    "print(f\"Scaled target range: {y_train_pool_scaled.min():.3f} to {y_train_pool_scaled.max():.3f}\")\n",
    "\n",
    "# Convert to PyTorch tensors - USE FLOAT TENSORS FOR REGRESSION!\n",
    "import torch\n",
    "X_train_pool_tensor = torch.FloatTensor(X_train_pool_scaled)\n",
    "y_train_pool_tensor = torch.FloatTensor(y_train_pool_scaled)  # Use scaled targets\n",
    "X_val_tensor = torch.FloatTensor(X_val_scaled)\n",
    "y_val_tensor = torch.FloatTensor(y_val_scaled)  # Use scaled targets\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.FloatTensor(y_test_scaled)  # Use scaled targets\n",
    "\n",
    "print(f\"Data shapes:\")\n",
    "print(f\"X_train_pool: {X_train_pool_tensor.shape}\")\n",
    "print(f\"y_train_pool: {y_train_pool_tensor.shape} (dtype: {y_train_pool_tensor.dtype})\")\n",
    "print(f\"Target values range: {y_train_pool_tensor.min():.3f} to {y_train_pool_tensor.max():.3f}\")\n",
    "\n",
    "# Store metadata for later use (including scalers for inverse transform)\n",
    "dataset_info = {\n",
    "    'name': dataset_name,\n",
    "    'choice': DATASET_CHOICE,\n",
    "    'target_column': target_column,\n",
    "    'features': X.shape[1],\n",
    "    'samples': X.shape[0],\n",
    "    'target_range': f\"{y.min():.2f} - {y.max():.2f}\",\n",
    "    'feature_scaler': feature_scaler,\n",
    "    'target_scaler': target_scaler\n",
    "}\n",
    "\n",
    "training_pool_size = len(X_train_pool_tensor)\n",
    "target_samples = int(0.8 * training_pool_size)  \n",
    "initial_size = int(0.05 * training_pool_size)  \n",
    "query_size = 5   \n",
    "n_iterations =  (target_samples - initial_size) // query_size + 1 \n",
    "epochs_per_iteration = 1500 // n_iterations\n",
    "print(f\"Training pool size: {training_pool_size}\")\n",
    "print(f\"Target samples (80%): {target_samples}\")\n",
    "print(f\"Initial size (5%): {initial_size}\")\n",
    "print(f\"Query size: {query_size}\")\n",
    "print(f\"Number of iterations: {n_iterations}\")\n",
    "print(f\"Final labeled size: {initial_size + (n_iterations-1) * query_size}\")\n",
    "\n",
    "\n",
    "\n",
    "model, history = run_active_learning_with_visualization(\n",
    "    uncertainty_method='ensemble_regression',  \n",
    "    initial_size=initial_size,                       \n",
    "    query_size=query_size,                         \n",
    "    n_iterations=n_iterations,                         \n",
    "    epochs_per_iteration=epochs_per_iteration,  \n",
    "    learning_rate=0.05,       \n",
    "    weight_decay=0.001,        \n",
    "    hidden_size=64,\n",
    "    classification=False\n",
    ")\n",
    "\n",
    "best_iter, best_perf, plateau = analyze_stopping_points(history, 'regression')\n",
    "print(f\"Best iteration: {best_iter}, Best performance: {best_perf:.4f}, Plateau: {plateau}\")\n",
    "\n",
    "\n",
    "model, history = run_active_learning_with_visualization(\n",
    "    uncertainty_method='ensemble_regression',  \n",
    "    initial_size=initial_size,                       \n",
    "    query_size=query_size,                         \n",
    "    n_iterations=best_iter,                         \n",
    "    epochs_per_iteration=epochs_per_iteration,  \n",
    "    learning_rate=0.05,       \n",
    "    weight_decay=0.001,        \n",
    "    hidden_size=64,\n",
    "    classification=False\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
