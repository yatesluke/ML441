{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0031926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the same datasets as in uncertainty.ipynb\n",
    "# Classification datasets (medical)\n",
    "df_class_simple = pd.read_csv('breast_cancer.csv')\n",
    "df_class_med = pd.read_csv('heart_disease.csv')\n",
    "df_class_complex = pd.read_csv('diabetes.csv')\n",
    "\n",
    "# Regression datasets\n",
    "df_reg_simple = pd.read_csv('housing.csv')\n",
    "df_reg_med = pd.read_csv('real_estate_valuation.csv')\n",
    "df_reg_complex = pd.read_csv('Housing.csv')\n",
    "\n",
    "print(\"Datasets loaded successfully!\")\n",
    "print(f\"Classification datasets: {df_class_simple.shape}, {df_class_med.shape}, {df_class_complex.shape}\")\n",
    "print(f\"Regression datasets: {df_reg_simple.shape}, {df_reg_med.shape}, {df_reg_complex.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffafa11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541bc503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size=1, task_type='classification'):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.task_type = task_type\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Hidden layer\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden_act = nn.LeakyReLU()\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # Final activations\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden_act(self.hidden(x))\n",
    "        x = self.output(x)\n",
    "        if self.task_type == 'classification':\n",
    "            x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            out = self.forward(x)\n",
    "            if self.output_size == 1:\n",
    "                probs = out\n",
    "                return torch.cat([1 - probs, probs], dim=1)\n",
    "            else:\n",
    "                return out\n",
    "    \n",
    "    def get_hidden_activation(self, x):\n",
    "        with torch.no_grad():\n",
    "            hidden_input = self.hidden(x)\n",
    "            hidden_output = self.hidden_act(hidden_input)\n",
    "            return hidden_output\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return {\n",
    "            'v_ji': self.hidden.weight.data.numpy(),  # input to hidden weights\n",
    "            'w_kj': self.output.weight.data.numpy(),  # hidden to output weights\n",
    "            'hidden_bias': self.hidden.bias.data.numpy(),\n",
    "            'output_bias': self.output.bias.data.numpy()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10b9172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, mean_squared_error, r2_score, confusion_matrix, classification_report\n",
    "import math\n",
    "\n",
    "class ActiveLearner:\n",
    "    def __init__(self, model, optimizer, criterion, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        n_batches = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.to(self.device)\n",
    "            batch_y = batch_y.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(batch_x)\n",
    "            \n",
    "            # Fix tensor shape issues - ensure proper dimensions\n",
    "            if outputs.dim() > 1:\n",
    "                outputs = outputs.squeeze(1)  # Remove only dimension 1, not all\n",
    "            if batch_y.dim() == 0:\n",
    "                batch_y = batch_y.unsqueeze(0)  # Add batch dimension if missing\n",
    "            \n",
    "            # Ensure both tensors have the same shape\n",
    "            outputs = outputs.view(-1)\n",
    "            batch_y = batch_y.view(-1).float()\n",
    "            \n",
    "            loss = self.criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        return total_loss / max(1, n_batches)\n",
    "\n",
    "    def evaluate_classification(self, val_loader):\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_true = []\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x = batch_x.to(self.device)\n",
    "                batch_y = batch_y.to(self.device)\n",
    "                outputs = self.model(batch_x)\n",
    "                \n",
    "                # Fix tensor shape issues\n",
    "                if outputs.dim() > 1:\n",
    "                    outputs = outputs.squeeze(1)\n",
    "                if batch_y.dim() == 0:\n",
    "                    batch_y = batch_y.unsqueeze(0)\n",
    "                \n",
    "                outputs = outputs.view(-1)\n",
    "                batch_y = batch_y.view(-1).float()\n",
    "                \n",
    "                val_loss += self.criterion(outputs, batch_y).item()\n",
    "                \n",
    "                # Convert predictions properly\n",
    "                preds = (outputs > 0.5).long().cpu().numpy()\n",
    "                all_preds.extend(preds)\n",
    "                all_true.extend(batch_y.cpu().numpy().astype(int))\n",
    "        \n",
    "        # Calculate metrics using sklearn\n",
    "        accuracy = accuracy_score(all_true, all_preds)\n",
    "        precision = precision_score(all_true, all_preds, average='weighted', zero_division=0)\n",
    "        recall = recall_score(all_true, all_preds, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(all_true, all_preds, average='weighted', zero_division=0)\n",
    "        conf_matrix = confusion_matrix(all_true, all_preds)\n",
    "        class_report = classification_report(all_true, all_preds)\n",
    "\n",
    "        avg_loss = val_loss / len(val_loader)\n",
    "        return accuracy, avg_loss, precision, recall, f1, conf_matrix, class_report\n",
    "    \n",
    "\n",
    "    def evaluate_regression(self, val_loader):\n",
    "        self.model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_true = []\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x = batch_x.to(self.device)\n",
    "                batch_y = batch_y.to(self.device)\n",
    "                outputs = self.model(batch_x)\n",
    "                \n",
    "                # Fix tensor shape issues for regression\n",
    "                if outputs.dim() > 1:\n",
    "                    outputs = outputs.squeeze(1)\n",
    "                if batch_y.dim() == 0:\n",
    "                    batch_y = batch_y.unsqueeze(0)\n",
    "                \n",
    "                outputs = outputs.view(-1)\n",
    "                batch_y = batch_y.view(-1).float()\n",
    "                \n",
    "                val_loss += self.criterion(outputs, batch_y).item()\n",
    "                all_preds.extend(outputs.cpu().numpy().tolist())\n",
    "                all_true.extend(batch_y.cpu().numpy().tolist())\n",
    "\n",
    "        avg_loss = val_loss / max(1, len(val_loader))\n",
    "        mse = mean_squared_error(all_true, all_preds)\n",
    "        rmse = math.sqrt(mse)\n",
    "        r2 = r2_score(all_true, all_preds)\n",
    "        return mse, rmse, r2, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b72087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Vectorized Sensitivity Analysis Implementation\n",
    "class SensitivityAnalysis:\n",
    "    @staticmethod\n",
    "    def compute_sensitivity_matrix_vectorized(network, X_batch):\n",
    "        network.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X_batch)\n",
    "            \n",
    "            # Get weights\n",
    "            weights = network.get_weights()\n",
    "            v_ji = weights['v_ji']  # [hidden_size, input_size]\n",
    "            w_kj = weights['w_kj']  # [output_size, hidden_size]\n",
    "            \n",
    "            # Forward pass to get activations\n",
    "            hidden_activations = network.get_hidden_activation(X_tensor).numpy()  # [batch_size, hidden_size]\n",
    "            outputs = network(X_tensor).numpy()  # [batch_size, output_size]\n",
    "            \n",
    "            batch_size = X_batch.shape[0]\n",
    "            input_size = X_batch.shape[1]\n",
    "            hidden_size = hidden_activations.shape[1]\n",
    "            output_size = outputs.shape[1] if outputs.ndim > 1 else 1\n",
    "            \n",
    "            if output_size == 1 and outputs.ndim == 1:\n",
    "                outputs = outputs.reshape(-1, 1)\n",
    "            \n",
    "            # Vectorized computation of sensitivity matrix\n",
    "            # Shape: [batch_size, output_size, input_size]\n",
    "            S_oz_batch = np.zeros((batch_size, output_size, input_size))\n",
    "            \n",
    "            for b in range(batch_size):\n",
    "                o_k = outputs[b]  # [output_size]\n",
    "                y_j = hidden_activations[b]  # [hidden_size]\n",
    "                \n",
    "                # Compute (1-y_j) * y_j for sigmoid derivative\n",
    "                sigmoid_deriv = (1 - y_j) * y_j  # [hidden_size]\n",
    "                \n",
    "                # Compute w_kj * sigmoid_deriv for each output\n",
    "                # w_kj: [output_size, hidden_size], sigmoid_deriv: [hidden_size]\n",
    "                weighted_deriv = w_kj * sigmoid_deriv[np.newaxis, :]  # [output_size, hidden_size]\n",
    "                \n",
    "                # Multiply by input weights v_ji\n",
    "                # weighted_deriv: [output_size, hidden_size], v_ji: [hidden_size, input_size]\n",
    "                inner_sum = np.dot(weighted_deriv, v_ji)  # [output_size, input_size]\n",
    "                \n",
    "                # Apply output sigmoid derivative\n",
    "                output_deriv = (1 - o_k) * o_k  # [output_size]\n",
    "                S_oz_batch[b] = output_deriv[:, np.newaxis] * inner_sum  # [output_size, input_size]\n",
    "            \n",
    "            return S_oz_batch\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_output_sensitivity_vector_vectorized(S_oz_batch, norm_type='euclidean'):\n",
    "        batch_size, output_size, input_size = S_oz_batch.shape\n",
    "        S_o_batch = np.zeros((batch_size, output_size))\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for k in range(output_size):\n",
    "                if norm_type == 'sum':\n",
    "                    S_o_batch[b, k] = np.sum(np.abs(S_oz_batch[b, k, :]))\n",
    "                elif norm_type == 'euclidean':\n",
    "                    S_o_batch[b, k] = np.sqrt(np.sum(S_oz_batch[b, k, :] ** 2))\n",
    "        \n",
    "        return S_o_batch\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_pattern_informativeness_vectorized(S_o_batch, norm_type='max'):\n",
    "        batch_size, output_size = S_o_batch.shape\n",
    "        informativeness_batch = np.zeros(batch_size)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            if norm_type == 'max':\n",
    "                informativeness_batch[b] = np.max(np.abs(S_o_batch[b]))\n",
    "            else:\n",
    "                informativeness_batch[b] = np.linalg.norm(S_o_batch[b])\n",
    "        \n",
    "        return informativeness_batch\n",
    "    \n",
    "    # Keep old methods for compatibility\n",
    "    @staticmethod\n",
    "    def compute_sensitivity_matrix(network, x):\n",
    "        if len(x.shape) == 1:\n",
    "            x = x.reshape(1, -1)\n",
    "        result = SensitivityAnalysis.compute_sensitivity_matrix_vectorized(network, x)\n",
    "        return result[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_output_sensitivity_vector(S_oz, norm_type='sum'):\n",
    "        if len(S_oz.shape) == 2:\n",
    "            S_oz = S_oz.reshape(1, S_oz.shape[0], S_oz.shape[1])\n",
    "        result = SensitivityAnalysis.compute_output_sensitivity_vector_vectorized(S_oz, norm_type)\n",
    "        return result[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_pattern_informativeness(S_o, norm_type='max'):\n",
    "        if len(S_o.shape) == 1:\n",
    "            S_o = S_o.reshape(1, -1)\n",
    "        result = SensitivityAnalysis.compute_pattern_informativeness_vectorized(S_o, norm_type)\n",
    "        return result[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98967de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_dataset(dataset, task_type=None, target_column=None, random_state=42):\n",
    "    dataset = dataset.copy()\n",
    "    dataset = dataset.dropna()\n",
    "    \n",
    "    # Auto-detect task type and target if not specified\n",
    "    if task_type is None or target_column is None:\n",
    "        # Classification targets\n",
    "        classification_targets = ['Default', 'loan_status', 'LoanApproved', 'Diagnosis', 'Outcome', 'num']\n",
    "        # Regression targets  \n",
    "        regression_targets = ['median_house_value', 'Y house price of unit area', 'price']\n",
    "        \n",
    "        for col in classification_targets:\n",
    "            if col in dataset.columns:\n",
    "                target_column = col\n",
    "                task_type = 'classification'\n",
    "                break\n",
    "        \n",
    "        if target_column is None:\n",
    "            for col in regression_targets:\n",
    "                if col in dataset.columns:\n",
    "                    target_column = col\n",
    "                    task_type = 'regression'\n",
    "                    break\n",
    "    \n",
    "    if target_column is None or target_column not in dataset.columns:\n",
    "        available_cols = list(dataset.columns)\n",
    "        raise ValueError(f\"No suitable target column found. Available columns: {available_cols}\")\n",
    "    \n",
    "    print(f\"Using target column: {target_column} for {task_type}\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = dataset.drop(columns=[target_column])\n",
    "    y = dataset[target_column]\n",
    "    \n",
    "    # Handle categorical and binary features in X\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "    \n",
    "    # Handle binary columns (columns with only 2 unique values)\n",
    "    for col in X.columns:\n",
    "        if X[col].nunique() == 2 and X[col].dtype in ['object', 'bool']:\n",
    "            le = LabelEncoder()\n",
    "            X[col] = le.fit_transform(X[col])\n",
    "    \n",
    "    # Convert to numpy arrays first\n",
    "    X = X.astype(np.float32).values\n",
    "    \n",
    "    # Handle target variable\n",
    "    if task_type == 'classification':\n",
    "        if y.dtype == 'object':\n",
    "            le = LabelEncoder()\n",
    "            y = le.fit_transform(y)\n",
    "        # Convert to binary if multi-class\n",
    "        if len(np.unique(y)) > 2:\n",
    "            # For multi-class, convert to binary (0 vs >0)\n",
    "            y = (y > 0).astype(int)\n",
    "        y = np.array(y, dtype=np.float32)\n",
    "        \n",
    "        # Ensure binary classification\n",
    "        print(f\"Target distribution: {np.unique(y, return_counts=True)}\")\n",
    "    else:  # regression\n",
    "        y = np.array(y, dtype=np.float32)\n",
    "        print(f\"Original target range: [{y.min():.2f}, {y.max():.2f}]\")\n",
    "        print(f\"Target mean: {y.mean():.2f}, std: {y.std():.2f}\")\n",
    "    \n",
    "    # Split into training and test sets\n",
    "    if task_type == 'classification':\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=random_state, stratify=y)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "    # Standardize features using StandardScaler (better for neural networks)\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    feature_scaler = StandardScaler()\n",
    "    X_train = feature_scaler.fit_transform(X_train)\n",
    "    X_test = feature_scaler.transform(X_test)\n",
    "    \n",
    "    # For regression, also scale the target values to prevent numerical issues\n",
    "    target_scaler = None\n",
    "    if task_type == 'regression':\n",
    "        target_scaler = StandardScaler()\n",
    "        y_train_original = y_train.copy()  # Keep original for reference\n",
    "        y_test_original = y_test.copy()\n",
    "        \n",
    "        # Scale targets to have mean 0 and std 1\n",
    "        y_train = target_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "        y_test = target_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        print(f\"Scaled target range: [{y_train.min():.3f}, {y_train.max():.3f}]\")\n",
    "        print(f\"Scaled target mean: {y_train.mean():.3f}, std: {y_train.std():.3f}\")\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}, Test samples: {len(X_test)}, Features: {X_train.shape[1]}\")\n",
    "    print(f\"Feature range after scaling: [{X_train.min():.3f}, {X_train.max():.3f}]\")\n",
    "    \n",
    "    # Return scalers for potential inverse transformation\n",
    "    if task_type == 'regression':\n",
    "        return X_train, y_train, X_test, y_test, feature_scaler, target_scaler\n",
    "    else:\n",
    "        return X_train, y_train, X_test, y_test, feature_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12540f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SASLA (Sensitivity Analysis-based Selective Learning Algorithm) Class\n",
    "class SASLA_class:\n",
    "    def __init__(self, input_size, hidden_size, output_size=1, learning_rate=0.01, \n",
    "                 alpha=0.9, task_type='classification', device='cpu', weight_decay=0.0001):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.alpha = alpha  # Selection parameter (higher = more selective)\n",
    "        self.task_type = task_type\n",
    "        self.device = device\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        # Initialize model\n",
    "        self.model = SimpleNN(input_size, hidden_size, output_size, task_type).to(device)\n",
    "        \n",
    "        # Initialize optimizer and loss function\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        if task_type == 'classification':\n",
    "            self.criterion = nn.BCELoss()\n",
    "        else:\n",
    "            self.criterion = nn.MSELoss()\n",
    "        \n",
    "        # Initialize active learner\n",
    "        self.active_learner = ActiveLearner(self.model, self.optimizer, self.criterion, device)\n",
    "        \n",
    "        # Initialize sensitivity analyzer\n",
    "        self.sensitivity_analyzer = SensitivityAnalysis()\n",
    "        \n",
    "        print(f\"SASLA initialized: {input_size}->{hidden_size}->{output_size}, α={alpha}, task={task_type}\")\n",
    "    \n",
    "    def select_informative_patterns_vectorized(self, X_candidate, y_candidate, batch_size=256):\n",
    "        P_C = len(X_candidate)\n",
    "        informativeness = np.zeros(P_C)\n",
    "        \n",
    "        # Process in batches for memory efficiency\n",
    "        for start_idx in range(0, P_C, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, P_C)\n",
    "            X_batch = X_candidate[start_idx:end_idx]\n",
    "            \n",
    "            # Vectorized computation for the batch\n",
    "            S_oz_batch = self.sensitivity_analyzer.compute_sensitivity_matrix_vectorized(self.model, X_batch)\n",
    "            S_o_batch = self.sensitivity_analyzer.compute_output_sensitivity_vector_vectorized(\n",
    "                S_oz_batch, norm_type='euclidean')\n",
    "            informativeness_batch = self.sensitivity_analyzer.compute_pattern_informativeness_vectorized(\n",
    "                S_o_batch, norm_type='max')\n",
    "            \n",
    "            informativeness[start_idx:end_idx] = informativeness_batch\n",
    "        \n",
    "        avg_informativeness = np.mean(informativeness)\n",
    "        threshold = (1 - self.alpha) * avg_informativeness\n",
    "        selected_indices = informativeness > threshold\n",
    "        \n",
    "        return selected_indices, informativeness, avg_informativeness\n",
    "    \n",
    "    def select_informative_patterns(self, X_candidate, y_candidate):\n",
    "        return self.select_informative_patterns_vectorized(X_candidate, y_candidate)\n",
    "    \n",
    "    def train(self, X_train, y_train, X_test, y_test, max_epochs=100, batch_size=32, verbose=False):\n",
    "        history = {\n",
    "            'epochs': [],\n",
    "            'training_error': [],\n",
    "            'validation_error': [],  # Added validation loss tracking\n",
    "            'selected_patterns': [],\n",
    "            'total_patterns': []\n",
    "        }\n",
    "        \n",
    "        for epoch in range(max_epochs):\n",
    "            if epoch == 0:\n",
    "                # Use full candidate set for first epoch\n",
    "                selected_mask = np.ones(len(X_train), dtype=bool)\n",
    "                avg_info = None\n",
    "            else:\n",
    "                # Select informative patterns using vectorized method\n",
    "                selected_mask, informativeness, avg_info = self.select_informative_patterns_vectorized(X_train, y_train)\n",
    "\n",
    "                    \n",
    "            # Get selected patterns\n",
    "            X_selected = X_train[selected_mask]\n",
    "            y_selected = y_train[selected_mask]\n",
    "            \n",
    "            if len(X_selected) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Create data loader for selected patterns\n",
    "            dataset = TensorDataset(torch.FloatTensor(X_selected), torch.FloatTensor(y_selected))\n",
    "            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "            # Train one epoch\n",
    "            train_loss = self.active_learner.train_epoch(dataloader)\n",
    "            \n",
    "            # Calculate validation loss\n",
    "            val_dataset = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))\n",
    "            val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            # Calculate validation loss\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in val_dataloader:\n",
    "                    batch_x = batch_x.to(self.device)\n",
    "                    batch_y = batch_y.to(self.device)\n",
    "                    outputs = self.model(batch_x)\n",
    "                    \n",
    "                    # Fix tensor shape issues\n",
    "                    if outputs.dim() > 1:\n",
    "                        outputs = outputs.squeeze(1)\n",
    "                    if batch_y.dim() == 0:\n",
    "                        batch_y = batch_y.unsqueeze(0)\n",
    "                    \n",
    "                    outputs = outputs.view(-1)\n",
    "                    batch_y = batch_y.view(-1).float()\n",
    "                    \n",
    "                    val_loss += self.criterion(outputs, batch_y).item()\n",
    "            \n",
    "            avg_val_loss = val_loss / len(val_dataloader)\n",
    "            \n",
    "            # Record history\n",
    "            history['epochs'].append(epoch + 1)\n",
    "            history['training_error'].append(train_loss)\n",
    "            history['validation_error'].append(avg_val_loss)  # Record validation loss\n",
    "            history['selected_patterns'].append(np.sum(selected_mask))\n",
    "            history['total_patterns'].append(len(X_train))\n",
    "            \n",
    "            if verbose and (epoch + 1) % 1 == 0:\n",
    "                selection_rate = np.sum(selected_mask) / len(X_train) * 100\n",
    "                print(f\"Epoch {epoch+1:3d}: Train Loss={train_loss:.6f}, Val Loss={avg_val_loss:.6f}, Selected={np.sum(selected_mask):3d}/{len(X_train)} ({selection_rate:.1f}%)\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def evaluate(self, X_test, y_test, batch_size=32):\n",
    "        dataset = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        if self.task_type == 'classification':\n",
    "            accuracy, loss, precision, recall, f1, conf_matrix, class_report = self.active_learner.evaluate_classification(dataloader)\n",
    "            return {\n",
    "                'accuracy': accuracy * 100,\n",
    "                'loss': loss,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1,\n",
    "                'confusion_matrix': conf_matrix,\n",
    "                'classification_report': class_report\n",
    "            }\n",
    "        else:\n",
    "            mse, rmse, r2, loss = self.active_learner.evaluate_regression(dataloader)\n",
    "            return {\n",
    "                'mse': mse,\n",
    "                'rmse': rmse,\n",
    "                'r2': r2,\n",
    "                'loss': loss\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6a3fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e53e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "result = load_and_prepare_dataset(dataset=df_class_simple, task_type='classification', random_state=42)\n",
    "X_train_class, y_train_class, X_test_class, y_test_class, feature_scaler = result\n",
    "\n",
    "\n",
    "input_size_class = X_train_class.shape[1]\n",
    "hidden_size_class = 32    # Adjust network size\n",
    "learning_rate_class = 0.001  # Adjust learning rate\n",
    "alpha_class = 0.9        # Adjust selectivity (0.9 = 10% selection, 0.95 = 5% selection)\n",
    "max_epochs_class = 1500     # Adjust training duration\n",
    "weight_decay_class = 0.01 # L2 regularization\n",
    "\n",
    "# Create and train SASLA model\n",
    "sasla_classifier = SASLA_class(\n",
    "    input_size=input_size_class,\n",
    "    hidden_size=hidden_size_class,\n",
    "    output_size=1,\n",
    "    learning_rate=learning_rate_class,\n",
    "    weight_decay=weight_decay_class,\n",
    "    alpha=alpha_class,\n",
    "    task_type='classification',\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "# Train\n",
    "history_class = sasla_classifier.train(\n",
    "    X_train_class, y_train_class, X_test_class, y_test_class,\n",
    "    max_epochs=max_epochs_class, verbose=True\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "metrics_class = sasla_classifier.evaluate(X_test_class, y_test_class)\n",
    "avg_selection_class = np.mean([s/t*100 for s, t in zip(history_class['selected_patterns'], history_class['total_patterns'])])\n",
    "\n",
    "# 1. Training and Validation Loss Over Time\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(history_class['epochs'], history_class['training_error'], 'b-', linewidth=2, label='Training Loss')\n",
    "plt.plot(history_class['epochs'], history_class['validation_error'], 'r-', linewidth=2, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (BCE)')\n",
    "plt.title('SASLA Classification - Training & Validation Loss (Diabetes)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 2. Pattern Selection Over Time\n",
    "plt.figure(figsize=(8, 6))\n",
    "selection_rates_class = [s/t*100 for s, t in zip(history_class['selected_patterns'], history_class['total_patterns'])]\n",
    "plt.plot(history_class['epochs'], selection_rates_class, 'g-', linewidth=2, label='Selection Rate')\n",
    "plt.axhline(y=avg_selection_class, color='g', linestyle='--', alpha=0.7, label=f'Average: {avg_selection_class:.1f}%')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Selection Rate (%)')\n",
    "plt.title('SASLA Classification - Pattern Selection (Diabetes)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Summary Statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SASLA CLASSIFICATION SUMMARY (BREAST CANCER)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {metrics_class['accuracy']:.2f}%\")\n",
    "print(f\"Precision: {metrics_class['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics_class['recall']:.4f}\")\n",
    "print(f\"F1-Score: {metrics_class['f1_score']:.4f}\")\n",
    "print(f\"Final Training Loss: {history_class['training_error'][-1]:.6f}\")\n",
    "print(f\"Final Validation Loss: {history_class['validation_error'][-1]:.6f}\")\n",
    "print(f\"Data Efficiency: Only used {avg_selection_class:.1f}% of training data\")\n",
    "print(f\"Data Savings: {100-avg_selection_class:.1f}% reduction in training data\")\n",
    "print(f\"Total Epochs: {max_epochs_class}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(metrics_class['confusion_matrix'])\n",
    "print(\"\\nClassification Report:\")\n",
    "print(metrics_class['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cd72c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = load_and_prepare_dataset(dataset=df_class_med, task_type='classification', random_state=42)\n",
    "X_train_class_med, y_train_class_med, X_test_class_med, y_test_class_med, feature_scaler_med = result\n",
    "\n",
    "# Model parameters - EASILY ADJUSTABLE\n",
    "input_size_class_med = X_train_class_med.shape[1]\n",
    "hidden_size_class_med = 64    # Adjust network size\n",
    "learning_rate_class_med = 0.001  # Adjust learning rate\n",
    "alpha_class_med = 0.9        # Adjust selectivity\n",
    "max_epochs_class_med = 1000     # Adjust training duration\n",
    "weight_decay_class_med = 0.01 # L2 regularization\n",
    "\n",
    "# Create and train SASLA model\n",
    "sasla_classifier_med = SASLA_class(\n",
    "    input_size=input_size_class_med,\n",
    "    hidden_size=hidden_size_class_med,\n",
    "    output_size=1,\n",
    "    learning_rate=learning_rate_class_med,\n",
    "    weight_decay=weight_decay_class_med,\n",
    "    alpha=alpha_class_med,\n",
    "    task_type='classification',\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "# Train\n",
    "history_class_med = sasla_classifier_med.train(\n",
    "    X_train_class_med, y_train_class_med, X_test_class_med, y_test_class_med,\n",
    "    max_epochs=max_epochs_class_med, verbose=True\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "metrics_class_med = sasla_classifier_med.evaluate(X_test_class_med, y_test_class_med)\n",
    "avg_selection_class_med = np.mean([s/t*100 for s, t in zip(history_class_med['selected_patterns'], history_class_med['total_patterns'])])\n",
    "\n",
    "# 1. Training and Validation Loss Over Time\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(history_class_med['epochs'], history_class_med['training_error'], 'b-', linewidth=2, label='Training Loss')\n",
    "plt.plot(history_class_med['epochs'], history_class_med['validation_error'], 'r-', linewidth=2, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (BCE)')\n",
    "plt.title('SASLA Classification - Training & Validation Loss (Diabetes)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 2. Pattern Selection Over Time\n",
    "plt.figure(figsize=(8, 6))\n",
    "selection_rates_class_med = [s/t*100 for s, t in zip(history_class_med['selected_patterns'], history_class_med['total_patterns'])]\n",
    "plt.plot(history_class_med['epochs'], selection_rates_class_med, 'g-', linewidth=2, label='Selection Rate')\n",
    "plt.axhline(y=avg_selection_class_med, color='g', linestyle='--', alpha=0.7, label=f'Average: {avg_selection_class_med:.1f}%')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Selection Rate (%)')\n",
    "plt.title('SASLA Classification - Pattern Selection (Diabetes)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Summary Statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SASLA CLASSIFICATION SUMMARY (HEART DISEASE)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {metrics_class_med['accuracy']:.2f}%\")\n",
    "print(f\"Precision: {metrics_class_med['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics_class_med['recall']:.4f}\")\n",
    "print(f\"F1-Score: {metrics_class_med['f1_score']:.4f}\")\n",
    "print(f\"Final Training Loss: {history_class_med['training_error'][-1]:.6f}\")\n",
    "print(f\"Final Validation Loss: {history_class_med['validation_error'][-1]:.6f}\")\n",
    "print(f\"Data Efficiency: Only used {avg_selection_class_med:.1f}% of training data\")\n",
    "print(f\"Data Savings: {100-avg_selection_class_med:.1f}% reduction in training data\")\n",
    "print(f\"Total Epochs: {max_epochs_class_med}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(metrics_class_med['confusion_matrix'])\n",
    "print(\"\\nClassification Report:\")\n",
    "print(metrics_class_med['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ac2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = load_and_prepare_dataset(dataset=df_class_complex, task_type='classification', random_state=42)\n",
    "X_train_class_complex, y_train_class_complex, X_test_class_complex, y_test_class_complex, feature_scaler_complex = result\n",
    "\n",
    "# Model parameters - EASILY ADJUSTABLE\n",
    "input_size_class_complex = X_train_class_complex.shape[1]\n",
    "hidden_size_class_complex = 128    # Adjust network size\n",
    "learning_rate_class_complex = 0.001  # Adjust learning rate\n",
    "alpha_class_complex = 0.9        # Adjust selectivity\n",
    "max_epochs_class_complex = 1000     # Adjust training duration (more epochs for complex dataset)\n",
    "weight_decay_class_complex = 0.01 # L2 regularization\n",
    "\n",
    "# Create and train SASLA model\n",
    "sasla_classifier_complex = SASLA_class(\n",
    "    input_size=input_size_class_complex,\n",
    "    hidden_size=hidden_size_class_complex,\n",
    "    output_size=1,\n",
    "    learning_rate=learning_rate_class_complex,\n",
    "    weight_decay=weight_decay_class_complex,\n",
    "    alpha=alpha_class_complex,\n",
    "    task_type='classification',\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "# Train\n",
    "history_class_complex = sasla_classifier_complex.train(\n",
    "    X_train_class_complex, y_train_class_complex, X_test_class_complex, y_test_class_complex,\n",
    "    max_epochs=max_epochs_class_complex, verbose=True\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "metrics_class_complex = sasla_classifier_complex.evaluate(X_test_class_complex, y_test_class_complex)\n",
    "avg_selection_class_complex = np.mean([s/t*100 for s, t in zip(history_class_complex['selected_patterns'], history_class_complex['total_patterns'])])\n",
    "\n",
    "# 1. Training and Validation Loss Over Time\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(history_class_complex['epochs'], history_class_complex['training_error'], 'b-', linewidth=2, label='Training Loss')\n",
    "plt.plot(history_class_complex['epochs'], history_class_complex['validation_error'], 'r-', linewidth=2, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (BCE)')\n",
    "plt.title('SASLA Classification - Training & Validation Loss (Diabetes)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 2. Pattern Selection Over Time\n",
    "plt.figure(figsize=(8, 6))\n",
    "selection_rates_class_complex = [s/t*100 for s, t in zip(history_class_complex['selected_patterns'], history_class_complex['total_patterns'])]\n",
    "plt.plot(history_class_complex['epochs'], selection_rates_class_complex, 'g-', linewidth=2, label='Selection Rate')\n",
    "plt.axhline(y=avg_selection_class_complex, color='g', linestyle='--', alpha=0.7, label=f'Average: {avg_selection_class_complex:.1f}%')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Selection Rate (%)')\n",
    "plt.title('SASLA Classification - Pattern Selection (Diabetes)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Summary Statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SASLA CLASSIFICATION SUMMARY (DIABETES)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {metrics_class_complex['accuracy']:.2f}%\")\n",
    "print(f\"Precision: {metrics_class_complex['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics_class_complex['recall']:.4f}\")\n",
    "print(f\"F1-Score: {metrics_class_complex['f1_score']:.4f}\")\n",
    "print(f\"Final Training Loss: {history_class_complex['training_error'][-1]:.6f}\")\n",
    "print(f\"Final Validation Loss: {history_class_complex['validation_error'][-1]:.6f}\")\n",
    "print(f\"Data Efficiency: Only used {avg_selection_class_complex:.1f}% of training data\")\n",
    "print(f\"Data Savings: {100-avg_selection_class_complex:.1f}% reduction in training data\")\n",
    "print(f\"Total Epochs: {max_epochs_class_complex}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(metrics_class_complex['confusion_matrix'])\n",
    "print(\"\\nClassification Report:\")\n",
    "print(metrics_class_complex['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9112d724",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = load_and_prepare_dataset(dataset=df_reg_simple, task_type='regression', random_state=42)\n",
    "if len(result) == 6:\n",
    "    X_train_reg, y_train_reg, X_test_reg, y_test_reg, feature_scaler, target_scaler = result\n",
    "else:\n",
    "    X_train_reg, y_train_reg, X_test_reg, y_test_reg, feature_scaler = result\n",
    "    target_scaler = None\n",
    "\n",
    "# Model parameters - EASILY ADJUSTABLE\n",
    "input_size_reg = X_train_reg.shape[1]\n",
    "hidden_size_reg = 64    # Adjust network size\n",
    "learning_rate_reg = 0.0001  # Adjust learning rate\n",
    "alpha_reg = 0.9        # Adjust selectivity (0.9 = 10% selection, 0.95 = 5% selection)\n",
    "max_epochs_reg = 1500     # Adjust training duration\n",
    "weight_decay_reg = 0.01 # L2 regularization\n",
    "\n",
    "# Create and train SASLA model\n",
    "sasla_regressor = SASLA_class(\n",
    "    input_size=input_size_reg,\n",
    "    hidden_size=hidden_size_reg,\n",
    "    output_size=1,\n",
    "    learning_rate=learning_rate_reg,\n",
    "    weight_decay=weight_decay_reg,\n",
    "    alpha=alpha_reg,\n",
    "    task_type='regression',\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "# Train\n",
    "history_reg = sasla_regressor.train(\n",
    "    X_train_reg, y_train_reg, X_test_reg, y_test_reg,\n",
    "    max_epochs=max_epochs_reg, verbose=True\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "metrics_reg = sasla_regressor.evaluate(X_test_reg, y_test_reg)\n",
    "avg_selection_reg = np.mean([s/t*100 for s, t in zip(history_reg['selected_patterns'], history_reg['total_patterns'])])\n",
    "\n",
    "# Individual Plot 1: Training and Validation Loss Over Time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history_reg['epochs'], history_reg['training_error'], 'b-', linewidth=2, label='Training Loss')\n",
    "plt.plot(history_reg['epochs'], history_reg['validation_error'], 'r-', linewidth=2, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('SASLA Regression - Training & Validation Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Individual Plot 2: Pattern Selection Over Time\n",
    "selection_rates_reg = [s/t*100 for s, t in zip(history_reg['selected_patterns'], history_reg['total_patterns'])]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history_reg['epochs'], selection_rates_reg, 'g-', linewidth=2, label='Selection Rate')\n",
    "plt.axhline(y=avg_selection_reg, color='g', linestyle='--', alpha=0.7, label=f'Average: {avg_selection_reg:.1f}%')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Selection Rate (%)')\n",
    "plt.title('SASLA Regression - Pattern Selection')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary Statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SASLA REGRESSION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"R² Score: {metrics_reg['r2']:.4f}\")\n",
    "print(f\"RMSE (scaled): {metrics_reg['rmse']:.4f}\")\n",
    "print(f\"MSE (scaled): {metrics_reg['mse']:.4f}\")\n",
    "print(f\"Final Training Loss: {history_reg['training_error'][-1]:.6f}\")\n",
    "print(f\"Final Validation Loss: {history_reg['validation_error'][-1]:.6f}\")\n",
    "print(f\"Data Efficiency: Only used {avg_selection_reg:.1f}% of training data\")\n",
    "print(f\"Data Savings: {100-avg_selection_reg:.1f}% reduction in training data\")\n",
    "print(f\"Total Epochs: {max_epochs_reg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc98a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = load_and_prepare_dataset(dataset=df_reg_med, task_type='regression', random_state=42)\n",
    "if len(result) == 6:\n",
    "    X_train_reg, y_train_reg, X_test_reg, y_test_reg, feature_scaler, target_scaler = result\n",
    "else:\n",
    "    X_train_reg, y_train_reg, X_test_reg, y_test_reg, feature_scaler = result\n",
    "    target_scaler = None\n",
    "\n",
    "# Model parameters - EASILY ADJUSTABLE\n",
    "input_size_reg = X_train_reg.shape[1]\n",
    "hidden_size_reg = 64    # Adjust network size\n",
    "learning_rate_reg = 0.0005  # Adjust learning rate\n",
    "alpha_reg = 0.9        # Adjust selectivity (0.9 = 10% selection, 0.95 = 5% selection)\n",
    "max_epochs_reg = 1500     # Adjust training duration\n",
    "weight_decay_reg = 0.01 # L2 regularization\n",
    "\n",
    "# Create and train SASLA model\n",
    "sasla_regressor = SASLA_class(\n",
    "    input_size=input_size_reg,\n",
    "    hidden_size=hidden_size_reg,\n",
    "    output_size=1,\n",
    "    learning_rate=learning_rate_reg,\n",
    "    weight_decay=weight_decay_reg,\n",
    "    alpha=alpha_reg,\n",
    "    task_type='regression',\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "# Train\n",
    "history_reg = sasla_regressor.train(\n",
    "    X_train_reg, y_train_reg, X_test_reg, y_test_reg,\n",
    "    max_epochs=max_epochs_reg, verbose=True\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "metrics_reg = sasla_regressor.evaluate(X_test_reg, y_test_reg)\n",
    "avg_selection_reg = np.mean([s/t*100 for s, t in zip(history_reg['selected_patterns'], history_reg['total_patterns'])])\n",
    "\n",
    "# Individual Plot 1: Training and Validation Loss Over Time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history_reg['epochs'], history_reg['training_error'], 'b-', linewidth=2, label='Training Loss')\n",
    "plt.plot(history_reg['epochs'], history_reg['validation_error'], 'r-', linewidth=2, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('SASLA Regression - Training & Validation Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Individual Plot 2: Pattern Selection Over Time\n",
    "selection_rates_reg = [s/t*100 for s, t in zip(history_reg['selected_patterns'], history_reg['total_patterns'])]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history_reg['epochs'], selection_rates_reg, 'g-', linewidth=2, label='Selection Rate')\n",
    "plt.axhline(y=avg_selection_reg, color='g', linestyle='--', alpha=0.7, label=f'Average: {avg_selection_reg:.1f}%')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Selection Rate (%)')\n",
    "plt.title('SASLA Regression - Pattern Selection')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary Statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SASLA REGRESSION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"R² Score: {metrics_reg['r2']:.4f}\")\n",
    "print(f\"RMSE (scaled): {metrics_reg['rmse']:.4f}\")\n",
    "print(f\"MSE (scaled): {metrics_reg['mse']:.4f}\")\n",
    "print(f\"Final Training Loss: {history_reg['training_error'][-1]:.6f}\")\n",
    "print(f\"Final Validation Loss: {history_reg['validation_error'][-1]:.6f}\")\n",
    "print(f\"Data Efficiency: Only used {avg_selection_reg:.1f}% of training data\")\n",
    "print(f\"Data Savings: {100-avg_selection_reg:.1f}% reduction in training data\")\n",
    "print(f\"Total Epochs: {max_epochs_reg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337d7860",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = load_and_prepare_dataset(dataset=df_reg_complex, task_type='regression', random_state=42)\n",
    "if len(result) == 6:\n",
    "    X_train_reg, y_train_reg, X_test_reg, y_test_reg, feature_scaler, target_scaler = result\n",
    "else:\n",
    "    X_train_reg, y_train_reg, X_test_reg, y_test_reg, feature_scaler = result\n",
    "    target_scaler = None\n",
    "\n",
    "# Model parameters - EASILY ADJUSTABLE\n",
    "input_size_reg = X_train_reg.shape[1]\n",
    "hidden_size_reg = 64    # Adjust network size\n",
    "learning_rate_reg = 0.001  # Adjust learning rate\n",
    "alpha_reg = 0.9        # Adjust selectivity (0.9 = 10% selection, 0.95 = 5% selection)\n",
    "max_epochs_reg = 1500     # Adjust training duration\n",
    "weight_decay_reg = 0.01 # L2 regularization\n",
    "\n",
    "# Create and train SASLA model\n",
    "sasla_regressor = SASLA_class(\n",
    "    input_size=input_size_reg,\n",
    "    hidden_size=hidden_size_reg,\n",
    "    output_size=1,\n",
    "    learning_rate=learning_rate_reg,\n",
    "    weight_decay=weight_decay_reg,\n",
    "    alpha=alpha_reg,\n",
    "    task_type='regression',\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "# Train\n",
    "history_reg = sasla_regressor.train(\n",
    "    X_train_reg, y_train_reg, X_test_reg, y_test_reg,\n",
    "    max_epochs=max_epochs_reg, verbose=True\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "metrics_reg = sasla_regressor.evaluate(X_test_reg, y_test_reg)\n",
    "avg_selection_reg = np.mean([s/t*100 for s, t in zip(history_reg['selected_patterns'], history_reg['total_patterns'])])\n",
    "\n",
    "# Individual Plot 1: Training and Validation Loss Over Time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history_reg['epochs'], history_reg['training_error'], 'b-', linewidth=2, label='Training Loss')\n",
    "plt.plot(history_reg['epochs'], history_reg['validation_error'], 'r-', linewidth=2, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('SASLA Regression - Training & Validation Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Individual Plot 2: Pattern Selection Over Time\n",
    "selection_rates_reg = [s/t*100 for s, t in zip(history_reg['selected_patterns'], history_reg['total_patterns'])]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history_reg['epochs'], selection_rates_reg, 'g-', linewidth=2, label='Selection Rate')\n",
    "plt.axhline(y=avg_selection_reg, color='g', linestyle='--', alpha=0.7, label=f'Average: {avg_selection_reg:.1f}%')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Selection Rate (%)')\n",
    "plt.title('SASLA Regression - Pattern Selection')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary Statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SASLA REGRESSION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"R² Score: {metrics_reg['r2']:.4f}\")\n",
    "print(f\"RMSE (scaled): {metrics_reg['rmse']:.4f}\")\n",
    "print(f\"MSE (scaled): {metrics_reg['mse']:.4f}\")\n",
    "print(f\"Final Training Loss: {history_reg['training_error'][-1]:.6f}\")\n",
    "print(f\"Final Validation Loss: {history_reg['validation_error'][-1]:.6f}\")\n",
    "print(f\"Data Efficiency: Only used {avg_selection_reg:.1f}% of training data\")\n",
    "print(f\"Data Savings: {100-avg_selection_reg:.1f}% reduction in training data\")\n",
    "print(f\"Total Epochs: {max_epochs_reg}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
