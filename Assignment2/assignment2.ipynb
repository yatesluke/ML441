{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25033fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <ins><a href=\"https://ydata.ai/register\">Upgrade to ydata-sdk</a></ins>\n",
       "                <p>\n",
       "                    Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
       "                </p>\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       count          mean            std           min           25%           50%           75%           max\n",
      "Elevation                           581012.0  3.270099e+06  309383.131347  2.054195e+06  3.103945e+06  3.310580e+06  3.495115e+06  4.263090e+06\n",
      "Aspect                              581012.0  1.556568e+02     111.913721  0.000000e+00  5.800000e+01  1.270000e+02  2.600000e+02  3.600000e+02\n",
      "Facet                               581012.0  3.899193e+02     280.343296  0.000000e+00  1.454909e+02  3.181596e+02  6.525290e+02  9.034134e+02\n",
      "Slope                               580714.0  1.410374e+01       7.488058  0.000000e+00  9.000000e+00  1.300000e+01  1.800000e+01  6.600000e+01\n",
      "Inclination                         581012.0 -4.844843e-04       0.577741 -9.999989e-01 -5.005386e-01 -1.061842e-03  5.009318e-01  9.999920e-01\n",
      "Horizontal_Distance_To_Hydrology    581012.0  5.062486e+03  952330.836884  0.000000e+00  1.080000e+02  2.180000e+02  3.840000e+02  3.742899e+08\n",
      "Vertical_Distance_To_Hydrology      581012.0  4.641886e+01      58.295232 -1.730000e+02  7.000000e+00  3.000000e+01  6.900000e+01  6.010000e+02\n",
      "Horizontal_Distance_To_Roadways     581012.0  2.350147e+03    1559.254870  0.000000e+00  1.106000e+03  1.997000e+03  3.328000e+03  7.117000e+03\n",
      "Hillshade_9am                       581012.0  2.121460e+02      26.769889  0.000000e+00  1.980000e+02  2.180000e+02  2.310000e+02  2.540000e+02\n",
      "Hillshade_Noon                      581012.0  2.233187e+02      19.768697  0.000000e+00  2.130000e+02  2.260000e+02  2.370000e+02  2.540000e+02\n",
      "Hillshade_3pm                       581012.0  1.425283e+02      38.274529  0.000000e+00  1.190000e+02  1.430000e+02  1.680000e+02  2.540000e+02\n",
      "Horizontal_Distance_To_Fire_Points  581012.0  1.980291e+03    1324.195210  0.000000e+00  1.024000e+03  1.710000e+03  2.550000e+03  7.173000e+03\n",
      "Wilderness_Area1                    581012.0  4.488651e-01       0.497379  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00  1.000000e+00\n",
      "Wilderness_Area2                    581012.0  5.143439e-02       0.220882  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Wilderness_Area3                    581012.0  4.360736e-01       0.495897  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00  1.000000e+00\n",
      "Wilderness_Area4                    581012.0  6.362691e-02       0.244087  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type1                          581012.0  5.216760e-03       0.072039  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type2                          581012.0  1.295154e-02       0.113066  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type3                          581012.0  8.301033e-03       0.090731  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type4                          581012.0  2.133519e-02       0.144499  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type5                          581012.0  2.748652e-03       0.052356  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type6                          581012.0  1.131646e-02       0.105775  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type7                          581012.0  1.807192e-04       0.013442  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type8                          581012.0  3.080831e-04       0.017550  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type9                          581012.0  1.974142e-03       0.044387  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type10                         581012.0  5.616751e-02       0.230245  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type11                         581012.0  2.135928e-02       0.144579  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type12                         581012.0  5.158413e-02       0.221186  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type13                         581012.0  3.000110e-02       0.170590  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type14                         581012.0  1.030960e-03       0.032092  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type15                         581012.0  5.163405e-06       0.002272  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type16                         581012.0  4.896629e-03       0.069804  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type17                         581012.0  5.889723e-03       0.076518  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type18                         581012.0  3.268435e-03       0.057077  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type19                         581012.0  6.920683e-03       0.082902  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type20                         581012.0  1.593599e-02       0.125228  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type21                         581012.0  1.442311e-03       0.037950  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type22                         581012.0  5.743943e-02       0.232681  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type23                         581012.0  9.939898e-02       0.299197  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type24                         581012.0  3.662231e-02       0.187833  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type25                         581012.0  8.158179e-04       0.028551  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type26                         581012.0  4.456018e-03       0.066605  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type27                         581012.0  1.869152e-03       0.043193  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type28                         581012.0  1.628194e-03       0.040318  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type29                         581012.0  1.983556e-01       0.398762  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type30                         581012.0  5.192664e-02       0.221879  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type31                         581012.0  4.417465e-02       0.205483  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type32                         581012.0  9.039228e-02       0.286743  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type33                         581012.0  7.771612e-02       0.267725  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type34                         581012.0  2.772748e-03       0.052584  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type35                         581012.0  3.254666e-03       0.056957  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type36                         581012.0  2.048150e-04       0.014310  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type37                         581012.0  5.128982e-04       0.022641  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type38                         581012.0  2.680323e-02       0.161508  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type39                         581012.0  2.376199e-02       0.152307  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Soil_Type40                         581012.0  1.505993e-02       0.121791  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00\n",
      "Water_Level                         581012.0  1.000000e+00       0.000000  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00\n",
      "Observation_ID                      581012.0  2.905075e+05  167723.861639  2.000000e+00  1.452548e+05  2.905075e+05  4.357602e+05  5.810130e+05\n",
      "Cover_Type                          581012.0  2.051471e+00       1.396504  1.000000e+00  1.000000e+00  2.000000e+00  2.000000e+00  7.000000e+00\n",
      "\n",
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 581012 entries, 0 to 581011\n",
      "Data columns (total 59 columns):\n",
      " #   Column                              Non-Null Count   Dtype  \n",
      "---  ------                              --------------   -----  \n",
      " 0   Elevation                           581012 non-null  int64  \n",
      " 1   Aspect                              581012 non-null  int64  \n",
      " 2   Facet                               581012 non-null  float64\n",
      " 3   Slope                               580714 non-null  float64\n",
      " 4   Inclination                         581012 non-null  float64\n",
      " 5   Horizontal_Distance_To_Hydrology    581012 non-null  int64  \n",
      " 6   Vertical_Distance_To_Hydrology      581012 non-null  int64  \n",
      " 7   Horizontal_Distance_To_Roadways     581012 non-null  int64  \n",
      " 8   Hillshade_9am                       581012 non-null  int64  \n",
      " 9   Hillshade_Noon                      581012 non-null  int64  \n",
      " 10  Hillshade_3pm                       581012 non-null  int64  \n",
      " 11  Horizontal_Distance_To_Fire_Points  581012 non-null  int64  \n",
      " 12  Wilderness_Area1                    581012 non-null  int64  \n",
      " 13  Wilderness_Area2                    581012 non-null  int64  \n",
      " 14  Wilderness_Area3                    581012 non-null  int64  \n",
      " 15  Wilderness_Area4                    581012 non-null  int64  \n",
      " 16  Soil_Type1                          581012 non-null  int64  \n",
      " 17  Soil_Type2                          581012 non-null  int64  \n",
      " 18  Soil_Type3                          581012 non-null  int64  \n",
      " 19  Soil_Type4                          581012 non-null  int64  \n",
      " 20  Soil_Type5                          581012 non-null  int64  \n",
      " 21  Soil_Type6                          581012 non-null  int64  \n",
      " 22  Soil_Type7                          581012 non-null  int64  \n",
      " 23  Soil_Type8                          581012 non-null  int64  \n",
      " 24  Soil_Type9                          581012 non-null  int64  \n",
      " 25  Soil_Type10                         581012 non-null  int64  \n",
      " 26  Soil_Type11                         581012 non-null  int64  \n",
      " 27  Soil_Type12                         581012 non-null  int64  \n",
      " 28  Soil_Type13                         581012 non-null  int64  \n",
      " 29  Soil_Type14                         581012 non-null  int64  \n",
      " 30  Soil_Type15                         581012 non-null  int64  \n",
      " 31  Soil_Type16                         581012 non-null  int64  \n",
      " 32  Soil_Type17                         581012 non-null  int64  \n",
      " 33  Soil_Type18                         581012 non-null  int64  \n",
      " 34  Soil_Type19                         581012 non-null  int64  \n",
      " 35  Soil_Type20                         581012 non-null  int64  \n",
      " 36  Soil_Type21                         581012 non-null  int64  \n",
      " 37  Soil_Type22                         581012 non-null  int64  \n",
      " 38  Soil_Type23                         581012 non-null  int64  \n",
      " 39  Soil_Type24                         581012 non-null  int64  \n",
      " 40  Soil_Type25                         581012 non-null  int64  \n",
      " 41  Soil_Type26                         581012 non-null  int64  \n",
      " 42  Soil_Type27                         581012 non-null  int64  \n",
      " 43  Soil_Type28                         581012 non-null  int64  \n",
      " 44  Soil_Type29                         581012 non-null  int64  \n",
      " 45  Soil_Type30                         581012 non-null  int64  \n",
      " 46  Soil_Type31                         581012 non-null  int64  \n",
      " 47  Soil_Type32                         581012 non-null  int64  \n",
      " 48  Soil_Type33                         581012 non-null  int64  \n",
      " 49  Soil_Type34                         581012 non-null  int64  \n",
      " 50  Soil_Type35                         581012 non-null  int64  \n",
      " 51  Soil_Type36                         581012 non-null  int64  \n",
      " 52  Soil_Type37                         581012 non-null  int64  \n",
      " 53  Soil_Type38                         581012 non-null  int64  \n",
      " 54  Soil_Type39                         581012 non-null  int64  \n",
      " 55  Soil_Type40                         581012 non-null  int64  \n",
      " 56  Water_Level                         581012 non-null  int64  \n",
      " 57  Observation_ID                      581012 non-null  int64  \n",
      " 58  Cover_Type                          581012 non-null  int64  \n",
      "dtypes: float64(3), int64(56)\n",
      "memory usage: 261.5 MB\n"
     ]
    }
   ],
   "source": [
    "import ydata_profiling as ydp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('forestCover.csv',na_values='?')\n",
    "\n",
    "\n",
    "df[['Water_Level', 'Observation_ID']] = df[['Observation_ID', 'Water_Level']]\n",
    "\n",
    "df['Soil_Type1'] = df['Soil_Type1'].str.lower().map({'positive': 0, 'negative': 1}).astype(int)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "desc = df.describe(include='all').transpose()\n",
    "print(desc.to_string())\n",
    "\n",
    "\n",
    "print(\"\\nDataFrame info:\")\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf7a37",
   "metadata": {},
   "source": [
    "### DATA QUALITY ISSUES ###\n",
    "\n",
    "- There remain features with missing values, with missing values indicated by the character symbol ‘?’.\n",
    "- The Facet feature is correlated with the Aspect feature.\n",
    "- The Inclination feature contains only noisy values.\n",
    "- There remain features with outliers.\n",
    "- There remain features with numeric ranges that differ significantly from one another.\n",
    "- There are numerical and categorical features.\n",
    "- Feature Water Level has cardinality of one.\n",
    "- Feature Observation ID has a unique value for each observation.\n",
    "- The class distribution remains skew."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4bf068",
   "metadata": {},
   "source": [
    "## KNN DATA PRE PROCESSING ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c45b53eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Inclination</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>Wilderness_Area1</th>\n",
       "      <th>Wilderness_Area2</th>\n",
       "      <th>Wilderness_Area3</th>\n",
       "      <th>Wilderness_Area4</th>\n",
       "      <th>Soil_Type1</th>\n",
       "      <th>Soil_Type2</th>\n",
       "      <th>Soil_Type3</th>\n",
       "      <th>Soil_Type4</th>\n",
       "      <th>Soil_Type5</th>\n",
       "      <th>Soil_Type6</th>\n",
       "      <th>Soil_Type7</th>\n",
       "      <th>Soil_Type8</th>\n",
       "      <th>Soil_Type9</th>\n",
       "      <th>Soil_Type10</th>\n",
       "      <th>Soil_Type11</th>\n",
       "      <th>Soil_Type12</th>\n",
       "      <th>Soil_Type13</th>\n",
       "      <th>Soil_Type14</th>\n",
       "      <th>Soil_Type15</th>\n",
       "      <th>Soil_Type16</th>\n",
       "      <th>Soil_Type17</th>\n",
       "      <th>Soil_Type18</th>\n",
       "      <th>Soil_Type19</th>\n",
       "      <th>Soil_Type20</th>\n",
       "      <th>Soil_Type21</th>\n",
       "      <th>Soil_Type22</th>\n",
       "      <th>Soil_Type23</th>\n",
       "      <th>Soil_Type24</th>\n",
       "      <th>Soil_Type25</th>\n",
       "      <th>Soil_Type26</th>\n",
       "      <th>Soil_Type27</th>\n",
       "      <th>Soil_Type28</th>\n",
       "      <th>Soil_Type29</th>\n",
       "      <th>Soil_Type30</th>\n",
       "      <th>Soil_Type31</th>\n",
       "      <th>Soil_Type32</th>\n",
       "      <th>Soil_Type33</th>\n",
       "      <th>Soil_Type34</th>\n",
       "      <th>Soil_Type35</th>\n",
       "      <th>Soil_Type36</th>\n",
       "      <th>Soil_Type37</th>\n",
       "      <th>Soil_Type38</th>\n",
       "      <th>Soil_Type39</th>\n",
       "      <th>Soil_Type40</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.810120e+05</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>5.810120e+05</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.270099e+06</td>\n",
       "      <td>155.656807</td>\n",
       "      <td>14.103172</td>\n",
       "      <td>-0.000484</td>\n",
       "      <td>5.062486e+03</td>\n",
       "      <td>46.418855</td>\n",
       "      <td>2350.146611</td>\n",
       "      <td>212.146049</td>\n",
       "      <td>223.318716</td>\n",
       "      <td>142.528263</td>\n",
       "      <td>1980.291226</td>\n",
       "      <td>0.448865</td>\n",
       "      <td>0.051434</td>\n",
       "      <td>0.436074</td>\n",
       "      <td>0.063627</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.012952</td>\n",
       "      <td>0.008301</td>\n",
       "      <td>0.021335</td>\n",
       "      <td>0.002749</td>\n",
       "      <td>0.011316</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.001974</td>\n",
       "      <td>0.056168</td>\n",
       "      <td>0.021359</td>\n",
       "      <td>0.051584</td>\n",
       "      <td>0.030001</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.004897</td>\n",
       "      <td>0.005890</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>0.006921</td>\n",
       "      <td>0.015936</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>0.057439</td>\n",
       "      <td>0.099399</td>\n",
       "      <td>0.036622</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.004456</td>\n",
       "      <td>0.001869</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.198356</td>\n",
       "      <td>0.051927</td>\n",
       "      <td>0.044175</td>\n",
       "      <td>0.090392</td>\n",
       "      <td>0.077716</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.026803</td>\n",
       "      <td>0.023762</td>\n",
       "      <td>0.015060</td>\n",
       "      <td>2.051471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.093831e+05</td>\n",
       "      <td>111.913721</td>\n",
       "      <td>7.486179</td>\n",
       "      <td>0.577741</td>\n",
       "      <td>9.523308e+05</td>\n",
       "      <td>58.295232</td>\n",
       "      <td>1559.254870</td>\n",
       "      <td>26.769889</td>\n",
       "      <td>19.768697</td>\n",
       "      <td>38.274529</td>\n",
       "      <td>1324.195210</td>\n",
       "      <td>0.497379</td>\n",
       "      <td>0.220882</td>\n",
       "      <td>0.495897</td>\n",
       "      <td>0.244087</td>\n",
       "      <td>0.072039</td>\n",
       "      <td>0.113066</td>\n",
       "      <td>0.090731</td>\n",
       "      <td>0.144499</td>\n",
       "      <td>0.052356</td>\n",
       "      <td>0.105775</td>\n",
       "      <td>0.013442</td>\n",
       "      <td>0.017550</td>\n",
       "      <td>0.044387</td>\n",
       "      <td>0.230245</td>\n",
       "      <td>0.144579</td>\n",
       "      <td>0.221186</td>\n",
       "      <td>0.170590</td>\n",
       "      <td>0.032092</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>0.069804</td>\n",
       "      <td>0.076518</td>\n",
       "      <td>0.057077</td>\n",
       "      <td>0.082902</td>\n",
       "      <td>0.125228</td>\n",
       "      <td>0.037950</td>\n",
       "      <td>0.232681</td>\n",
       "      <td>0.299197</td>\n",
       "      <td>0.187833</td>\n",
       "      <td>0.028551</td>\n",
       "      <td>0.066605</td>\n",
       "      <td>0.043193</td>\n",
       "      <td>0.040318</td>\n",
       "      <td>0.398762</td>\n",
       "      <td>0.221879</td>\n",
       "      <td>0.205483</td>\n",
       "      <td>0.286743</td>\n",
       "      <td>0.267725</td>\n",
       "      <td>0.052584</td>\n",
       "      <td>0.056957</td>\n",
       "      <td>0.014310</td>\n",
       "      <td>0.022641</td>\n",
       "      <td>0.161508</td>\n",
       "      <td>0.152307</td>\n",
       "      <td>0.121791</td>\n",
       "      <td>1.396504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.054195e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-173.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.103945e+06</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>-0.500539</td>\n",
       "      <td>1.080000e+02</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1106.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>213.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>1024.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.310580e+06</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>-0.001062</td>\n",
       "      <td>2.180000e+02</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1997.000000</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>1710.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.495115e+06</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.500932</td>\n",
       "      <td>3.840000e+02</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>3328.000000</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>2550.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.263090e+06</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>3.742899e+08</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>7117.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>7173.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Elevation         Aspect          Slope    Inclination  Horizontal_Distance_To_Hydrology  Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  Hillshade_9am  Hillshade_Noon  Hillshade_3pm  Horizontal_Distance_To_Fire_Points  Wilderness_Area1  Wilderness_Area2  Wilderness_Area3  Wilderness_Area4     Soil_Type1     Soil_Type2     Soil_Type3     Soil_Type4     Soil_Type5     Soil_Type6     Soil_Type7     Soil_Type8     Soil_Type9    Soil_Type10    Soil_Type11    Soil_Type12    Soil_Type13    Soil_Type14    Soil_Type15    Soil_Type16    Soil_Type17    Soil_Type18    Soil_Type19    Soil_Type20    Soil_Type21    Soil_Type22    Soil_Type23    Soil_Type24    Soil_Type25    Soil_Type26    Soil_Type27    Soil_Type28    Soil_Type29    Soil_Type30    Soil_Type31    Soil_Type32    Soil_Type33    Soil_Type34    Soil_Type35    Soil_Type36    Soil_Type37    Soil_Type38    Soil_Type39    Soil_Type40     Cover_Type\n",
       "count  5.810120e+05  581012.000000  581012.000000  581012.000000                      5.810120e+05                   581012.000000                    581012.000000  581012.000000   581012.000000  581012.000000                       581012.000000     581012.000000     581012.000000     581012.000000     581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000  581012.000000\n",
       "mean   3.270099e+06     155.656807      14.103172      -0.000484                      5.062486e+03                       46.418855                      2350.146611     212.146049      223.318716     142.528263                         1980.291226          0.448865          0.051434          0.436074          0.063627       0.005217       0.012952       0.008301       0.021335       0.002749       0.011316       0.000181       0.000308       0.001974       0.056168       0.021359       0.051584       0.030001       0.001031       0.000005       0.004897       0.005890       0.003268       0.006921       0.015936       0.001442       0.057439       0.099399       0.036622       0.000816       0.004456       0.001869       0.001628       0.198356       0.051927       0.044175       0.090392       0.077716       0.002773       0.003255       0.000205       0.000513       0.026803       0.023762       0.015060       2.051471\n",
       "std    3.093831e+05     111.913721       7.486179       0.577741                      9.523308e+05                       58.295232                      1559.254870      26.769889       19.768697      38.274529                         1324.195210          0.497379          0.220882          0.495897          0.244087       0.072039       0.113066       0.090731       0.144499       0.052356       0.105775       0.013442       0.017550       0.044387       0.230245       0.144579       0.221186       0.170590       0.032092       0.002272       0.069804       0.076518       0.057077       0.082902       0.125228       0.037950       0.232681       0.299197       0.187833       0.028551       0.066605       0.043193       0.040318       0.398762       0.221879       0.205483       0.286743       0.267725       0.052584       0.056957       0.014310       0.022641       0.161508       0.152307       0.121791       1.396504\n",
       "min    2.054195e+06       0.000000       0.000000      -0.999999                      0.000000e+00                     -173.000000                         0.000000       0.000000        0.000000       0.000000                            0.000000          0.000000          0.000000          0.000000          0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       1.000000\n",
       "25%    3.103945e+06      58.000000       9.000000      -0.500539                      1.080000e+02                        7.000000                      1106.000000     198.000000      213.000000     119.000000                         1024.000000          0.000000          0.000000          0.000000          0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       1.000000\n",
       "50%    3.310580e+06     127.000000      13.000000      -0.001062                      2.180000e+02                       30.000000                      1997.000000     218.000000      226.000000     143.000000                         1710.000000          0.000000          0.000000          0.000000          0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       2.000000\n",
       "75%    3.495115e+06     260.000000      18.000000       0.500932                      3.840000e+02                       69.000000                      3328.000000     231.000000      237.000000     168.000000                         2550.000000          1.000000          0.000000          1.000000          0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       2.000000\n",
       "max    4.263090e+06     360.000000      66.000000       0.999992                      3.742899e+08                      601.000000                      7117.000000     254.000000      254.000000     254.000000                         7173.000000          1.000000          1.000000          1.000000          1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       1.000000       7.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "df_KNN = df.copy()\n",
    "\n",
    "#Removing Facet\n",
    "df_KNN = df_KNN.drop(columns=['Facet'])\n",
    "#Removing Water_Level\n",
    "df_KNN = df_KNN.drop(columns=['Water_Level'])\n",
    "#Removing Observation_ID\n",
    "df_KNN = df_KNN.drop(columns=['Observation_ID'])\n",
    "\n",
    "#Noise, outliers, are all dependent on K\n",
    "#Got to deal with skew class distributions\n",
    "\n",
    "#Impute Missing Values\n",
    "df_KNN['Slope'] = df_KNN['Slope'].fillna(df_KNN['Slope'].median())\n",
    "\n",
    "df_KNN.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b404c29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution (counts and %):\n",
      "             count  percent\n",
      "Cover_Type                 \n",
      "1           211840  36.4605\n",
      "2           283301  48.7599\n",
      "3            35754   6.1537\n",
      "4             2747   0.4728\n",
      "5             9493   1.6339\n",
      "6            17367   2.9891\n",
      "7            20510   3.5300\n",
      "\n",
      "Total samples: 581012, majority/minority ratio: 283301/2747 = 103.13\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show class imbalance for Cover_Type in df_KNN\n",
    "counts = df_KNN['Cover_Type'].value_counts().sort_index()\n",
    "pct = counts / counts.sum() * 100\n",
    "imbalance_df = pd.DataFrame({'count': counts, 'percent': pct.round(4)})\n",
    "print(\"Class distribution (counts and %):\")\n",
    "print(imbalance_df)\n",
    "\n",
    "maj = counts.max()\n",
    "minc = counts.min()\n",
    "print(f\"\\nTotal samples: {counts.sum()}, majority/minority ratio: {maj}/{minc} = {maj/minc:.2f}\")\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "bars = ax.bar(counts.index.astype(str), counts.values, align='center')\n",
    "ax.set_xlabel('Cover_Type')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Cover_Type class distribution')\n",
    "\n",
    "ymax = counts.max() * 1.12\n",
    "ax.set_ylim(0, ymax)\n",
    "\n",
    "for p in ax.patches:\n",
    "    h = p.get_height()\n",
    "    ax.annotate(f\"{int(h)}\\n{h/counts.sum()*100:.1f}%\", \n",
    "                (p.get_x() + p.get_width() / 2, h + ymax * 0.01),\n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcef5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset: 581012 samples\n",
      "Training set: 406708 samples (70.0%)\n",
      "Test set: 174304 samples (30.0%)\n",
      "\n",
      "Training set class distribution:\n",
      "Cover_Type\n",
      "1    148288\n",
      "2    198310\n",
      "3     25028\n",
      "4      1923\n",
      "5      6645\n",
      "6     12157\n",
      "7     14357\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set class distribution:\n",
      "Cover_Type\n",
      "1    63552\n",
      "2    84991\n",
      "3    10726\n",
      "4      824\n",
      "5     2848\n",
      "6     5210\n",
      "7     6153\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test sets (80/20 split)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_KNN.drop(columns=['Cover_Type'])\n",
    "y = df_KNN['Cover_Type']\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Full dataset: {X.shape[0]} samples\")\n",
    "print(f\"Training set: {X_train_full.shape[0]} samples ({X_train_full.shape[0]/X.shape[0]*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/X.shape[0]*100:.1f}%)\")\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(y_train_full.value_counts().sort_index())\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print(y_test.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c69662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 -- Acc: 0.8633, F1(macro): 0.7814 -- saved to tests/knn/\n",
      "Fold 2 -- Acc: 0.8657, F1(macro): 0.7861 -- saved to tests/knn/\n",
      "Fold 2 -- Acc: 0.8657, F1(macro): 0.7861 -- saved to tests/knn/\n",
      "Fold 3 -- Acc: 0.8640, F1(macro): 0.7745 -- saved to tests/knn/\n",
      "Fold 3 -- Acc: 0.8640, F1(macro): 0.7745 -- saved to tests/knn/\n",
      "Fold 4 -- Acc: 0.8629, F1(macro): 0.7716 -- saved to tests/knn/\n",
      "Fold 4 -- Acc: 0.8629, F1(macro): 0.7716 -- saved to tests/knn/\n",
      "Fold 5 -- Acc: 0.8653, F1(macro): 0.7818 -- saved to tests/knn/\n",
      "Fold 5 -- Acc: 0.8653, F1(macro): 0.7818 -- saved to tests/knn/\n",
      "K-Fold (5) completed. Saved per-fold results and reports to 'tests/knn/'.\n",
      "Cross-validation metrics:\n",
      "      fold  accuracy  f1_macro\n",
      "0      1.0  0.863306  0.781408\n",
      "1      2.0  0.865654  0.786053\n",
      "2      3.0  0.863957  0.774465\n",
      "3      4.0  0.862886  0.771625\n",
      "4      5.0  0.865345  0.781794\n",
      "mean   3.0  0.864229  0.779069\n",
      "K-Fold (5) completed. Saved per-fold results and reports to 'tests/knn/'.\n",
      "Cross-validation metrics:\n",
      "      fold  accuracy  f1_macro\n",
      "0      1.0  0.863306  0.781408\n",
      "1      2.0  0.865654  0.786053\n",
      "2      3.0  0.863957  0.774465\n",
      "3      4.0  0.862886  0.771625\n",
      "4      5.0  0.865345  0.781794\n",
      "mean   3.0  0.864229  0.779069\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os\n",
    "\n",
    "os.makedirs('tests/knn', exist_ok=True)\n",
    "\n",
    "X_train = X_train_full\n",
    "y_train = y_train_full\n",
    "\n",
    "n_splits = 5\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "results_list = []\n",
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train), start=1):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_fold_train_scaled = scaler.fit_transform(X_fold_train)\n",
    "    X_fold_val_scaled = scaler.transform(X_fold_val)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "    knn.fit(X_fold_train_scaled, y_fold_train)\n",
    "    y_pred = knn.predict(X_fold_val_scaled)\n",
    "\n",
    "    acc = accuracy_score(y_fold_val, y_pred)\n",
    "    f1 = f1_score(y_fold_val, y_pred, average='macro')\n",
    "\n",
    "    metrics_list.append({'fold': fold, 'accuracy': acc, 'f1_macro': f1})\n",
    "\n",
    "    df_fold = X_fold_val.copy()\n",
    "    df_fold['y_true'] = y_fold_val\n",
    "    df_fold['y_pred'] = y_pred\n",
    "\n",
    "    if hasattr(knn, \"predict_proba\"):\n",
    "        probs = knn.predict_proba(X_fold_val_scaled)\n",
    "        for i, cls in enumerate(knn.classes_):\n",
    "            df_fold[f'prob_{cls}'] = probs[:, i]\n",
    "\n",
    "    fold_results_path = f'tests/knn/knn_baseline_results_fold_{fold}.csv'\n",
    "    df_fold.to_csv(fold_results_path, index=True)\n",
    "\n",
    "    creport = classification_report(y_fold_val, y_pred)\n",
    "    with open(f'tests/knn/knn_baseline_classification_report_fold_{fold}.txt', 'w') as f:\n",
    "        f.write(creport)\n",
    "    cm = confusion_matrix(y_fold_val, y_pred)\n",
    "    pd.DataFrame(cm).to_csv(f'tests/knn/knn_baseline_confusion_matrix_fold_{fold}.csv', index=False)\n",
    "\n",
    "    results_list.append(df_fold)\n",
    "    print(f\"Fold {fold} -- Acc: {acc:.4f}, F1(macro): {f1:.4f} -- saved to tests/knn/\")\n",
    "\n",
    "all_results = pd.concat(results_list)\n",
    "all_results.to_csv('tests/knn/knn_baseline_results_all_folds.csv', index=True)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "metrics_df.loc['mean'] = metrics_df.mean(numeric_only=True)\n",
    "metrics_df.to_csv('tests/knn/knn_baseline_cv_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab327a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: After SMOTE: X_res.shape = (1110536, 55), y_res distribution:\n",
      "Cover_Type\n",
      "1    158648\n",
      "2    158648\n",
      "3    158648\n",
      "4    158648\n",
      "6    158648\n",
      "7    158648\n",
      "5    158648\n",
      "Name: count, dtype: int64\n",
      "Fold 1 -- Acc: 0.8423, F1(macro): 0.7518 -- saved to tests/knn/\n",
      "Fold 1 -- Acc: 0.8423, F1(macro): 0.7518 -- saved to tests/knn/\n",
      "Fold 2: After SMOTE: X_res.shape = (1110536, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    158648\n",
      "1    158648\n",
      "3    158648\n",
      "4    158648\n",
      "6    158648\n",
      "7    158648\n",
      "5    158648\n",
      "Name: count, dtype: int64\n",
      "Fold 2: After SMOTE: X_res.shape = (1110536, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    158648\n",
      "1    158648\n",
      "3    158648\n",
      "4    158648\n",
      "6    158648\n",
      "7    158648\n",
      "5    158648\n",
      "Name: count, dtype: int64\n",
      "Fold 2 -- Acc: 0.8448, F1(macro): 0.7616 -- saved to tests/knn/\n",
      "Fold 2 -- Acc: 0.8448, F1(macro): 0.7616 -- saved to tests/knn/\n",
      "Fold 3: After SMOTE: X_res.shape = (1110536, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    158648\n",
      "1    158648\n",
      "3    158648\n",
      "6    158648\n",
      "7    158648\n",
      "5    158648\n",
      "4    158648\n",
      "Name: count, dtype: int64\n",
      "Fold 3: After SMOTE: X_res.shape = (1110536, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    158648\n",
      "1    158648\n",
      "3    158648\n",
      "6    158648\n",
      "7    158648\n",
      "5    158648\n",
      "4    158648\n",
      "Name: count, dtype: int64\n",
      "Fold 3 -- Acc: 0.8435, F1(macro): 0.7558 -- saved to tests/knn/\n",
      "Fold 3 -- Acc: 0.8435, F1(macro): 0.7558 -- saved to tests/knn/\n",
      "Fold 4: After SMOTE: X_res.shape = (1110536, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    158648\n",
      "1    158648\n",
      "3    158648\n",
      "4    158648\n",
      "6    158648\n",
      "7    158648\n",
      "5    158648\n",
      "Name: count, dtype: int64\n",
      "Fold 4: After SMOTE: X_res.shape = (1110536, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    158648\n",
      "1    158648\n",
      "3    158648\n",
      "4    158648\n",
      "6    158648\n",
      "7    158648\n",
      "5    158648\n",
      "Name: count, dtype: int64\n",
      "Fold 4 -- Acc: 0.8420, F1(macro): 0.7488 -- saved to tests/knn/\n",
      "Fold 4 -- Acc: 0.8420, F1(macro): 0.7488 -- saved to tests/knn/\n",
      "Fold 5: After SMOTE: X_res.shape = (1110536, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    158648\n",
      "1    158648\n",
      "3    158648\n",
      "4    158648\n",
      "6    158648\n",
      "7    158648\n",
      "5    158648\n",
      "Name: count, dtype: int64\n",
      "Fold 5: After SMOTE: X_res.shape = (1110536, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    158648\n",
      "1    158648\n",
      "3    158648\n",
      "4    158648\n",
      "6    158648\n",
      "7    158648\n",
      "5    158648\n",
      "Name: count, dtype: int64\n",
      "Fold 5 -- Acc: 0.8450, F1(macro): 0.7616 -- saved to tests/knn/\n",
      "Fold 5 -- Acc: 0.8450, F1(macro): 0.7616 -- saved to tests/knn/\n",
      "K-Fold (5) with SMOTE completed. Saved per-fold results and reports to 'tests/knn/'.\n",
      "Cross-validation metrics:\n",
      "      fold  accuracy  f1_macro\n",
      "0      1.0  0.842295  0.751794\n",
      "1      2.0  0.844803  0.761643\n",
      "2      3.0  0.843525  0.755758\n",
      "3      4.0  0.842035  0.748795\n",
      "4      5.0  0.845035  0.761595\n",
      "mean   3.0  0.843539  0.755917\n",
      "K-Fold (5) with SMOTE completed. Saved per-fold results and reports to 'tests/knn/'.\n",
      "Cross-validation metrics:\n",
      "      fold  accuracy  f1_macro\n",
      "0      1.0  0.842295  0.751794\n",
      "1      2.0  0.844803  0.761643\n",
      "2      3.0  0.843525  0.755758\n",
      "3      4.0  0.842035  0.748795\n",
      "4      5.0  0.845035  0.761595\n",
      "mean   3.0  0.843539  0.755917\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os\n",
    "\n",
    "# 5-fold CV KNN with SMOTE applied to each training fold; save outputs to tests/knn/\n",
    "os.makedirs('tests/knn', exist_ok=True)\n",
    "\n",
    "X_train = X_train_full\n",
    "y_train = y_train_full\n",
    "\n",
    "n_splits = 5\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "results_list = []\n",
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train), start=1):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_fold_train_scaled = scaler.fit_transform(X_fold_train)\n",
    "    X_fold_val_scaled = scaler.transform(X_fold_val)\n",
    "\n",
    "    smt = SMOTE(random_state=42)\n",
    "    X_res, y_res = smt.fit_resample(X_fold_train_scaled, y_fold_train)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "    knn.fit(X_res, y_res)\n",
    "    y_pred = knn.predict(X_fold_val_scaled)\n",
    "\n",
    "    acc = accuracy_score(y_fold_val, y_pred)\n",
    "    f1 = f1_score(y_fold_val, y_pred, average='macro')\n",
    "\n",
    "    metrics_list.append({'fold': fold, 'accuracy': acc, 'f1_macro': f1})\n",
    "\n",
    "    df_fold = X_fold_val.copy()\n",
    "    df_fold['y_true'] = y_fold_val\n",
    "    df_fold['y_pred'] = y_pred\n",
    "\n",
    "    if hasattr(knn, \"predict_proba\"):\n",
    "        probs = knn.predict_proba(X_fold_val_scaled)\n",
    "        for i, cls in enumerate(knn.classes_):\n",
    "            df_fold[f'prob_{cls}'] = probs[:, i]\n",
    "\n",
    "    fold_results_path = f'tests/knn/knn_smote_results_fold_{fold}.csv'\n",
    "    df_fold.to_csv(fold_results_path, index=True)\n",
    "\n",
    "    creport = classification_report(y_fold_val, y_pred)\n",
    "    with open(f'tests/knn/knn_smote_classification_report_fold_{fold}.txt', 'w') as f:\n",
    "        f.write(creport)\n",
    "    cm = confusion_matrix(y_fold_val, y_pred)\n",
    "    pd.DataFrame(cm).to_csv(f'tests/knn/knn_smote_confusion_matrix_fold_{fold}.csv', index=False)\n",
    "\n",
    "    results_list.append(df_fold)\n",
    "    print(f\"Fold {fold} -- Acc: {acc:.4f}, F1(macro): {f1:.4f} -- saved to tests/knn/\")\n",
    "\n",
    "all_results = pd.concat(results_list)\n",
    "all_results.to_csv('tests/knn/knn_smote_results_all_folds.csv', index=True)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "metrics_df.loc['mean'] = metrics_df.mean(numeric_only=True)\n",
    "metrics_df.to_csv('tests/knn/knn_smote_cv_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac159ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: After Tomek: X_res.shape = (306946, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    150976\n",
      "1    111857\n",
      "3     18652\n",
      "7     10937\n",
      "6      8505\n",
      "5      4480\n",
      "4      1539\n",
      "Name: count, dtype: int64\n",
      "Fold 2: After Tomek: X_res.shape = (306760, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    150938\n",
      "1    111792\n",
      "3     18610\n",
      "7     10954\n",
      "6      8438\n",
      "5      4490\n",
      "4      1538\n",
      "Name: count, dtype: int64\n",
      "Fold 2: After Tomek: X_res.shape = (306760, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    150938\n",
      "1    111792\n",
      "3     18610\n",
      "7     10954\n",
      "6      8438\n",
      "5      4490\n",
      "4      1538\n",
      "Name: count, dtype: int64\n",
      "Fold 3: After Tomek: X_res.shape = (306752, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    150928\n",
      "1    111798\n",
      "3     18589\n",
      "7     10953\n",
      "6      8438\n",
      "5      4508\n",
      "4      1538\n",
      "Name: count, dtype: int64\n",
      "Fold 3: After Tomek: X_res.shape = (306752, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    150928\n",
      "1    111798\n",
      "3     18589\n",
      "7     10953\n",
      "6      8438\n",
      "5      4508\n",
      "4      1538\n",
      "Name: count, dtype: int64\n",
      "Fold 4: After Tomek: X_res.shape = (306700, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    150909\n",
      "1    111800\n",
      "3     18596\n",
      "7     10945\n",
      "6      8402\n",
      "5      4510\n",
      "4      1538\n",
      "Name: count, dtype: int64\n",
      "Fold 4: After Tomek: X_res.shape = (306700, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    150909\n",
      "1    111800\n",
      "3     18596\n",
      "7     10945\n",
      "6      8402\n",
      "5      4510\n",
      "4      1538\n",
      "Name: count, dtype: int64\n",
      "Fold 5: After Tomek: X_res.shape = (306684, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    150854\n",
      "1    111714\n",
      "3     18641\n",
      "7     10941\n",
      "6      8512\n",
      "5      4483\n",
      "4      1539\n",
      "Name: count, dtype: int64\n",
      "Fold 5: After Tomek: X_res.shape = (306684, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    150854\n",
      "1    111714\n",
      "3     18641\n",
      "7     10941\n",
      "6      8512\n",
      "5      4483\n",
      "4      1539\n",
      "Name: count, dtype: int64\n",
      "K-Fold (5) with Tomek completed. Saved per-fold results and reports to 'tests/knn'.\n",
      "Cross-validation metrics:\n",
      "      fold  accuracy  f1_macro\n",
      "0      1.0  0.861757  0.777198\n",
      "1      2.0  0.864596  0.782913\n",
      "2      3.0  0.863047  0.771055\n",
      "3      4.0  0.861841  0.768217\n",
      "4      5.0  0.863992  0.779878\n",
      "mean   3.0  0.863047  0.775852\n",
      "K-Fold (5) with Tomek completed. Saved per-fold results and reports to 'tests/knn'.\n",
      "Cross-validation metrics:\n",
      "      fold  accuracy  f1_macro\n",
      "0      1.0  0.861757  0.777198\n",
      "1      2.0  0.864596  0.782913\n",
      "2      3.0  0.863047  0.771055\n",
      "3      4.0  0.861841  0.768217\n",
      "4      5.0  0.863992  0.779878\n",
      "mean   3.0  0.863047  0.775852\n"
     ]
    }
   ],
   "source": [
    "# K-Fold cross-validated KNN with Tomek Links applied to the training set; save outputs to folder 'tests'\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.under_sampling import TomekLinks as Tomek\n",
    "\n",
    "os.makedirs('tests/knn', exist_ok=True)\n",
    "\n",
    "X_train = X_train_full\n",
    "y_train = y_train_full\n",
    "\n",
    "n_splits = 5\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "results_list = []\n",
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train), start=1):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_fold_train_scaled = scaler.fit_transform(X_fold_train)\n",
    "    X_fold_val_scaled = scaler.transform(X_fold_val)\n",
    "\n",
    "    tomek = Tomek()\n",
    "    X_res, y_res = tomek.fit_resample(X_fold_train_scaled, y_fold_train)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "    knn.fit(X_res, y_res)\n",
    "    y_pred = knn.predict(X_fold_val_scaled)\n",
    "\n",
    "    acc = accuracy_score(y_fold_val, y_pred)\n",
    "    f1 = f1_score(y_fold_val, y_pred, average='macro')\n",
    "\n",
    "    metrics_list.append({'fold': fold, 'accuracy': acc, 'f1_macro': f1})\n",
    "\n",
    "    df_fold = X_fold_val.copy()\n",
    "    df_fold['y_true'] = y_fold_val\n",
    "    df_fold['y_pred'] = y_pred\n",
    "\n",
    "    if hasattr(knn, \"predict_proba\"):\n",
    "        probs = knn.predict_proba(X_fold_val_scaled)\n",
    "        for i, cls in enumerate(knn.classes_):\n",
    "            df_fold[f'prob_{cls}'] = probs[:, i]\n",
    "\n",
    "    fold_results_path = f'tests/knn/knn_tomek_results_fold_{fold}.csv'\n",
    "    df_fold.to_csv(fold_results_path, index=True)\n",
    "\n",
    "    creport = classification_report(y_fold_val, y_pred)\n",
    "    with open(f'tests/knn/knn_tomek_classification_report_fold_{fold}.txt', 'w') as f:\n",
    "        f.write(creport)\n",
    "    cm = confusion_matrix(y_fold_val, y_pred)\n",
    "    pd.DataFrame(cm).to_csv(f'tests/knn/knn_tomek_confusion_matrix_fold_{fold}.csv', index=False)\n",
    "\n",
    "    results_list.append(df_fold)\n",
    "\n",
    "all_results = pd.concat(results_list)\n",
    "all_results.to_csv('tests/knn/knn_tomek_results_all_folds.csv', index=True)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "metrics_df.loc['mean'] = metrics_df.mean(numeric_only=True)\n",
    "metrics_df.to_csv('tests/knn/knn_tomek_cv_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474cd82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: After SMOTE: X_res.shape = (1110536, 55), y_res distribution:\n",
      "Cover_Type\n",
      "1    158648\n",
      "2    158648\n",
      "3    158648\n",
      "4    158648\n",
      "6    158648\n",
      "7    158648\n",
      "5    158648\n",
      "Name: count, dtype: int64\n",
      "Fold 2: After SMOTE: X_res.shape = (1110536, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    158648\n",
      "1    158648\n",
      "3    158648\n",
      "4    158648\n",
      "6    158648\n",
      "7    158648\n",
      "5    158648\n",
      "Name: count, dtype: int64\n",
      "Fold 2: After SMOTE: X_res.shape = (1110536, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    158648\n",
      "1    158648\n",
      "3    158648\n",
      "4    158648\n",
      "6    158648\n",
      "7    158648\n",
      "5    158648\n",
      "Name: count, dtype: int64\n",
      "Fold 3: After SMOTE: X_res.shape = (1110536, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    158648\n",
      "1    158648\n",
      "3    158648\n",
      "6    158648\n",
      "7    158648\n",
      "5    158648\n",
      "4    158648\n",
      "Name: count, dtype: int64\n",
      "Fold 3: After SMOTE: X_res.shape = (1110536, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    158648\n",
      "1    158648\n",
      "3    158648\n",
      "6    158648\n",
      "7    158648\n",
      "5    158648\n",
      "4    158648\n",
      "Name: count, dtype: int64\n",
      "Fold 4: After SMOTE: X_res.shape = (1110536, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    158648\n",
      "1    158648\n",
      "3    158648\n",
      "4    158648\n",
      "6    158648\n",
      "7    158648\n",
      "5    158648\n",
      "Name: count, dtype: int64\n",
      "Fold 4: After SMOTE: X_res.shape = (1110536, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    158648\n",
      "1    158648\n",
      "3    158648\n",
      "4    158648\n",
      "6    158648\n",
      "7    158648\n",
      "5    158648\n",
      "Name: count, dtype: int64\n",
      "Fold 5: After SMOTE: X_res.shape = (1110536, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    158648\n",
      "1    158648\n",
      "3    158648\n",
      "4    158648\n",
      "6    158648\n",
      "7    158648\n",
      "5    158648\n",
      "Name: count, dtype: int64\n",
      "Fold 5: After SMOTE: X_res.shape = (1110536, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    158648\n",
      "1    158648\n",
      "3    158648\n",
      "4    158648\n",
      "6    158648\n",
      "7    158648\n",
      "5    158648\n",
      "Name: count, dtype: int64\n",
      "K-Fold (5) with SMOTE + Manhattan completed. Saved per-fold results and reports to 'tests/knn'.\n",
      "Cross-validation metrics:\n",
      "      fold  accuracy  f1_macro\n",
      "0      1.0  0.857282  0.768057\n",
      "1      2.0  0.858437  0.773969\n",
      "2      3.0  0.858585  0.772440\n",
      "3      4.0  0.856419  0.764478\n",
      "4      5.0  0.860710  0.779163\n",
      "mean   3.0  0.858287  0.771621\n",
      "K-Fold (5) with SMOTE + Manhattan completed. Saved per-fold results and reports to 'tests/knn'.\n",
      "Cross-validation metrics:\n",
      "      fold  accuracy  f1_macro\n",
      "0      1.0  0.857282  0.768057\n",
      "1      2.0  0.858437  0.773969\n",
      "2      3.0  0.858585  0.772440\n",
      "3      4.0  0.856419  0.764478\n",
      "4      5.0  0.860710  0.779163\n",
      "mean   3.0  0.858287  0.771621\n"
     ]
    }
   ],
   "source": [
    "# K-Fold cross-validated KNN with SMOTE using Manhattan distance; save outputs to tests/knn/\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "os.makedirs('tests/knn', exist_ok=True)\n",
    "\n",
    "X_train = X_train_full\n",
    "y_train = y_train_full\n",
    "\n",
    "n_splits = 5\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "results_list = []\n",
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train), start=1):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_fold_train_scaled = scaler.fit_transform(X_fold_train)\n",
    "    X_fold_val_scaled = scaler.transform(X_fold_val)\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_res, y_res = smote.fit_resample(X_fold_train_scaled, y_fold_train)\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=5, metric='manhattan', n_jobs=-1)\n",
    "    knn.fit(X_res, y_res)\n",
    "    y_pred = knn.predict(X_fold_val_scaled)\n",
    "\n",
    "    acc = accuracy_score(y_fold_val, y_pred)\n",
    "    f1 = f1_score(y_fold_val, y_pred, average='macro')\n",
    "\n",
    "    metrics_list.append({'fold': fold, 'accuracy': acc, 'f1_macro': f1})\n",
    "\n",
    "    df_fold = X_fold_val.copy()\n",
    "    df_fold['y_true'] = y_fold_val\n",
    "    df_fold['y_pred'] = y_pred\n",
    "\n",
    "    if hasattr(knn, \"predict_proba\"):\n",
    "        probs = knn.predict_proba(X_fold_val_scaled)\n",
    "        for i, cls in enumerate(knn.classes_):\n",
    "            df_fold[f'prob_{cls}'] = probs[:, i]\n",
    "\n",
    "    fold_results_path = f'tests/knn/knn_SMOTE_manhattan_results_fold_{fold}.csv'\n",
    "    df_fold.to_csv(fold_results_path, index=True)\n",
    "\n",
    "    creport = classification_report(y_fold_val, y_pred)\n",
    "    with open(f'tests/knn/knn_SMOTE_manhattan_classification_report_fold_{fold}.txt', 'w') as f:\n",
    "        f.write(creport)\n",
    "    cm = confusion_matrix(y_fold_val, y_pred)\n",
    "    pd.DataFrame(cm).to_csv(f'tests/knn/knn_SMOTE_manhattan_confusion_matrix_fold_{fold}.csv', index=False)\n",
    "\n",
    "    results_list.append(df_fold)\n",
    "\n",
    "all_results = pd.concat(results_list)\n",
    "all_results.to_csv('tests/knn/knn_SMOTE_manhattan_results_all_folds.csv', index=True)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "metrics_df.loc['mean'] = metrics_df.mean(numeric_only=True)\n",
    "metrics_df.to_csv('tests/knn/knn_SMOTE_manhattan_cv_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db63f330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: After Tomek: X_res.shape = (306946, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    150976\n",
      "1    111857\n",
      "3     18652\n",
      "7     10937\n",
      "6      8505\n",
      "5      4480\n",
      "4      1539\n",
      "Name: count, dtype: int64\n",
      "Fold 2: After Tomek: X_res.shape = (306760, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    150938\n",
      "1    111792\n",
      "3     18610\n",
      "7     10954\n",
      "6      8438\n",
      "5      4490\n",
      "4      1538\n",
      "Name: count, dtype: int64\n",
      "Fold 2: After Tomek: X_res.shape = (306760, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    150938\n",
      "1    111792\n",
      "3     18610\n",
      "7     10954\n",
      "6      8438\n",
      "5      4490\n",
      "4      1538\n",
      "Name: count, dtype: int64\n",
      "Fold 3: After Tomek: X_res.shape = (306752, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    150928\n",
      "1    111798\n",
      "3     18589\n",
      "7     10953\n",
      "6      8438\n",
      "5      4508\n",
      "4      1538\n",
      "Name: count, dtype: int64\n",
      "Fold 3: After Tomek: X_res.shape = (306752, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    150928\n",
      "1    111798\n",
      "3     18589\n",
      "7     10953\n",
      "6      8438\n",
      "5      4508\n",
      "4      1538\n",
      "Name: count, dtype: int64\n",
      "Fold 4: After Tomek: X_res.shape = (306700, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    150909\n",
      "1    111800\n",
      "3     18596\n",
      "7     10945\n",
      "6      8402\n",
      "5      4510\n",
      "4      1538\n",
      "Name: count, dtype: int64\n",
      "Fold 4: After Tomek: X_res.shape = (306700, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    150909\n",
      "1    111800\n",
      "3     18596\n",
      "7     10945\n",
      "6      8402\n",
      "5      4510\n",
      "4      1538\n",
      "Name: count, dtype: int64\n",
      "Fold 5: After Tomek: X_res.shape = (306684, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    150854\n",
      "1    111714\n",
      "3     18641\n",
      "7     10941\n",
      "6      8512\n",
      "5      4483\n",
      "4      1539\n",
      "Name: count, dtype: int64\n",
      "Fold 5: After Tomek: X_res.shape = (306684, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    150854\n",
      "1    111714\n",
      "3     18641\n",
      "7     10941\n",
      "6      8512\n",
      "5      4483\n",
      "4      1539\n",
      "Name: count, dtype: int64\n",
      "K-Fold (5) with Tomek + Manhattan completed. Saved per-fold results and reports to 'tests/knn'.\n",
      "Cross-validation metrics:\n",
      "      fold  accuracy  f1_macro\n",
      "0      1.0  0.878353  0.803546\n",
      "1      2.0  0.880750  0.810463\n",
      "2      3.0  0.879656  0.800056\n",
      "3      4.0  0.877860  0.795984\n",
      "4      5.0  0.880233  0.805901\n",
      "mean   3.0  0.879370  0.803190\n",
      "K-Fold (5) with Tomek + Manhattan completed. Saved per-fold results and reports to 'tests/knn'.\n",
      "Cross-validation metrics:\n",
      "      fold  accuracy  f1_macro\n",
      "0      1.0  0.878353  0.803546\n",
      "1      2.0  0.880750  0.810463\n",
      "2      3.0  0.879656  0.800056\n",
      "3      4.0  0.877860  0.795984\n",
      "4      5.0  0.880233  0.805901\n",
      "mean   3.0  0.879370  0.803190\n"
     ]
    }
   ],
   "source": [
    "# K-Fold cross-validated KNN with Tomek Links using Manhattan distance; save outputs to tests/knn/\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.under_sampling import TomekLinks as Tomek\n",
    "\n",
    "os.makedirs('tests/knn', exist_ok=True)\n",
    "\n",
    "X_train = X_train_full\n",
    "y_train = y_train_full\n",
    "\n",
    "n_splits = 5\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "results_list = []\n",
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train), start=1):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_fold_train_scaled = scaler.fit_transform(X_fold_train)\n",
    "    X_fold_val_scaled = scaler.transform(X_fold_val)\n",
    "\n",
    "    tomek = Tomek()\n",
    "    X_res, y_res = tomek.fit_resample(X_fold_train_scaled, y_fold_train)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, metric='manhattan', n_jobs=-1)\n",
    "    knn.fit(X_res, y_res)\n",
    "    y_pred = knn.predict(X_fold_val_scaled)\n",
    "\n",
    "    acc = accuracy_score(y_fold_val, y_pred)\n",
    "    f1 = f1_score(y_fold_val, y_pred, average='macro')\n",
    "\n",
    "    metrics_list.append({'fold': fold, 'accuracy': acc, 'f1_macro': f1})\n",
    "\n",
    "    df_fold = X_fold_val.copy()\n",
    "    df_fold['y_true'] = y_fold_val\n",
    "    df_fold['y_pred'] = y_pred\n",
    "\n",
    "    if hasattr(knn, \"predict_proba\"):\n",
    "        probs = knn.predict_proba(X_fold_val_scaled)\n",
    "        for i, cls in enumerate(knn.classes_):\n",
    "            df_fold[f'prob_{cls}'] = probs[:, i]\n",
    "\n",
    "    fold_results_path = f'tests/knn/knn_tomek_manhattan_results_fold_{fold}.csv'\n",
    "    df_fold.to_csv(fold_results_path, index=True)\n",
    "\n",
    "    creport = classification_report(y_fold_val, y_pred)\n",
    "    with open(f'tests/knn/knn_tomek_manhattan_classification_report_fold_{fold}.txt', 'w') as f:\n",
    "        f.write(creport)\n",
    "    cm = confusion_matrix(y_fold_val, y_pred)\n",
    "    pd.DataFrame(cm).to_csv(f'tests/knn/knn_tomek_manhattan_confusion_matrix_fold_{fold}.csv', index=False)\n",
    "\n",
    "    results_list.append(df_fold)\n",
    "\n",
    "all_results = pd.concat(results_list)\n",
    "all_results.to_csv('tests/knn/knn_tomek_manhattan_results_all_folds.csv', index=True)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "metrics_df.loc['mean'] = metrics_df.mean(numeric_only=True)\n",
    "metrics_df.to_csv('tests/knn/knn_tomek_manhattan_cv_metrics.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf60ed51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: After Tomek: X_res.shape = (306946, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    150976\n",
      "1    111857\n",
      "3     18652\n",
      "7     10937\n",
      "6      8505\n",
      "5      4480\n",
      "4      1539\n",
      "Name: count, dtype: int64\n",
      "Fold 2: After Tomek: X_res.shape = (306760, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    150938\n",
      "1    111792\n",
      "3     18610\n",
      "7     10954\n",
      "6      8438\n",
      "5      4490\n",
      "4      1538\n",
      "Name: count, dtype: int64\n",
      "Fold 3: After Tomek: X_res.shape = (306752, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    150928\n",
      "1    111798\n",
      "3     18589\n",
      "7     10953\n",
      "6      8438\n",
      "5      4508\n",
      "4      1538\n",
      "Name: count, dtype: int64\n",
      "Fold 4: After Tomek: X_res.shape = (306700, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    150909\n",
      "1    111800\n",
      "3     18596\n",
      "7     10945\n",
      "6      8402\n",
      "5      4510\n",
      "4      1538\n",
      "Name: count, dtype: int64\n",
      "Fold 5: After Tomek: X_res.shape = (306684, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    150854\n",
      "1    111714\n",
      "3     18641\n",
      "7     10941\n",
      "6      8512\n",
      "5      4483\n",
      "4      1539\n",
      "Name: count, dtype: int64\n",
      "K-Fold (5) with Tomek + Manhattan completed. Saved per-fold results and reports to 'tests/knn_grid'.\n",
      "Cross-validation metrics:\n",
      "      fold  accuracy  f1_macro\n",
      "0      1.0  0.869133  0.784848\n",
      "1      2.0  0.870276  0.788266\n",
      "2      3.0  0.868985  0.778954\n",
      "3      4.0  0.867201  0.779205\n",
      "4      5.0  0.869414  0.786124\n",
      "mean   3.0  0.869002  0.783479\n"
     ]
    }
   ],
   "source": [
    "# K-Fold cross-validated KNN with Tomek Links using Manhattan distance; save outputs to tests/knn/\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.under_sampling import TomekLinks as Tomek\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "os.makedirs('tests/knn_grid', exist_ok=True)\n",
    "\n",
    "X_train = X_train_full\n",
    "y_train = y_train_full\n",
    "\n",
    "n_splits = 5\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "results_list = []\n",
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train), start=1):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_fold_train_scaled = scaler.fit_transform(X_fold_train)\n",
    "    X_fold_val_scaled = scaler.transform(X_fold_val)\n",
    "\n",
    "    tomek = Tomek()\n",
    "    X_res, y_res = tomek.fit_resample(X_fold_train_scaled, y_fold_train)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=13, metric='manhattan', n_jobs=-1)\n",
    "    knn.fit(X_res, y_res)\n",
    "    y_pred = knn.predict(X_fold_val_scaled)\n",
    "\n",
    "    acc = accuracy_score(y_fold_val, y_pred)\n",
    "    f1 = f1_score(y_fold_val, y_pred, average='macro')\n",
    "\n",
    "    metrics_list.append({'fold': fold, 'accuracy': acc, 'f1_macro': f1})\n",
    "\n",
    "    df_fold = X_fold_val.copy()\n",
    "    df_fold['y_true'] = y_fold_val\n",
    "    df_fold['y_pred'] = y_pred\n",
    "\n",
    "    if hasattr(knn, \"predict_proba\"):\n",
    "        probs = knn.predict_proba(X_fold_val_scaled)\n",
    "        for i, cls in enumerate(knn.classes_):\n",
    "            df_fold[f'prob_{cls}'] = probs[:, i]\n",
    "\n",
    "    fold_results_path = f'tests/knn_grid/knn_k13_tomek_manhattan_results_fold_{fold}.csv'\n",
    "    df_fold.to_csv(fold_results_path, index=True)\n",
    "\n",
    "    creport = classification_report(y_fold_val, y_pred)\n",
    "    with open(f'tests/knn_grid/knn_k13_tomek_manhattan_classification_report_fold_{fold}.txt', 'w') as f:\n",
    "        f.write(creport)\n",
    "    cm = confusion_matrix(y_fold_val, y_pred)\n",
    "    pd.DataFrame(cm).to_csv(f'tests/knn_grid/knn_k13_tomek_manhattan_confusion_matrix_fold_{fold}.csv', index=False)\n",
    "\n",
    "    results_list.append(df_fold)\n",
    "\n",
    "all_results = pd.concat(results_list)\n",
    "all_results.to_csv('tests/knn_grid/knn_k13_tomek_manhattan_results_all_folds.csv', index=True)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "metrics_df.loc['mean'] = metrics_df.mean(numeric_only=True)\n",
    "metrics_df.to_csv('tests/knn_grid/knn_k13_tomek_manhattan_cv_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45092210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 10-fold CV for KNN (k=3, Manhattan, Tomek)...\n",
      "============================================================\n",
      "Fold  1 | Acc: 0.8850 | F1-macro: 0.8134 | Precision: 0.8357 | Recall: 0.7943 | Specificity: 0.9729 | Kappa: 0.8143\n",
      "Fold  1 | Acc: 0.8850 | F1-macro: 0.8134 | Precision: 0.8357 | Recall: 0.7943 | Specificity: 0.9729 | Kappa: 0.8143\n",
      "Fold  2 | Acc: 0.8819 | F1-macro: 0.8138 | Precision: 0.8334 | Recall: 0.7966 | Specificity: 0.9723 | Kappa: 0.8096\n",
      "Fold  2 | Acc: 0.8819 | F1-macro: 0.8138 | Precision: 0.8334 | Recall: 0.7966 | Specificity: 0.9723 | Kappa: 0.8096\n",
      "Fold  3 | Acc: 0.8858 | F1-macro: 0.8261 | Precision: 0.8469 | Recall: 0.8085 | Specificity: 0.9732 | Kappa: 0.8158\n",
      "Fold  3 | Acc: 0.8858 | F1-macro: 0.8261 | Precision: 0.8469 | Recall: 0.8085 | Specificity: 0.9732 | Kappa: 0.8158\n",
      "Fold  4 | Acc: 0.8881 | F1-macro: 0.8245 | Precision: 0.8411 | Recall: 0.8102 | Specificity: 0.9736 | Kappa: 0.8194\n",
      "Fold  4 | Acc: 0.8881 | F1-macro: 0.8245 | Precision: 0.8411 | Recall: 0.8102 | Specificity: 0.9736 | Kappa: 0.8194\n",
      "Fold  5 | Acc: 0.8866 | F1-macro: 0.8179 | Precision: 0.8432 | Recall: 0.7965 | Specificity: 0.9733 | Kappa: 0.8169\n",
      "Fold  5 | Acc: 0.8866 | F1-macro: 0.8179 | Precision: 0.8432 | Recall: 0.7965 | Specificity: 0.9733 | Kappa: 0.8169\n",
      "Fold  6 | Acc: 0.8824 | F1-macro: 0.8092 | Precision: 0.8330 | Recall: 0.7893 | Specificity: 0.9724 | Kappa: 0.8101\n",
      "Fold  6 | Acc: 0.8824 | F1-macro: 0.8092 | Precision: 0.8330 | Recall: 0.7893 | Specificity: 0.9724 | Kappa: 0.8101\n",
      "Fold  7 | Acc: 0.8793 | F1-macro: 0.8040 | Precision: 0.8265 | Recall: 0.7848 | Specificity: 0.9717 | Kappa: 0.8051\n",
      "Fold  7 | Acc: 0.8793 | F1-macro: 0.8040 | Precision: 0.8265 | Recall: 0.7848 | Specificity: 0.9717 | Kappa: 0.8051\n",
      "Fold  8 | Acc: 0.8855 | F1-macro: 0.8114 | Precision: 0.8295 | Recall: 0.7960 | Specificity: 0.9732 | Kappa: 0.8154\n",
      "Fold  9 | Acc: 0.8859 | F1-macro: 0.8157 | Precision: 0.8401 | Recall: 0.7952 | Specificity: 0.9731 | Kappa: 0.8157\n",
      "Fold 10 | Acc: 0.8850 | F1-macro: 0.8229 | Precision: 0.8434 | Recall: 0.8056 | Specificity: 0.9729 | Kappa: 0.8144\n",
      "============================================================\n",
      "10-Fold CV completed for KNN (k=3, Manhattan, Tomek)\n",
      "Results saved to 'tests/Final/' directory\n",
      "\n",
      "Final Cross-validation Summary:\n",
      "--------------------------------------------------\n",
      "Accuracy:     0.8845 ± 0.0026\n",
      "F1-macro:     0.8159 ± 0.0071\n",
      "F1-weighted:  0.8839 ± 0.0026\n",
      "Precision:    0.8373 ± 0.0067\n",
      "Recall:       0.7977 ± 0.0081\n",
      "Specificity:  0.9728 ± 0.0006\n",
      "Cohen's Kappa: 0.8137 ± 0.0042\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 10-fold cross-validated KNN with k=3, Manhattan distance, and Tomek Links\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.under_sampling import TomekLinks as Tomek\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, \n",
    "                           classification_report, confusion_matrix, cohen_kappa_score)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "os.makedirs('tests/Final', exist_ok=True)\n",
    "\n",
    "X_train = X_train_full\n",
    "y_train = y_train_full\n",
    "\n",
    "n_splits = 10 \n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "results_list = []\n",
    "metrics_list = []\n",
    "\n",
    "def calculate_specificity_multiclass(y_true, y_pred, classes):\n",
    "    \"\"\"Calculate macro-averaged specificity for multiclass classification\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    specificities = []\n",
    "    \n",
    "    for i, cls in enumerate(classes):\n",
    "        tn = np.sum(cm) - (np.sum(cm[i, :]) + np.sum(cm[:, i]) - cm[i, i])\n",
    "        fp = np.sum(cm[:, i]) - cm[i, i]\n",
    "        if tn + fp > 0:\n",
    "            specificity = tn / (tn + fp)\n",
    "        else:\n",
    "            specificity = 0.0\n",
    "        specificities.append(specificity)\n",
    "    \n",
    "    return np.mean(specificities)\n",
    "\n",
    "print(\"Starting 10-fold CV for KNN (k=3, Manhattan, Tomek)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train), start=1):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_fold_train_scaled = scaler.fit_transform(X_fold_train)\n",
    "    X_fold_val_scaled = scaler.transform(X_fold_val)\n",
    "\n",
    "    tomek = Tomek()\n",
    "    X_res, y_res = tomek.fit_resample(X_fold_train_scaled, y_fold_train)\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=3, metric='manhattan', n_jobs=-1)\n",
    "    knn.fit(X_res, y_res)\n",
    "    y_pred = knn.predict(X_fold_val_scaled)\n",
    "\n",
    "    acc = accuracy_score(y_fold_val, y_pred)\n",
    "    f1_macro = f1_score(y_fold_val, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_fold_val, y_pred, average='weighted')\n",
    "    precision_macro = precision_score(y_fold_val, y_pred, average='macro', zero_division=0)\n",
    "    recall_macro = recall_score(y_fold_val, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    classes = np.unique(np.concatenate([y_fold_val, y_pred]))\n",
    "    specificity_macro = calculate_specificity_multiclass(y_fold_val, y_pred, classes)\n",
    "    kappa = cohen_kappa_score(y_fold_val, y_pred)\n",
    "\n",
    "    metrics_list.append({\n",
    "        'fold': fold, \n",
    "        'accuracy': acc, \n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'specificity_macro': specificity_macro,\n",
    "        'cohen_kappa': kappa\n",
    "    })\n",
    "\n",
    "    df_fold = X_fold_val.copy()\n",
    "    df_fold['y_true'] = y_fold_val\n",
    "    df_fold['y_pred'] = y_pred\n",
    "\n",
    "    if hasattr(knn, \"predict_proba\"):\n",
    "        probs = knn.predict_proba(X_fold_val_scaled)\n",
    "        for i, cls in enumerate(knn.classes_):\n",
    "            df_fold[f'prob_{cls}'] = probs[:, i]\n",
    "\n",
    "    fold_results_path = f'tests/Final/knn_k3_tomek_manhattan_10fold_results_fold_{fold}.csv'\n",
    "    df_fold.to_csv(fold_results_path, index=True)\n",
    "\n",
    "    creport = classification_report(y_fold_val, y_pred)\n",
    "    with open(f'tests/Final/knn_k3_tomek_manhattan_10fold_classification_report_fold_{fold}.txt', 'w') as f:\n",
    "        f.write(creport)\n",
    "    cm = confusion_matrix(y_fold_val, y_pred)\n",
    "    pd.DataFrame(cm).to_csv(f'tests/Final/knn_k3_tomek_manhattan_10fold_confusion_matrix_fold_{fold}.csv', index=False)\n",
    "\n",
    "    results_list.append(df_fold)\n",
    "    \n",
    "    print(f\"Fold {fold:2d} | Acc: {acc:.4f} | F1-macro: {f1_macro:.4f} | Precision: {precision_macro:.4f} | Recall: {recall_macro:.4f} | Specificity: {specificity_macro:.4f} | Kappa: {kappa:.4f}\")\n",
    "\n",
    "all_results = pd.concat(results_list)\n",
    "all_results.to_csv('tests/Final/knn_k3_tomek_manhattan_10fold_results_all_folds.csv', index=True)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "mean_metrics = metrics_df.select_dtypes(include=[np.number]).mean()\n",
    "std_metrics = metrics_df.select_dtypes(include=[np.number]).std()\n",
    "\n",
    "summary_row = pd.DataFrame([mean_metrics, std_metrics], index=['mean', 'std'])\n",
    "metrics_summary = pd.concat([metrics_df, summary_row])\n",
    "metrics_summary.to_csv('tests/Final/knn_k3_tomek_manhattan_10fold_cv_metrics.csv', index=False)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"10-Fold CV completed for KNN (k=3, Manhattan, Tomek)\")\n",
    "print(f\"Results saved to 'tests/Final/' directory\")\n",
    "print(\"\\nFinal Cross-validation Summary:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Accuracy:     {mean_metrics['accuracy']:.4f} ± {std_metrics['accuracy']:.4f}\")\n",
    "print(f\"F1-macro:     {mean_metrics['f1_macro']:.4f} ± {std_metrics['f1_macro']:.4f}\")\n",
    "print(f\"F1-weighted:  {mean_metrics['f1_weighted']:.4f} ± {std_metrics['f1_weighted']:.4f}\")\n",
    "print(f\"Precision:    {mean_metrics['precision_macro']:.4f} ± {std_metrics['precision_macro']:.4f}\")\n",
    "print(f\"Recall:       {mean_metrics['recall_macro']:.4f} ± {std_metrics['recall_macro']:.4f}\")\n",
    "print(f\"Specificity:  {mean_metrics['specificity_macro']:.4f} ± {std_metrics['specificity_macro']:.4f}\")\n",
    "print(f\"Cohen's Kappa: {mean_metrics['cohen_kappa']:.4f} ± {std_metrics['cohen_kappa']:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c650909",
   "metadata": {},
   "source": [
    "### CLASSIFICATION TREES PRE PROCESSING ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2edb81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CT = df.copy()\n",
    "\n",
    "#Removing Facet\n",
    "df_CT = df_CT.drop(columns=['Facet'])\n",
    "#Removing Water_Level\n",
    "df_CT = df_CT.drop(columns=['Water_Level'])\n",
    "#Removing Observation_ID\n",
    "df_CT = df_CT.drop(columns=['Observation_ID'])\n",
    "\n",
    "#Noise, outliers, are all dependent on K\n",
    "#Got to deal with skew class distributions\n",
    "\n",
    "#Impute Missing Values\n",
    "df_CT['Slope'] = df_CT['Slope'].fillna(df_CT['Slope'].mean())\n",
    "\n",
    "#df_CT.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59be298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full CT dataset: 581012 samples\n",
      "Training set: 406708 samples (70.0%)\n",
      "Test set: 174304 samples (30.0%)\n",
      "\n",
      "Training set class distribution:\n",
      "Cover_Type\n",
      "1    148288\n",
      "2    198310\n",
      "3     25028\n",
      "4      1923\n",
      "5      6645\n",
      "6     12157\n",
      "7     14357\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set class distribution:\n",
      "Cover_Type\n",
      "1    63552\n",
      "2    84991\n",
      "3    10726\n",
      "4      824\n",
      "5     2848\n",
      "6     5210\n",
      "7     6153\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_CT = df_CT.drop(columns=['Cover_Type'])\n",
    "y_CT = df_CT['Cover_Type']\n",
    "\n",
    "X_train_full_CT, X_test_CT, y_train_full_CT, y_test_CT = train_test_split(\n",
    "    X_CT, y_CT, test_size=0.3, random_state=42, stratify=y_CT\n",
    ")\n",
    "\n",
    "print(f\"Full CT dataset: {X_CT.shape[0]} samples\")\n",
    "print(f\"Training set: {X_train_full_CT.shape[0]} samples ({X_train_full_CT.shape[0]/X_CT.shape[0]*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test_CT.shape[0]} samples ({X_test_CT.shape[0]/X_CT.shape[0]*100:.1f}%)\")\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(y_train_full_CT.value_counts().sort_index())\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print(y_test_CT.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f18b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 -- Accuracy: 0.9320, F1 (macro): 0.8918, F1 (weighted): 0.9319\n",
      "Fold 2 -- Accuracy: 0.9332, F1 (macro): 0.8970, F1 (weighted): 0.9331\n",
      "Fold 3 -- Accuracy: 0.9345, F1 (macro): 0.8969, F1 (weighted): 0.9345\n",
      "Fold 4 -- Accuracy: 0.9338, F1 (macro): 0.8982, F1 (weighted): 0.9338\n",
      "Fold 5 -- Accuracy: 0.9328, F1 (macro): 0.8958, F1 (weighted): 0.9328\n",
      "5-fold CV complete. Saved fold results and metrics to tests/.\n",
      "Cross-validation metrics:\n",
      "      fold  accuracy  f1_macro  f1_weighted\n",
      "0      1.0  0.931994  0.891765     0.931940\n",
      "1      2.0  0.933155  0.897003     0.933124\n",
      "2      3.0  0.934500  0.896938     0.934454\n",
      "3      4.0  0.933844  0.898236     0.933794\n",
      "4      5.0  0.932800  0.895767     0.932780\n",
      "mean   3.0  0.933259  0.895942     0.933218\n"
     ]
    }
   ],
   "source": [
    "# 5-fold cross-validated Decision Tree baseline\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "os.makedirs('tests/CT', exist_ok=True)\n",
    "\n",
    "X_train = X_train_full_CT\n",
    "y_train = y_train_full_CT\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "fold_metrics = []\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), start=1):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    clf = DecisionTreeClassifier(\n",
    "        criterion='entropy', \n",
    "        ccp_alpha=1.0961774439107219e-05, \n",
    "        random_state=42\n",
    "    )\n",
    "    clf.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "    y_pred = clf.predict(X_fold_val)\n",
    "    acc = accuracy_score(y_fold_val, y_pred)\n",
    "    f1_macro = f1_score(y_fold_val, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_fold_val, y_pred, average='weighted')\n",
    "    creport = classification_report(y_fold_val, y_pred)\n",
    "    cm = confusion_matrix(y_fold_val, y_pred)\n",
    "\n",
    "    print(f\"Fold {fold} -- Accuracy: {acc:.4f}, F1 (macro): {f1_macro:.4f}, F1 (weighted): {f1_weighted:.4f}\")\n",
    "    fold_metrics.append({'fold': fold, 'accuracy': acc, 'f1_macro': f1_macro, 'f1_weighted': f1_weighted})\n",
    "\n",
    "    df_fold = X_fold_val.copy()\n",
    "    df_fold['y_true'] = y_fold_val\n",
    "    df_fold['y_pred'] = y_pred\n",
    "    df_fold.to_csv(f'tests/CT/decision_tree_results_fold_{fold}.csv', index=True)\n",
    "\n",
    "    with open(f'tests/CT/decision_tree_classification_report_fold_{fold}.txt', 'w') as f:\n",
    "        f.write(creport)\n",
    "    pd.DataFrame(cm).to_csv(f'tests/CT/decision_tree_confusion_matrix_fold_{fold}.csv', index=False)\n",
    "\n",
    "    fold_results.append(df_fold)\n",
    "\n",
    "all_results = pd.concat(fold_results)\n",
    "all_results.to_csv('tests/CT/decision_tree_results_all_folds.csv', index=True)\n",
    "\n",
    "metrics_df = pd.DataFrame(fold_metrics)\n",
    "metrics_df.loc['mean'] = metrics_df.mean(numeric_only=True)\n",
    "metrics_df.to_csv('tests/CT/decision_tree_cv_metrics.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd94f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: after SMOTE X_res.shape=(1269184, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    181312\n",
      "7    181312\n",
      "1    181312\n",
      "5    181312\n",
      "3    181312\n",
      "4    181312\n",
      "6    181312\n",
      "Name: count, dtype: int64\n",
      "Fold 1 -- Accuracy: 0.9089, F1 (macro): 0.8584, F1 (weighted): 0.9095\n",
      "Fold 2: after SMOTE X_res.shape=(1269184, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    181312\n",
      "7    181312\n",
      "1    181312\n",
      "5    181312\n",
      "3    181312\n",
      "4    181312\n",
      "6    181312\n",
      "Name: count, dtype: int64\n",
      "Fold 2 -- Accuracy: 0.9110, F1 (macro): 0.8648, F1 (weighted): 0.9116\n",
      "Fold 3: after SMOTE X_res.shape=(1269184, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    181312\n",
      "1    181312\n",
      "5    181312\n",
      "7    181312\n",
      "3    181312\n",
      "4    181312\n",
      "6    181312\n",
      "Name: count, dtype: int64\n",
      "Fold 3 -- Accuracy: 0.9137, F1 (macro): 0.8703, F1 (weighted): 0.9141\n",
      "Fold 4: after SMOTE X_res.shape=(1269184, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    181312\n",
      "7    181312\n",
      "1    181312\n",
      "5    181312\n",
      "3    181312\n",
      "4    181312\n",
      "6    181312\n",
      "Name: count, dtype: int64\n",
      "Fold 4 -- Accuracy: 0.9095, F1 (macro): 0.8633, F1 (weighted): 0.9101\n",
      "Fold 5: after SMOTE X_res.shape=(1269184, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    181312\n",
      "7    181312\n",
      "1    181312\n",
      "3    181312\n",
      "4    181312\n",
      "5    181312\n",
      "6    181312\n",
      "Name: count, dtype: int64\n",
      "Fold 5 -- Accuracy: 0.9106, F1 (macro): 0.8629, F1 (weighted): 0.9112\n",
      "5-fold CV complete. Saved fold results and metrics to tests/.\n",
      "Cross-validation metrics:\n",
      "      fold  accuracy  f1_macro  f1_weighted\n",
      "0      1.0  0.908920  0.858447     0.909535\n",
      "1      2.0  0.910996  0.864820     0.911566\n",
      "2      3.0  0.913653  0.870258     0.914122\n",
      "3      4.0  0.909468  0.863294     0.910100\n",
      "4      5.0  0.910618  0.862929     0.911172\n",
      "mean   3.0  0.910731  0.863950     0.911299\n"
     ]
    }
   ],
   "source": [
    "# 5-fold cross-validated Decision Tree with SMOTE applied on each training fold\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.over_sampling import SMOTE    \n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "os.makedirs('tests/CT', exist_ok=True)\n",
    "\n",
    "X_train = X_train_full_CT\n",
    "y_train = y_train_full_CT\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "fold_metrics = []\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), start=1):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    smt = SMOTE(random_state=42)\n",
    "    X_res, y_res = smt.fit_resample(X_fold_train, y_fold_train)\n",
    "\n",
    "    clf = DecisionTreeClassifier(\n",
    "        criterion='entropy', \n",
    "        ccp_alpha=1.0961774439107219e-05, \n",
    "        random_state=42\n",
    "    )\n",
    "    clf.fit(X_res, y_res)\n",
    "\n",
    "    y_pred = clf.predict(X_fold_val)\n",
    "    acc = accuracy_score(y_fold_val, y_pred)\n",
    "    f1_macro = f1_score(y_fold_val, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_fold_val, y_pred, average='weighted')\n",
    "    creport = classification_report(y_fold_val, y_pred)\n",
    "    cm = confusion_matrix(y_fold_val, y_pred)\n",
    "\n",
    "    print(f\"Fold {fold} -- Accuracy: {acc:.4f}, F1 (macro): {f1_macro:.4f}, F1 (weighted): {f1_weighted:.4f}\")\n",
    "    fold_metrics.append({'fold': fold, 'accuracy': acc, 'f1_macro': f1_macro, 'f1_weighted': f1_weighted})\n",
    "\n",
    "    df_fold = X_fold_val.copy()\n",
    "    df_fold['y_true'] = y_fold_val\n",
    "    df_fold['y_pred'] = y_pred\n",
    "    df_fold.to_csv(f'tests/CT/decision_tree_SMOTE_results_fold_{fold}.csv', index=True)\n",
    "\n",
    "    with open(f'tests/CT/decision_tree_SMOTE_classification_report_fold_{fold}.txt', 'w') as f:\n",
    "        f.write(creport)\n",
    "    pd.DataFrame(cm).to_csv(f'tests/CT/decision_tree_SMOTE_confusion_matrix_fold_{fold}.csv', index=False)\n",
    "\n",
    "    fold_results.append(df_fold)\n",
    "\n",
    "all_results = pd.concat(fold_results)\n",
    "all_results.to_csv('tests/CT/decision_tree_SMOTE_results_all_folds.csv', index=True)\n",
    "\n",
    "metrics_df = pd.DataFrame(fold_metrics)\n",
    "metrics_df.loc['mean'] = metrics_df.mean(numeric_only=True)\n",
    "metrics_df.to_csv('tests/CT/decision_tree_SMOTE_cv_metrics.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45347aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: after Tomek X_res.shape=(341628, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    169296\n",
      "1    125307\n",
      "3     19841\n",
      "7     11888\n",
      "6      8795\n",
      "5      4742\n",
      "4      1759\n",
      "Name: count, dtype: int64\n",
      "Fold 1 -- Accuracy: 0.9245, F1 (macro): 0.8790, F1 (weighted): 0.9244\n",
      "Fold 2: after Tomek X_res.shape=(341629, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    169303\n",
      "1    125396\n",
      "3     19770\n",
      "7     11944\n",
      "6      8726\n",
      "5      4731\n",
      "4      1759\n",
      "Name: count, dtype: int64\n",
      "Fold 2 -- Accuracy: 0.9267, F1 (macro): 0.8851, F1 (weighted): 0.9266\n",
      "Fold 3: after Tomek X_res.shape=(341366, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    169221\n",
      "1    125321\n",
      "3     19747\n",
      "7     11884\n",
      "6      8704\n",
      "5      4731\n",
      "4      1758\n",
      "Name: count, dtype: int64\n",
      "Fold 3 -- Accuracy: 0.9275, F1 (macro): 0.8864, F1 (weighted): 0.9274\n",
      "Fold 4: after Tomek X_res.shape=(341609, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    169341\n",
      "1    125342\n",
      "3     19804\n",
      "7     11841\n",
      "6      8772\n",
      "5      4751\n",
      "4      1758\n",
      "Name: count, dtype: int64\n",
      "Fold 4 -- Accuracy: 0.9263, F1 (macro): 0.8863, F1 (weighted): 0.9262\n",
      "Fold 5: after Tomek X_res.shape=(341632, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    169313\n",
      "1    125296\n",
      "3     19886\n",
      "7     11845\n",
      "6      8794\n",
      "5      4740\n",
      "4      1758\n",
      "Name: count, dtype: int64\n",
      "Fold 5 -- Accuracy: 0.9264, F1 (macro): 0.8796, F1 (weighted): 0.9263\n",
      "5-fold CV with Tomek completed. Saved fold results and metrics to tests/.\n",
      "Cross-validation metrics:\n",
      "      fold  accuracy  f1_macro  f1_weighted\n",
      "0      1.0  0.924528  0.878984     0.924421\n",
      "1      2.0  0.926701  0.885059     0.926639\n",
      "2      3.0  0.927530  0.886414     0.927406\n",
      "3      4.0  0.926303  0.886304     0.926184\n",
      "4      5.0  0.926378  0.879637     0.926281\n",
      "mean   3.0  0.926288  0.883280     0.926186\n"
     ]
    }
   ],
   "source": [
    "# 5-fold cross-validated Decision Tree with Tomek Links applied to each training fold; save outputs to tests/\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from imblearn.under_sampling import TomekLinks as tomek\n",
    "\n",
    "os.makedirs('tests/CT', exist_ok=True)\n",
    "\n",
    "X_train = X_train_full_CT\n",
    "y_train = y_train_full_CT\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "fold_metrics = []\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), start=1):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_idx].copy(), X_train.iloc[val_idx].copy()\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_idx].copy(), y_train.iloc[val_idx].copy()\n",
    "\n",
    "    tomek_sampler = tomek()\n",
    "    X_res, y_res = tomek_sampler.fit_resample(X_fold_train, y_fold_train)\n",
    "\n",
    "    clf = DecisionTreeClassifier(\n",
    "        criterion='entropy', \n",
    "        ccp_alpha=1.0961774439107219e-05, \n",
    "        random_state=42\n",
    "    )\n",
    "    clf.fit(X_res, y_res)\n",
    "\n",
    "    y_pred = clf.predict(X_fold_val)\n",
    "    acc = accuracy_score(y_fold_val, y_pred)\n",
    "    f1_macro = f1_score(y_fold_val, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_fold_val, y_pred, average='weighted')\n",
    "    creport = classification_report(y_fold_val, y_pred)\n",
    "    cm = confusion_matrix(y_fold_val, y_pred)\n",
    "\n",
    "    print(f\"Fold {fold} -- Accuracy: {acc:.4f}, F1 (macro): {f1_macro:.4f}, F1 (weighted): {f1_weighted:.4f}\")\n",
    "    fold_metrics.append({'fold': fold, 'accuracy': acc, 'f1_macro': f1_macro, 'f1_weighted': f1_weighted})\n",
    "\n",
    "    df_fold = X_fold_val.copy()\n",
    "    df_fold['y_true'] = y_fold_val\n",
    "    df_fold['y_pred'] = y_pred\n",
    "    df_fold.to_csv(f'tests/CT/decision_tree_tomek_results_fold_{fold}.csv', index=True)\n",
    "\n",
    "    with open(f'tests/CT/decision_tree_tomek_classification_report_fold_{fold}.txt', 'w') as f:\n",
    "        f.write(creport)\n",
    "    pd.DataFrame(cm).to_csv(f'tests/CT/decision_tree_tomek_confusion_matrix_fold_{fold}.csv', index=False)\n",
    "\n",
    "    fold_results.append(df_fold)\n",
    "\n",
    "all_results = pd.concat(fold_results)\n",
    "all_results.to_csv('tests/CT/decision_tree_tomek_results_all_folds.csv', index=True)\n",
    "\n",
    "metrics_df = pd.DataFrame(fold_metrics)\n",
    "metrics_df.loc['mean'] = metrics_df.mean(numeric_only=True)\n",
    "metrics_df.to_csv('tests/CT/decision_tree_tomek_cv_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45347aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After SMOTE: X_train_prune_res.shape = (971719, 55), y_train_prune_res distribution:\n",
      "Cover_Type\n",
      "1    138817\n",
      "2    138817\n",
      "3    138817\n",
      "7    138817\n",
      "5    138817\n",
      "6    138817\n",
      "4    138817\n",
      "Name: count, dtype: int64\n",
      "Training accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X_train = X_train_full_CT\n",
    "y_train = y_train_full_CT\n",
    "\n",
    "X_train_prune, X_val_prune, y_train_prune, y_val_prune = train_test_split(\n",
    "    X_train, y_train, test_size=0.3, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "smt = SMOTE(random_state=42)\n",
    "X_train_prune_res, y_train_prune_res = smt.fit_resample(X_train_prune, y_train_prune)\n",
    "\n",
    "full_tree = DecisionTreeClassifier(\n",
    "    criterion='entropy',\n",
    "    min_samples_split=2,    \n",
    "    min_samples_leaf=1,     \n",
    "    max_depth=None,         \n",
    "    random_state=42\n",
    ")\n",
    "full_tree.fit(X_train_prune, y_train_prune)\n",
    "\n",
    "print(f\"Training accuracy: {full_tree.score(X_train_prune, y_train_prune):.4f}\")  \n",
    "\n",
    "path = full_tree.cost_complexity_pruning_path(X_train_prune, y_train_prune)\n",
    "alphas = path.ccp_alphas\n",
    "\n",
    "best_score = 0\n",
    "best_alpha = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d930914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique alphas: 7037\n"
     ]
    }
   ],
   "source": [
    "unique_alphas = np.unique(alphas)\n",
    "alphas = unique_alphas\n",
    "print(f\"Unique alphas: {len(unique_alphas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d930914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing alpha: 0.0\n",
      "Testing alpha: 8.543190100035498e-06\n",
      "Testing alpha: 9.477934396234958e-06\n",
      "Testing alpha: 1.018652238466907e-05\n",
      "Testing alpha: 1.0837088615439781e-05\n",
      "Testing alpha: 1.130157422746196e-05\n",
      "Testing alpha: 1.1691941806324674e-05\n",
      "Testing alpha: 1.2043295203214815e-05\n",
      "Testing alpha: 1.242035709635054e-05\n",
      "Testing alpha: 1.2721142772821291e-05\n",
      "Testing alpha: 1.3008313179060527e-05\n",
      "Testing alpha: 1.3233149538734479e-05\n",
      "Testing alpha: 1.3504311330388642e-05\n",
      "Testing alpha: 1.3803641077687873e-05\n",
      "Testing alpha: 1.4020442079078808e-05\n",
      "Testing alpha: 1.4223709669046553e-05\n",
      "Testing alpha: 1.4391419565917253e-05\n",
      "Testing alpha: 1.4547882646608802e-05\n",
      "Testing alpha: 1.4751684926745293e-05\n",
      "Testing alpha: 1.493254410544438e-05\n",
      "Testing alpha: 1.5190777146116696e-05\n",
      "Testing alpha: 1.5332109885158767e-05\n",
      "Testing alpha: 1.5462271014065216e-05\n",
      "Testing alpha: 1.561794547636763e-05\n",
      "Testing alpha: 1.5786485894141878e-05\n",
      "Testing alpha: 1.596626302492974e-05\n",
      "Testing alpha: 1.613092769050886e-05\n",
      "Testing alpha: 1.6340661038918502e-05\n",
      "Testing alpha: 1.6498467100558638e-05\n",
      "Testing alpha: 1.6732454894708268e-05\n",
      "Testing alpha: 1.689464109880274e-05\n",
      "Testing alpha: 1.7097837147666088e-05\n",
      "Testing alpha: 1.7160845002022752e-05\n",
      "Testing alpha: 1.7364368623027236e-05\n",
      "Testing alpha: 1.749398476034397e-05\n",
      "Testing alpha: 1.7670219723860117e-05\n",
      "Testing alpha: 1.781565228434301e-05\n",
      "Testing alpha: 1.794155740568548e-05\n",
      "Testing alpha: 1.813671699668921e-05\n",
      "Testing alpha: 1.8272439792389576e-05\n",
      "Testing alpha: 1.8439885316366533e-05\n",
      "Testing alpha: 1.8548243448066054e-05\n",
      "Testing alpha: 1.8678163234645168e-05\n",
      "Testing alpha: 1.8789646399293178e-05\n",
      "Testing alpha: 1.8957107640582406e-05\n",
      "Testing alpha: 1.9141023428113926e-05\n",
      "Testing alpha: 1.9353255253260276e-05\n",
      "Testing alpha: 1.953537529093939e-05\n",
      "Testing alpha: 1.9662934989513014e-05\n",
      "Testing alpha: 1.982730054273028e-05\n",
      "Testing alpha: 1.994380374167519e-05\n",
      "Testing alpha: 2.0104373282631635e-05\n",
      "Testing alpha: 2.0216612462452323e-05\n",
      "Testing alpha: 2.035780808429562e-05\n",
      "Testing alpha: 2.048670605128816e-05\n",
      "Testing alpha: 2.067624563412467e-05\n",
      "Testing alpha: 2.083494408854108e-05\n",
      "Testing alpha: 2.092902123031255e-05\n",
      "Testing alpha: 2.1101759055738403e-05\n",
      "Testing alpha: 2.132101699249714e-05\n",
      "Testing alpha: 2.143495330943748e-05\n",
      "Testing alpha: 2.156894016508681e-05\n",
      "Testing alpha: 2.1705302227327406e-05\n",
      "Testing alpha: 2.1861905755981878e-05\n",
      "Testing alpha: 2.1986479419448042e-05\n",
      "Testing alpha: 2.2131068520624462e-05\n",
      "Testing alpha: 2.2290515184772035e-05\n",
      "Testing alpha: 2.2456635738321087e-05\n",
      "Testing alpha: 2.2609045869879855e-05\n",
      "Testing alpha: 2.2725730571716015e-05\n",
      "Testing alpha: 2.2831234873135445e-05\n",
      "Testing alpha: 2.302967763107956e-05\n",
      "Testing alpha: 2.3169590666708664e-05\n",
      "Testing alpha: 2.3367798727115936e-05\n",
      "Testing alpha: 2.352352900891671e-05\n",
      "Testing alpha: 2.3632981315924834e-05\n",
      "Testing alpha: 2.3830781928717705e-05\n",
      "Testing alpha: 2.400739472969525e-05\n",
      "Testing alpha: 2.4181666263284457e-05\n",
      "Testing alpha: 2.4285801331634985e-05\n",
      "Testing alpha: 2.4415420971473544e-05\n",
      "Testing alpha: 2.463726369029371e-05\n",
      "Testing alpha: 2.475762345244512e-05\n",
      "Testing alpha: 2.485263087566666e-05\n",
      "Testing alpha: 2.4992067865838736e-05\n",
      "Testing alpha: 2.5138898640695357e-05\n",
      "Testing alpha: 2.5357947799833587e-05\n",
      "Testing alpha: 2.5511003607946925e-05\n",
      "Testing alpha: 2.5640043741173368e-05\n",
      "Testing alpha: 2.575142603095653e-05\n",
      "Testing alpha: 2.5942325452403427e-05\n",
      "Testing alpha: 2.6082691792222616e-05\n",
      "Testing alpha: 2.624097714051595e-05\n",
      "Testing alpha: 2.6451712576202825e-05\n",
      "Testing alpha: 2.661821088799959e-05\n",
      "Testing alpha: 2.679797499348596e-05\n",
      "Testing alpha: 2.6934808597331463e-05\n",
      "Testing alpha: 2.7073960527198916e-05\n",
      "Testing alpha: 2.72191391658942e-05\n",
      "Testing alpha: 2.7419809480898073e-05\n",
      "Testing alpha: 2.7591635346101746e-05\n",
      "Testing alpha: 2.7689747955186263e-05\n",
      "Testing alpha: 2.790770947727263e-05\n",
      "Testing alpha: 2.803859083522992e-05\n",
      "Testing alpha: 2.8238801651613767e-05\n",
      "Testing alpha: 2.8409886362775127e-05\n",
      "Testing alpha: 2.8560454464706554e-05\n",
      "Testing alpha: 2.8703147518462333e-05\n",
      "Testing alpha: 2.8911463047374554e-05\n",
      "Testing alpha: 2.903888577571081e-05\n",
      "Testing alpha: 2.9204983234544256e-05\n",
      "Testing alpha: 2.9395092406089988e-05\n",
      "Testing alpha: 2.95615518198513e-05\n",
      "Testing alpha: 2.969947168666061e-05\n",
      "Testing alpha: 2.9838344745053e-05\n",
      "Testing alpha: 3.0002529783557046e-05\n",
      "Testing alpha: 3.0173636435682656e-05\n",
      "Testing alpha: 3.029917808599757e-05\n",
      "Testing alpha: 3.048435455782074e-05\n",
      "Testing alpha: 3.0647639091187164e-05\n",
      "Testing alpha: 3.091992281925478e-05\n",
      "Testing alpha: 3.108571059557967e-05\n",
      "Testing alpha: 3.1264785629820765e-05\n",
      "Testing alpha: 3.142467399925573e-05\n",
      "Testing alpha: 3.166153673265587e-05\n",
      "Testing alpha: 3.1879167820747144e-05\n",
      "Testing alpha: 3.220359579823954e-05\n",
      "Testing alpha: 3.238824328761701e-05\n",
      "Testing alpha: 3.252076426189771e-05\n",
      "Testing alpha: 3.2780390372075913e-05\n",
      "Testing alpha: 3.293178598474944e-05\n",
      "Testing alpha: 3.311657161505199e-05\n",
      "Testing alpha: 3.3236124020321985e-05\n",
      "Testing alpha: 3.342067202056546e-05\n",
      "Testing alpha: 3.365111142203571e-05\n",
      "Testing alpha: 3.3901137899869075e-05\n",
      "Testing alpha: 3.412610976191095e-05\n",
      "Testing alpha: 3.429061477357383e-05\n",
      "Testing alpha: 3.449841841112852e-05\n",
      "Testing alpha: 3.4713909352000565e-05\n",
      "Testing alpha: 3.5020379618010624e-05\n",
      "Testing alpha: 3.5199322352710307e-05\n",
      "Testing alpha: 3.544411779354595e-05\n",
      "Testing alpha: 3.563931867111423e-05\n",
      "Testing alpha: 3.5860732082089014e-05\n",
      "Testing alpha: 3.619686285931494e-05\n",
      "Testing alpha: 3.651328919143072e-05\n",
      "Testing alpha: 3.675181070869438e-05\n",
      "Testing alpha: 3.6974884776384105e-05\n",
      "Testing alpha: 3.725006206499561e-05\n",
      "Testing alpha: 3.749205692570731e-05\n",
      "Testing alpha: 3.7672417569325743e-05\n",
      "Testing alpha: 3.786725856526521e-05\n",
      "Testing alpha: 3.809233676456486e-05\n",
      "Testing alpha: 3.8378062919239525e-05\n",
      "Testing alpha: 3.854558949932926e-05\n",
      "Testing alpha: 3.877743981869231e-05\n",
      "Testing alpha: 3.893757415781684e-05\n",
      "Testing alpha: 3.913558484642214e-05\n",
      "Testing alpha: 3.939079083153729e-05\n",
      "Testing alpha: 3.9604731765589634e-05\n",
      "Testing alpha: 3.979607337057285e-05\n",
      "Testing alpha: 3.992126433606966e-05\n",
      "Testing alpha: 4.0239127082099264e-05\n",
      "Testing alpha: 4.057707194142854e-05\n",
      "Testing alpha: 4.0869375802968745e-05\n",
      "Testing alpha: 4.107006623669113e-05\n",
      "Testing alpha: 4.131829924250224e-05\n",
      "Testing alpha: 4.158847240598185e-05\n",
      "Testing alpha: 4.1847523580219e-05\n",
      "Testing alpha: 4.211839914688943e-05\n",
      "Testing alpha: 4.226352779269682e-05\n",
      "Testing alpha: 4.2565203417479126e-05\n",
      "Testing alpha: 4.2839090652458855e-05\n",
      "Testing alpha: 4.306912144779124e-05\n",
      "Testing alpha: 4.331325203367362e-05\n",
      "Testing alpha: 4.356395229807364e-05\n",
      "Testing alpha: 4.373263640204312e-05\n",
      "Testing alpha: 4.408089755393495e-05\n",
      "Testing alpha: 4.429741863154013e-05\n",
      "Testing alpha: 4.456480276031444e-05\n",
      "Testing alpha: 4.479086205251895e-05\n",
      "Testing alpha: 4.5138428173335966e-05\n",
      "Testing alpha: 4.546780546586358e-05\n",
      "Testing alpha: 4.573947022612502e-05\n",
      "Testing alpha: 4.607494255613788e-05\n",
      "Testing alpha: 4.6287723786998705e-05\n",
      "Testing alpha: 4.650370931757466e-05\n",
      "Testing alpha: 4.676605289764383e-05\n",
      "Testing alpha: 4.704946373145527e-05\n",
      "Testing alpha: 4.732136734396242e-05\n",
      "Testing alpha: 4.768779124485786e-05\n",
      "Testing alpha: 4.790994602200862e-05\n",
      "Testing alpha: 4.831725571982355e-05\n",
      "Testing alpha: 4.865031713168018e-05\n",
      "Testing alpha: 4.890071531698034e-05\n",
      "Testing alpha: 4.934028127368315e-05\n",
      "Testing alpha: 4.9714821271406424e-05\n",
      "Testing alpha: 4.994851509555363e-05\n",
      "Testing alpha: 5.023616870642348e-05\n",
      "Testing alpha: 5.0592447962347276e-05\n",
      "Testing alpha: 5.110661413216725e-05\n",
      "Testing alpha: 5.142359773182851e-05\n",
      "Testing alpha: 5.1784789267026224e-05\n",
      "Testing alpha: 5.20450891647727e-05\n",
      "Testing alpha: 5.238751616521922e-05\n",
      "Testing alpha: 5.287789934871589e-05\n",
      "Testing alpha: 5.3308082969453585e-05\n",
      "Testing alpha: 5.368921289389977e-05\n",
      "Testing alpha: 5.40358042864936e-05\n",
      "Testing alpha: 5.445875999189576e-05\n",
      "Testing alpha: 5.479885401110272e-05\n",
      "Testing alpha: 5.523756643984685e-05\n",
      "Testing alpha: 5.567262502910041e-05\n",
      "Testing alpha: 5.602407393505591e-05\n",
      "Testing alpha: 5.6461406027054914e-05\n",
      "Testing alpha: 5.686760637740976e-05\n",
      "Testing alpha: 5.726210256506809e-05\n",
      "Testing alpha: 5.7684646713380695e-05\n",
      "Testing alpha: 5.815316843410013e-05\n",
      "Testing alpha: 5.852593013521648e-05\n",
      "Testing alpha: 5.8913664423366054e-05\n",
      "Testing alpha: 5.947252623361486e-05\n",
      "Testing alpha: 5.9915969771279805e-05\n",
      "Testing alpha: 6.0387020014452386e-05\n",
      "Testing alpha: 6.0781725550114766e-05\n",
      "Testing alpha: 6.138592892109224e-05\n",
      "Testing alpha: 6.18048803066127e-05\n",
      "Testing alpha: 6.225840755749006e-05\n",
      "Testing alpha: 6.262649738602709e-05\n",
      "Testing alpha: 6.302675024382727e-05\n",
      "Testing alpha: 6.345860737962794e-05\n",
      "Testing alpha: 6.417149941592248e-05\n",
      "Testing alpha: 6.450104961055733e-05\n",
      "Testing alpha: 6.491339030796213e-05\n",
      "Testing alpha: 6.525814525790445e-05\n",
      "Testing alpha: 6.570725225326347e-05\n",
      "Testing alpha: 6.631145640745577e-05\n",
      "Testing alpha: 6.68006861157631e-05\n",
      "Testing alpha: 6.735169085037891e-05\n",
      "Testing alpha: 6.793760875540654e-05\n",
      "Testing alpha: 6.84570557306091e-05\n",
      "Testing alpha: 6.89472896000938e-05\n",
      "Testing alpha: 6.949395590922028e-05\n",
      "Testing alpha: 7.021989715570368e-05\n",
      "Testing alpha: 7.07591082893201e-05\n",
      "Testing alpha: 7.114130303363448e-05\n",
      "Testing alpha: 7.188161083217378e-05\n",
      "Testing alpha: 7.262520983266713e-05\n",
      "Testing alpha: 7.342100872237384e-05\n",
      "Testing alpha: 7.411846245452977e-05\n",
      "Testing alpha: 7.474882844774852e-05\n",
      "Testing alpha: 7.537463616227034e-05\n",
      "Testing alpha: 7.606242235876837e-05\n",
      "Testing alpha: 7.682414167206573e-05\n",
      "Testing alpha: 7.755639728235298e-05\n",
      "Testing alpha: 7.810240357911975e-05\n",
      "Testing alpha: 7.911251830703368e-05\n",
      "Testing alpha: 8.00370215741988e-05\n",
      "Testing alpha: 8.058493315769757e-05\n",
      "Testing alpha: 8.122990843938182e-05\n",
      "Testing alpha: 8.23833581531548e-05\n",
      "Testing alpha: 8.316641943882049e-05\n",
      "Testing alpha: 8.413622804402925e-05\n",
      "Testing alpha: 8.48583686381432e-05\n",
      "Testing alpha: 8.553487723330657e-05\n",
      "Testing alpha: 8.64136187249044e-05\n",
      "Testing alpha: 8.709987245980925e-05\n",
      "Testing alpha: 8.801852964543326e-05\n",
      "Testing alpha: 8.917783058123101e-05\n",
      "Testing alpha: 9.005748480703522e-05\n",
      "Testing alpha: 9.103703521560146e-05\n",
      "Testing alpha: 9.239794464566736e-05\n",
      "Testing alpha: 9.346221121569076e-05\n",
      "Testing alpha: 9.421971101533338e-05\n",
      "Testing alpha: 9.522099640883793e-05\n",
      "Testing alpha: 9.658918359722654e-05\n",
      "Testing alpha: 9.78327570376704e-05\n",
      "Testing alpha: 9.880158116971427e-05\n",
      "Testing alpha: 9.986207808795455e-05\n",
      "Testing alpha: 0.00010070529826433438\n",
      "Testing alpha: 0.00010225956810572625\n",
      "Testing alpha: 0.00010389938466939602\n",
      "Testing alpha: 0.00010493489018685572\n",
      "Testing alpha: 0.00010598131254945938\n",
      "Testing alpha: 0.00010742664921347235\n",
      "Testing alpha: 0.00010927340341801976\n",
      "Testing alpha: 0.00011108821963983799\n",
      "Testing alpha: 0.00011278676334942482\n",
      "Testing alpha: 0.00011365687476142182\n",
      "Testing alpha: 0.0001152629392439814\n",
      "Testing alpha: 0.00011718558939181997\n",
      "Testing alpha: 0.00011958017658694995\n",
      "Testing alpha: 0.00012206618679041174\n",
      "Testing alpha: 0.00012323974050260122\n",
      "Testing alpha: 0.00012545572105846902\n",
      "Testing alpha: 0.00012708149783410952\n",
      "Testing alpha: 0.00012955835680504641\n",
      "Testing alpha: 0.00013173041784813076\n",
      "Testing alpha: 0.0001334091304933944\n",
      "Testing alpha: 0.00013549427375812457\n",
      "Testing alpha: 0.00013729915507409778\n",
      "Testing alpha: 0.00013941319929026627\n",
      "Testing alpha: 0.00014216800666391784\n",
      "Testing alpha: 0.00014577095589185648\n",
      "Testing alpha: 0.0001485150589212354\n",
      "Testing alpha: 0.00015023274244313554\n",
      "Testing alpha: 0.00015344568025087884\n",
      "Testing alpha: 0.00015601800816888238\n",
      "Testing alpha: 0.00016074589028220983\n",
      "Testing alpha: 0.00016305871644539167\n",
      "Testing alpha: 0.00016594443807474793\n",
      "Testing alpha: 0.0001683267686941925\n",
      "Testing alpha: 0.00017182323846365137\n",
      "Testing alpha: 0.00017693393420874302\n",
      "Testing alpha: 0.00018141002378399725\n",
      "Testing alpha: 0.0001861528417074741\n",
      "Testing alpha: 0.0001900326801658541\n",
      "Testing alpha: 0.00019579959783688088\n",
      "Testing alpha: 0.00019922714781462264\n",
      "Testing alpha: 0.0002033034630117747\n",
      "Testing alpha: 0.00020719603790763042\n",
      "Testing alpha: 0.00021364415820793062\n",
      "Testing alpha: 0.00021819987018759405\n",
      "Testing alpha: 0.0002251851303408109\n",
      "Testing alpha: 0.00023166792987219234\n",
      "Testing alpha: 0.0002412179677689294\n",
      "Testing alpha: 0.00024964597469208176\n",
      "Testing alpha: 0.0002553494967176479\n",
      "Testing alpha: 0.00026521480715427685\n",
      "Testing alpha: 0.00027834462924285533\n",
      "Testing alpha: 0.00029069506889766415\n",
      "Testing alpha: 0.00030438736687225885\n",
      "Testing alpha: 0.0003121316010427506\n",
      "Testing alpha: 0.0003257267932153408\n",
      "Testing alpha: 0.00033780706420492553\n",
      "Testing alpha: 0.00035806908493397406\n",
      "Testing alpha: 0.0003736550142250509\n",
      "Testing alpha: 0.0003914866855214585\n",
      "Testing alpha: 0.0004256717423113967\n",
      "Testing alpha: 0.00046124831638336746\n",
      "Testing alpha: 0.00048741700355075054\n",
      "Testing alpha: 0.0005177009131950372\n",
      "Testing alpha: 0.0005508683076330138\n",
      "Testing alpha: 0.0006050093339888367\n",
      "Testing alpha: 0.0006874835944067504\n",
      "Testing alpha: 0.0008015408864868127\n",
      "Testing alpha: 0.0008876978373506079\n",
      "Testing alpha: 0.0010994336522461255\n",
      "Testing alpha: 0.001333135235380348\n",
      "Testing alpha: 0.001973858418298488\n",
      "Testing alpha: 0.0038972271244916527\n",
      "Saved pruning results to tests/SMOTE_decision_tree_pruning_alphas.csv\n",
      "Top candidates:\n",
      "      alpha  f1_macro  n_nodes  depth\n",
      "0  0.000000  0.884129    31845     41\n",
      "1  0.000011  0.884092    28793     39\n",
      "2  0.000009  0.884033    30349     41\n",
      "3  0.000011  0.883987    28925     39\n",
      "4  0.000009  0.883972    30571     41\n",
      "5  0.000010  0.883907    29143     39\n",
      "6  0.000016  0.883742    23699     38\n",
      "7  0.000012  0.883684    27493     39\n",
      "8  0.000012  0.883591    27757     39\n",
      "9  0.000017  0.883568    23143     38\n",
      "Best alpha: 0.0\n",
      "Validation F1 (macro): 0.8841\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "os.makedirs('tests', exist_ok=True)\n",
    "\n",
    "alphas_subset = alphas[::20]\n",
    "\n",
    "results = []\n",
    "best_score = -1.0\n",
    "best_alpha = None\n",
    "\n",
    "for alpha in alphas_subset:\n",
    "    print(f\"Testing alpha: {alpha}\")\n",
    "    tree = DecisionTreeClassifier(\n",
    "        criterion='entropy', \n",
    "        ccp_alpha=alpha, \n",
    "        random_state=42\n",
    "    )\n",
    "    tree.fit(X_train_prune, y_train_prune)\n",
    "\n",
    "    y_val_pred = tree.predict(X_val_prune)\n",
    "    score = f1_score(y_val_prune, y_val_pred, average='macro')\n",
    "\n",
    "    try:\n",
    "        n_nodes = int(tree.tree_.node_count)\n",
    "    except Exception:\n",
    "        n_nodes = None\n",
    "    try:\n",
    "        depth = int(tree.get_depth())\n",
    "    except Exception:\n",
    "        depth = None\n",
    "\n",
    "    results.append({'alpha': float(alpha), 'f1_macro': float(score), 'n_nodes': n_nodes, 'depth': depth})\n",
    "\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_alpha = alpha\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_alpha_results = pd.DataFrame(results)\n",
    "df_alpha_results = df_alpha_results.sort_values('f1_macro', ascending=False).reset_index(drop=True)\n",
    "df_alpha_results.to_csv('tests/SMOTE_decision_tree_pruning_alphas.csv', index=False)\n",
    "\n",
    "print(df_alpha_results.head(10))\n",
    "\n",
    "pruned_tree = DecisionTreeClassifier(\n",
    "    criterion='entropy', \n",
    "    ccp_alpha=best_alpha, \n",
    "    random_state=42,\n",
    ")\n",
    "pruned_tree.fit(X_train_prune, y_train_prune)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa5dd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 -- Accuracy: 0.9288, F1 (macro): 0.8897, F1 (weighted): 0.9289\n",
      "Fold 2 -- Accuracy: 0.9301, F1 (macro): 0.8864, F1 (weighted): 0.9302\n",
      "Fold 2 -- Accuracy: 0.9301, F1 (macro): 0.8864, F1 (weighted): 0.9302\n",
      "Fold 3 -- Accuracy: 0.9299, F1 (macro): 0.8860, F1 (weighted): 0.9298\n",
      "Fold 3 -- Accuracy: 0.9299, F1 (macro): 0.8860, F1 (weighted): 0.9298\n",
      "Fold 4 -- Accuracy: 0.9285, F1 (macro): 0.8884, F1 (weighted): 0.9285\n",
      "Fold 4 -- Accuracy: 0.9285, F1 (macro): 0.8884, F1 (weighted): 0.9285\n",
      "Fold 5 -- Accuracy: 0.9312, F1 (macro): 0.8891, F1 (weighted): 0.9312\n",
      "Fold 5 -- Accuracy: 0.9312, F1 (macro): 0.8891, F1 (weighted): 0.9312\n",
      "5-fold CV with Gini criterion complete. Saved fold results and metrics to tests/.\n",
      "Cross-validation metrics:\n",
      "      fold  accuracy  f1_macro  f1_weighted\n",
      "0      1.0  0.928842  0.889670     0.928851\n",
      "1      2.0  0.930122  0.886444     0.930152\n",
      "2      3.0  0.929875  0.886022     0.929844\n",
      "3      4.0  0.928487  0.888357     0.928487\n",
      "4      5.0  0.931165  0.889093     0.931154\n",
      "mean   3.0  0.929698  0.887917     0.929697\n",
      "5-fold CV with Gini criterion complete. Saved fold results and metrics to tests/.\n",
      "Cross-validation metrics:\n",
      "      fold  accuracy  f1_macro  f1_weighted\n",
      "0      1.0  0.928842  0.889670     0.928851\n",
      "1      2.0  0.930122  0.886444     0.930152\n",
      "2      3.0  0.929875  0.886022     0.929844\n",
      "3      4.0  0.928487  0.888357     0.928487\n",
      "4      5.0  0.931165  0.889093     0.931154\n",
      "mean   3.0  0.929698  0.887917     0.929697\n"
     ]
    }
   ],
   "source": [
    "# 5-fold cross-validated Decision Tree with Gini criterion (for comparison with entropy)\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "os.makedirs('tests/CT', exist_ok=True)\n",
    "\n",
    "X_train = X_train_full_CT\n",
    "y_train = y_train_full_CT\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "fold_metrics = []\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), start=1):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "    clf.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "    y_pred = clf.predict(X_fold_val)\n",
    "    acc = accuracy_score(y_fold_val, y_pred)\n",
    "    f1_macro = f1_score(y_fold_val, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_fold_val, y_pred, average='weighted')\n",
    "    creport = classification_report(y_fold_val, y_pred)\n",
    "    cm = confusion_matrix(y_fold_val, y_pred)\n",
    "\n",
    "    print(f\"Fold {fold} -- Accuracy: {acc:.4f}, F1 (macro): {f1_macro:.4f}, F1 (weighted): {f1_weighted:.4f}\")\n",
    "    fold_metrics.append({'fold': fold, 'accuracy': acc, 'f1_macro': f1_macro, 'f1_weighted': f1_weighted})\n",
    "\n",
    "    df_fold = X_fold_val.copy()\n",
    "    df_fold['y_true'] = y_fold_val\n",
    "    df_fold['y_pred'] = y_pred\n",
    "    df_fold.to_csv(f'tests/CT/decision_tree_gini_base_results_fold_{fold}.csv', index=True)\n",
    "\n",
    "    with open(f'tests/CT/decision_tree_gini_base_classification_report_fold_{fold}.txt', 'w') as f:\n",
    "        f.write(creport)\n",
    "    pd.DataFrame(cm).to_csv(f'tests/CT/decision_tree_gini_base_confusion_matrix_fold_{fold}.csv', index=False)\n",
    "\n",
    "    fold_results.append(df_fold)\n",
    "\n",
    "all_results = pd.concat(fold_results)\n",
    "all_results.to_csv('tests/CT/decision_tree_gini_base_results_all_folds.csv', index=True)\n",
    "\n",
    "metrics_df = pd.DataFrame(fold_metrics)\n",
    "metrics_df.loc['mean'] = metrics_df.mean(numeric_only=True)\n",
    "metrics_df.to_csv('tests/CT/decision_tree_gini_base_cv_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c021b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: after SMOTE X_res.shape=(1269184, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    181312\n",
      "7    181312\n",
      "1    181312\n",
      "5    181312\n",
      "3    181312\n",
      "4    181312\n",
      "6    181312\n",
      "Name: count, dtype: int64\n",
      "Fold 1 -- Accuracy: 0.9236, F1 (macro): 0.8705, F1 (weighted): 0.9240\n",
      "Fold 1 -- Accuracy: 0.9236, F1 (macro): 0.8705, F1 (weighted): 0.9240\n",
      "Fold 2: after SMOTE X_res.shape=(1269184, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    181312\n",
      "7    181312\n",
      "1    181312\n",
      "5    181312\n",
      "3    181312\n",
      "4    181312\n",
      "6    181312\n",
      "Name: count, dtype: int64\n",
      "Fold 2: after SMOTE X_res.shape=(1269184, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    181312\n",
      "7    181312\n",
      "1    181312\n",
      "5    181312\n",
      "3    181312\n",
      "4    181312\n",
      "6    181312\n",
      "Name: count, dtype: int64\n",
      "Fold 2 -- Accuracy: 0.9241, F1 (macro): 0.8711, F1 (weighted): 0.9244\n",
      "Fold 2 -- Accuracy: 0.9241, F1 (macro): 0.8711, F1 (weighted): 0.9244\n",
      "Fold 3: after SMOTE X_res.shape=(1269184, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    181312\n",
      "1    181312\n",
      "5    181312\n",
      "7    181312\n",
      "3    181312\n",
      "4    181312\n",
      "6    181312\n",
      "Name: count, dtype: int64\n",
      "Fold 3: after SMOTE X_res.shape=(1269184, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    181312\n",
      "1    181312\n",
      "5    181312\n",
      "7    181312\n",
      "3    181312\n",
      "4    181312\n",
      "6    181312\n",
      "Name: count, dtype: int64\n",
      "Fold 3 -- Accuracy: 0.9261, F1 (macro): 0.8752, F1 (weighted): 0.9264\n",
      "Fold 3 -- Accuracy: 0.9261, F1 (macro): 0.8752, F1 (weighted): 0.9264\n",
      "Fold 4: after SMOTE X_res.shape=(1269184, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    181312\n",
      "7    181312\n",
      "1    181312\n",
      "5    181312\n",
      "3    181312\n",
      "4    181312\n",
      "6    181312\n",
      "Name: count, dtype: int64\n",
      "Fold 4: after SMOTE X_res.shape=(1269184, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    181312\n",
      "7    181312\n",
      "1    181312\n",
      "5    181312\n",
      "3    181312\n",
      "4    181312\n",
      "6    181312\n",
      "Name: count, dtype: int64\n",
      "Fold 4 -- Accuracy: 0.9261, F1 (macro): 0.8770, F1 (weighted): 0.9264\n",
      "Fold 4 -- Accuracy: 0.9261, F1 (macro): 0.8770, F1 (weighted): 0.9264\n",
      "Fold 5: after SMOTE X_res.shape=(1269184, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    181312\n",
      "7    181312\n",
      "1    181312\n",
      "3    181312\n",
      "4    181312\n",
      "5    181312\n",
      "6    181312\n",
      "Name: count, dtype: int64\n",
      "Fold 5: after SMOTE X_res.shape=(1269184, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    181312\n",
      "7    181312\n",
      "1    181312\n",
      "3    181312\n",
      "4    181312\n",
      "5    181312\n",
      "6    181312\n",
      "Name: count, dtype: int64\n",
      "Fold 5 -- Accuracy: 0.9244, F1 (macro): 0.8709, F1 (weighted): 0.9247\n",
      "Fold 5 -- Accuracy: 0.9244, F1 (macro): 0.8709, F1 (weighted): 0.9247\n",
      "5-fold CV with Gini criterion complete. Saved fold results and metrics to tests/.\n",
      "Cross-validation metrics:\n",
      "      fold  accuracy  f1_macro  f1_weighted\n",
      "0      1.0  0.923560  0.870522     0.923963\n",
      "1      2.0  0.924087  0.871060     0.924418\n",
      "2      3.0  0.926142  0.875151     0.926417\n",
      "3      4.0  0.926088  0.877007     0.926382\n",
      "4      5.0  0.924366  0.870854     0.924691\n",
      "mean   3.0  0.924849  0.872919     0.925174\n",
      "5-fold CV with Gini criterion complete. Saved fold results and metrics to tests/.\n",
      "Cross-validation metrics:\n",
      "      fold  accuracy  f1_macro  f1_weighted\n",
      "0      1.0  0.923560  0.870522     0.923963\n",
      "1      2.0  0.924087  0.871060     0.924418\n",
      "2      3.0  0.926142  0.875151     0.926417\n",
      "3      4.0  0.926088  0.877007     0.926382\n",
      "4      5.0  0.924366  0.870854     0.924691\n",
      "mean   3.0  0.924849  0.872919     0.925174\n"
     ]
    }
   ],
   "source": [
    "# 5-fold cross-validated Decision Tree with SMOTE using Gini criterion (for comparison with entropy)\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "os.makedirs('tests/CT', exist_ok=True)\n",
    "\n",
    "X_train = X_train_full_CT\n",
    "y_train = y_train_full_CT\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "fold_metrics = []\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), start=1):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    smt = SMOTE(random_state=42)\n",
    "    X_res, y_res = smt.fit_resample(X_fold_train, y_fold_train)\n",
    "    print(f\"Fold {fold}: after SMOTE X_res.shape={X_res.shape}, y_res distribution:\\n{pd.Series(y_res).value_counts()}\")\n",
    "\n",
    "    clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "    clf.fit(X_res, y_res)\n",
    "\n",
    "    y_pred = clf.predict(X_fold_val)\n",
    "    acc = accuracy_score(y_fold_val, y_pred)\n",
    "    f1_macro = f1_score(y_fold_val, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_fold_val, y_pred, average='weighted')\n",
    "    creport = classification_report(y_fold_val, y_pred)\n",
    "    cm = confusion_matrix(y_fold_val, y_pred)\n",
    "\n",
    "    print(f\"Fold {fold} -- Accuracy: {acc:.4f}, F1 (macro): {f1_macro:.4f}, F1 (weighted): {f1_weighted:.4f}\")\n",
    "    fold_metrics.append({'fold': fold, 'accuracy': acc, 'f1_macro': f1_macro, 'f1_weighted': f1_weighted})\n",
    "\n",
    "    df_fold = X_fold_val.copy()\n",
    "    df_fold['y_true'] = y_fold_val\n",
    "    df_fold['y_pred'] = y_pred\n",
    "    df_fold.to_csv(f'tests/CT/decision_tree_gini_smote_results_fold_{fold}.csv', index=True)\n",
    "\n",
    "    with open(f'tests/CT/decision_tree_gini_smote_classification_report_fold_{fold}.txt', 'w') as f:\n",
    "        f.write(creport)\n",
    "    pd.DataFrame(cm).to_csv(f'tests/CT/decision_tree_gini_smote_confusion_matrix_fold_{fold}.csv', index=False)\n",
    "\n",
    "    fold_results.append(df_fold)\n",
    "\n",
    "all_results = pd.concat(fold_results)\n",
    "all_results.to_csv('tests/CT/decision_tree_gini_smote_results_all_folds.csv', index=True)\n",
    "\n",
    "metrics_df = pd.DataFrame(fold_metrics)\n",
    "metrics_df.loc['mean'] = metrics_df.mean(numeric_only=True)\n",
    "metrics_df.to_csv('tests/CT/decision_tree_gini_smote_cv_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9024db60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: after Tomek X_res.shape=(341628, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    169296\n",
      "1    125307\n",
      "3     19841\n",
      "7     11888\n",
      "6      8795\n",
      "5      4742\n",
      "4      1759\n",
      "Name: count, dtype: int64\n",
      "Fold 1 -- Accuracy: 0.9238, F1 (macro): 0.8711, F1 (weighted): 0.9237\n",
      "Fold 1 -- Accuracy: 0.9238, F1 (macro): 0.8711, F1 (weighted): 0.9237\n",
      "Fold 2: after Tomek X_res.shape=(341629, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    169303\n",
      "1    125396\n",
      "3     19770\n",
      "7     11944\n",
      "6      8726\n",
      "5      4731\n",
      "4      1759\n",
      "Name: count, dtype: int64\n",
      "Fold 2: after Tomek X_res.shape=(341629, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    169303\n",
      "1    125396\n",
      "3     19770\n",
      "7     11944\n",
      "6      8726\n",
      "5      4731\n",
      "4      1759\n",
      "Name: count, dtype: int64\n",
      "Fold 2 -- Accuracy: 0.9219, F1 (macro): 0.8782, F1 (weighted): 0.9218\n",
      "Fold 2 -- Accuracy: 0.9219, F1 (macro): 0.8782, F1 (weighted): 0.9218\n",
      "Fold 3: after Tomek X_res.shape=(341366, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    169221\n",
      "1    125321\n",
      "3     19747\n",
      "7     11884\n",
      "6      8704\n",
      "5      4731\n",
      "4      1758\n",
      "Name: count, dtype: int64\n",
      "Fold 3: after Tomek X_res.shape=(341366, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    169221\n",
      "1    125321\n",
      "3     19747\n",
      "7     11884\n",
      "6      8704\n",
      "5      4731\n",
      "4      1758\n",
      "Name: count, dtype: int64\n",
      "Fold 3 -- Accuracy: 0.9238, F1 (macro): 0.8767, F1 (weighted): 0.9236\n",
      "Fold 3 -- Accuracy: 0.9238, F1 (macro): 0.8767, F1 (weighted): 0.9236\n",
      "Fold 4: after Tomek X_res.shape=(341609, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    169341\n",
      "1    125342\n",
      "3     19804\n",
      "7     11841\n",
      "6      8772\n",
      "5      4751\n",
      "4      1758\n",
      "Name: count, dtype: int64\n",
      "Fold 4: after Tomek X_res.shape=(341609, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    169341\n",
      "1    125342\n",
      "3     19804\n",
      "7     11841\n",
      "6      8772\n",
      "5      4751\n",
      "4      1758\n",
      "Name: count, dtype: int64\n",
      "Fold 4 -- Accuracy: 0.9236, F1 (macro): 0.8772, F1 (weighted): 0.9235\n",
      "Fold 4 -- Accuracy: 0.9236, F1 (macro): 0.8772, F1 (weighted): 0.9235\n",
      "Fold 5: after Tomek X_res.shape=(341632, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    169313\n",
      "1    125296\n",
      "3     19886\n",
      "7     11845\n",
      "6      8794\n",
      "5      4740\n",
      "4      1758\n",
      "Name: count, dtype: int64\n",
      "Fold 5: after Tomek X_res.shape=(341632, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    169313\n",
      "1    125296\n",
      "3     19886\n",
      "7     11845\n",
      "6      8794\n",
      "5      4740\n",
      "4      1758\n",
      "Name: count, dtype: int64\n",
      "Fold 5 -- Accuracy: 0.9220, F1 (macro): 0.8747, F1 (weighted): 0.9219\n",
      "Fold 5 -- Accuracy: 0.9220, F1 (macro): 0.8747, F1 (weighted): 0.9219\n",
      "5-fold CV with Gini criterion complete. Saved fold results and metrics to tests/.\n",
      "Cross-validation metrics:\n",
      "      fold  accuracy  f1_macro  f1_weighted\n",
      "0      1.0  0.923775  0.871104     0.923705\n",
      "1      2.0  0.921914  0.878185     0.921842\n",
      "2      3.0  0.923754  0.876670     0.923606\n",
      "3      4.0  0.923592  0.877203     0.923504\n",
      "4      5.0  0.921978  0.874650     0.921913\n",
      "mean   3.0  0.923003  0.875562     0.922914\n",
      "5-fold CV with Gini criterion complete. Saved fold results and metrics to tests/.\n",
      "Cross-validation metrics:\n",
      "      fold  accuracy  f1_macro  f1_weighted\n",
      "0      1.0  0.923775  0.871104     0.923705\n",
      "1      2.0  0.921914  0.878185     0.921842\n",
      "2      3.0  0.923754  0.876670     0.923606\n",
      "3      4.0  0.923592  0.877203     0.923504\n",
      "4      5.0  0.921978  0.874650     0.921913\n",
      "mean   3.0  0.923003  0.875562     0.922914\n"
     ]
    }
   ],
   "source": [
    "# 5-fold cross-validated Decision Tree with Tomek using Gini criterion (for comparison with entropy)\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.under_sampling import TomekLinks as Tomek\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "os.makedirs('tests/CT', exist_ok=True)\n",
    "\n",
    "X_train = X_train_full_CT\n",
    "y_train = y_train_full_CT\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "fold_metrics = []\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), start=1):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    smt = Tomek()\n",
    "    X_res, y_res = smt.fit_resample(X_fold_train, y_fold_train)\n",
    "    print(f\"Fold {fold}: after Tomek X_res.shape={X_res.shape}, y_res distribution:\\n{pd.Series(y_res).value_counts()}\")\n",
    "\n",
    "    clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "    clf.fit(X_res, y_res)\n",
    "\n",
    "    y_pred = clf.predict(X_fold_val)\n",
    "    acc = accuracy_score(y_fold_val, y_pred)\n",
    "    f1_macro = f1_score(y_fold_val, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_fold_val, y_pred, average='weighted')\n",
    "    creport = classification_report(y_fold_val, y_pred)\n",
    "    cm = confusion_matrix(y_fold_val, y_pred)\n",
    "\n",
    "    print(f\"Fold {fold} -- Accuracy: {acc:.4f}, F1 (macro): {f1_macro:.4f}, F1 (weighted): {f1_weighted:.4f}\")\n",
    "    fold_metrics.append({'fold': fold, 'accuracy': acc, 'f1_macro': f1_macro, 'f1_weighted': f1_weighted})\n",
    "\n",
    "    df_fold = X_fold_val.copy()\n",
    "    df_fold['y_true'] = y_fold_val\n",
    "    df_fold['y_pred'] = y_pred\n",
    "    df_fold.to_csv(f'tests/CT/decision_tree_gini_tomek_results_fold_{fold}.csv', index=True)\n",
    "\n",
    "    with open(f'tests/CT/decision_tree_gini_tomek_classification_report_fold_{fold}.txt', 'w') as f:\n",
    "        f.write(creport)\n",
    "    pd.DataFrame(cm).to_csv(f'tests/CT/decision_tree_gini_tomek_confusion_matrix_fold_{fold}.csv', index=False)\n",
    "\n",
    "    fold_results.append(df_fold)\n",
    "\n",
    "all_results = pd.concat(fold_results)\n",
    "all_results.to_csv('tests/CT/decision_tree_gini_tomek_results_all_folds.csv', index=True)\n",
    "\n",
    "metrics_df = pd.DataFrame(fold_metrics)\n",
    "metrics_df.loc['mean'] = metrics_df.mean(numeric_only=True)\n",
    "metrics_df.to_csv('tests/CT/decision_tree_gini_tomek_cv_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2821e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 10-fold CV for Decision Tree (Tomek + entropy)...\n",
      "============================================================\n",
      "Fold 1: After Tomek: X_res.shape = (335656, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    166413\n",
      "1    123131\n",
      "3     19451\n",
      "7     11687\n",
      "6      8570\n",
      "5      4673\n",
      "4      1731\n",
      "Name: count, dtype: int64\n",
      "Fold  1 | Acc: 0.9255 | F1-macro: 0.8844 | Precision: 0.8928 | Recall: 0.8764 | Specificity: 0.9827 | Kappa: 0.8802\n",
      "Fold 2: After Tomek: X_res.shape = (335722, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    166434\n",
      "1    123174\n",
      "3     19442\n",
      "7     11713\n",
      "6      8572\n",
      "5      4656\n",
      "4      1731\n",
      "Name: count, dtype: int64\n",
      "Fold  2 | Acc: 0.9260 | F1-macro: 0.8777 | Precision: 0.8797 | Recall: 0.8760 | Specificity: 0.9829 | Kappa: 0.8811\n",
      "Fold 3: After Tomek: X_res.shape = (335603, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    166404\n",
      "1    123136\n",
      "3     19423\n",
      "7     11703\n",
      "6      8543\n",
      "5      4663\n",
      "4      1731\n",
      "Name: count, dtype: int64\n",
      "Fold  3 | Acc: 0.9248 | F1-macro: 0.8798 | Precision: 0.8834 | Recall: 0.8768 | Specificity: 0.9826 | Kappa: 0.8791\n",
      "Fold 4: After Tomek: X_res.shape = (335541, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    166397\n",
      "1    123114\n",
      "3     19405\n",
      "7     11695\n",
      "6      8538\n",
      "5      4661\n",
      "4      1731\n",
      "Name: count, dtype: int64\n",
      "Fold  4 | Acc: 0.9285 | F1-macro: 0.8896 | Precision: 0.8934 | Recall: 0.8866 | Specificity: 0.9833 | Kappa: 0.8850\n",
      "Fold 5: After Tomek: X_res.shape = (335846, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    166543\n",
      "1    123288\n",
      "3     19414\n",
      "7     11706\n",
      "6      8514\n",
      "5      4650\n",
      "4      1731\n",
      "Name: count, dtype: int64\n",
      "Fold  5 | Acc: 0.9275 | F1-macro: 0.8856 | Precision: 0.8872 | Recall: 0.8840 | Specificity: 0.9832 | Kappa: 0.8836\n",
      "Fold 6: After Tomek: X_res.shape = (335623, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    166402\n",
      "1    123093\n",
      "3     19462\n",
      "7     11703\n",
      "6      8577\n",
      "5      4655\n",
      "4      1731\n",
      "Name: count, dtype: int64\n",
      "Fold  6 | Acc: 0.9251 | F1-macro: 0.8819 | Precision: 0.8869 | Recall: 0.8772 | Specificity: 0.9826 | Kappa: 0.8795\n",
      "Fold 7: After Tomek: X_res.shape = (335645, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    166366\n",
      "1    123182\n",
      "3     19444\n",
      "7     11715\n",
      "6      8564\n",
      "5      4644\n",
      "4      1730\n",
      "Name: count, dtype: int64\n",
      "Fold  7 | Acc: 0.9225 | F1-macro: 0.8810 | Precision: 0.8888 | Recall: 0.8742 | Specificity: 0.9819 | Kappa: 0.8754\n",
      "Fold 8: After Tomek: X_res.shape = (335755, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    166456\n",
      "1    123164\n",
      "3     19443\n",
      "7     11696\n",
      "6      8600\n",
      "5      4666\n",
      "4      1730\n",
      "Name: count, dtype: int64\n",
      "Fold  8 | Acc: 0.9275 | F1-macro: 0.8860 | Precision: 0.8926 | Recall: 0.8799 | Specificity: 0.9831 | Kappa: 0.8835\n",
      "Fold 9: After Tomek: X_res.shape = (336044, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    166569\n",
      "1    123246\n",
      "3     19494\n",
      "7     11718\n",
      "6      8609\n",
      "5      4678\n",
      "4      1730\n",
      "Name: count, dtype: int64\n",
      "Fold  9 | Acc: 0.9258 | F1-macro: 0.8787 | Precision: 0.8826 | Recall: 0.8755 | Specificity: 0.9827 | Kappa: 0.8807\n",
      "Fold 10: After Tomek: X_res.shape = (335557, 55), y_res distribution:\n",
      "Cover_Type\n",
      "2    166398\n",
      "1    123085\n",
      "3     19442\n",
      "7     11665\n",
      "6      8552\n",
      "5      4684\n",
      "4      1731\n",
      "Name: count, dtype: int64\n",
      "Fold 10 | Acc: 0.9259 | F1-macro: 0.8836 | Precision: 0.8898 | Recall: 0.8783 | Specificity: 0.9828 | Kappa: 0.8810\n",
      "============================================================\n",
      "10-Fold CV completed for Decision Tree (Tomek + entropy)\n",
      "Results saved to 'tests/CT/Final/'\n",
      "\n",
      "Final Cross-validation Summary:\n",
      "--------------------------------------------------\n",
      "Accuracy:     0.9259 ± 0.0017\n",
      "F1-macro:     0.8828 ± 0.0037\n",
      "F1-weighted:  0.9258 ± 0.0017\n",
      "Precision:    0.8877 ± 0.0047\n",
      "Recall:       0.8785 ± 0.0039\n",
      "Specificity:  0.9828 ± 0.0004\n",
      "Cohen's Kappa: 0.8809 ± 0.0027\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 10-fold cross-validated Decision Tree with Tomek Links (similar to the KNN example)\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.under_sampling import TomekLinks as Tomek\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, \n",
    "                             classification_report, confusion_matrix, cohen_kappa_score)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.makedirs('tests/CT/Final', exist_ok=True)\n",
    "\n",
    "X_train = X_train_full_CT\n",
    "y_train = y_train_full_CT\n",
    "\n",
    "n_splits = 10\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "results_list = []\n",
    "metrics_list = []\n",
    "\n",
    "def calculate_specificity_multiclass(y_true, y_pred, classes):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    specificities = []\n",
    "    for i, cls in enumerate(classes):\n",
    "        tn = np.sum(cm) - (np.sum(cm[i, :]) + np.sum(cm[:, i]) - cm[i, i])\n",
    "        fp = np.sum(cm[:, i]) - cm[i, i]\n",
    "        specificities.append(tn / (tn + fp) if (tn + fp) > 0 else 0.0)\n",
    "    return np.mean(specificities)\n",
    "\n",
    "ccp_alpha_val = globals().get('best_alpha', 1.0961774439107219e-05)\n",
    "\n",
    "print(\"Starting 10-fold CV for Decision Tree (Tomek + entropy)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train), start=1):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    tomek = Tomek()\n",
    "    X_res, y_res = tomek.fit_resample(X_fold_train, y_fold_train)\n",
    "    print(f\"Fold {fold}: After Tomek: X_res.shape = {X_res.shape}, y_res distribution:\\n{pd.Series(y_res).value_counts()}\")\n",
    "\n",
    "    clf = DecisionTreeClassifier(criterion='entropy', ccp_alpha=ccp_alpha_val, random_state=42)\n",
    "    clf.fit(X_res, y_res)\n",
    "    y_pred = clf.predict(X_fold_val)\n",
    "\n",
    "    acc = accuracy_score(y_fold_val, y_pred)\n",
    "    f1_macro = f1_score(y_fold_val, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_fold_val, y_pred, average='weighted')\n",
    "    precision_macro = precision_score(y_fold_val, y_pred, average='macro', zero_division=0)\n",
    "    recall_macro = recall_score(y_fold_val, y_pred, average='macro', zero_division=0)\n",
    "    classes = np.unique(np.concatenate([y_fold_val, y_pred]))\n",
    "    specificity_macro = calculate_specificity_multiclass(y_fold_val, y_pred, classes)\n",
    "    kappa = cohen_kappa_score(y_fold_val, y_pred)\n",
    "\n",
    "    metrics_list.append({\n",
    "        'fold': fold,\n",
    "        'accuracy': acc,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'specificity_macro': specificity_macro,\n",
    "        'cohen_kappa': kappa\n",
    "    })\n",
    "\n",
    "    df_fold = X_fold_val.copy()\n",
    "    df_fold['y_true'] = y_fold_val\n",
    "    df_fold['y_pred'] = y_pred\n",
    "\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        probs = clf.predict_proba(X_fold_val)\n",
    "        for i, cls in enumerate(clf.classes_):\n",
    "            df_fold[f'prob_{cls}'] = probs[:, i]\n",
    "\n",
    "    fold_results_path = f'tests/CT/Final/decision_tree_tomek_entropy_10fold_results_fold_{fold}.csv'\n",
    "    df_fold.to_csv(fold_results_path, index=True)\n",
    "\n",
    "    creport = classification_report(y_fold_val, y_pred)\n",
    "    with open(f'tests/CT/Final/decision_tree_tomek_entropy_10fold_classification_report_fold_{fold}.txt', 'w') as f:\n",
    "        f.write(creport)\n",
    "    cm = confusion_matrix(y_fold_val, y_pred)\n",
    "    pd.DataFrame(cm).to_csv(f'tests/CT/Final/decision_tree_tomek_entropy_10fold_confusion_matrix_fold_{fold}.csv', index=False)\n",
    "\n",
    "    results_list.append(df_fold)\n",
    "\n",
    "    print(f\"Fold {fold:2d} | Acc: {acc:.4f} | F1-macro: {f1_macro:.4f} | Precision: {precision_macro:.4f} | Recall: {recall_macro:.4f} | Specificity: {specificity_macro:.4f} | Kappa: {kappa:.4f}\")\n",
    "\n",
    "all_results = pd.concat(results_list)\n",
    "all_results.to_csv('tests/CT/Final/decision_tree_tomek_entropy_10fold_results_all_folds.csv', index=True)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "mean_metrics = metrics_df.select_dtypes(include=[np.number]).mean()\n",
    "std_metrics = metrics_df.select_dtypes(include=[np.number]).std()\n",
    "\n",
    "summary_row = pd.DataFrame([mean_metrics, std_metrics], index=['mean', 'std'])\n",
    "metrics_summary = pd.concat([metrics_df, summary_row])\n",
    "metrics_summary.to_csv('tests/CT/Final/decision_tree_tomek_entropy_10fold_cv_metrics.csv', index=False)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"10-Fold CV completed for Decision Tree (Tomek + entropy)\")\n",
    "print(\"Results saved to 'tests/CT/Final/'\")\n",
    "print(\"\\nFinal Cross-validation Summary:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Accuracy:     {mean_metrics['accuracy']:.4f} ± {std_metrics['accuracy']:.4f}\")\n",
    "print(f\"F1-macro:     {mean_metrics['f1_macro']:.4f} ± {std_metrics['f1_macro']:.4f}\")\n",
    "print(f\"F1-weighted:  {mean_metrics['f1_weighted']:.4f} ± {std_metrics['f1_weighted']:.4f}\")\n",
    "print(f\"Precision:    {mean_metrics['precision_macro']:.4f} ± {std_metrics['precision_macro']:.4f}\")\n",
    "print(f\"Recall:       {mean_metrics['recall_macro']:.4f} ± {std_metrics['recall_macro']:.4f}\")\n",
    "print(f\"Specificity:  {mean_metrics['specificity_macro']:.4f} ± {std_metrics['specificity_macro']:.4f}\")\n",
    "print(f\"Cohen's Kappa: {mean_metrics['cohen_kappa']:.4f} ± {std_metrics['cohen_kappa']:.4f}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262069bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: accuracy\n",
      "  KNN mean = 0.884541, DT mean = 0.925903\n",
      "  Mean diff (KNN - DT) = -0.041361\n",
      "  Paired t-stat = -82.6368, p-value = 2.8182e-14\n",
      "  Cohen's d (paired) = -26.1320\n",
      "------------------------------------------------------------\n",
      "Metric: f1_macro\n",
      "  KNN mean = 0.815891, DT mean = 0.882839\n",
      "  Mean diff (KNN - DT) = -0.066948\n",
      "  Paired t-stat = -29.7741, p-value = 2.6569e-10\n",
      "  Cohen's d (paired) = -9.4154\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "knn_path = 'tests/Final/knn_k3_tomek_manhattan_10fold_cv_metrics.csv'\n",
    "dt_path  = 'tests/CT/Final/decision_tree_tomek_entropy_10fold_cv_metrics.csv'\n",
    "\n",
    "def load_cv_metrics(path, n_splits=10):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    fold_col = 'fold' if 'fold' in df.columns else df.columns[0]\n",
    "    df['fold_num'] = pd.to_numeric(df[fold_col], errors='coerce')\n",
    "\n",
    "    df = df[df['fold_num'].apply(lambda x: float(x).is_integer())]\n",
    "    df = df[df['fold_num'].between(1, n_splits, inclusive=\"both\")]\n",
    "\n",
    "    df = df.sort_values('fold_num').reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_knn = load_cv_metrics(knn_path)\n",
    "df_dt  = load_cv_metrics(dt_path)\n",
    "\n",
    "metrics = ['accuracy', 'f1_macro']  \n",
    "\n",
    "results = {}\n",
    "for m in metrics:\n",
    "    a = df_knn[m].to_numpy(dtype=float)\n",
    "    b = df_dt[m].to_numpy(dtype=float)\n",
    "    if a.shape != b.shape:\n",
    "        raise ValueError(f\"Metric arrays for '{m}' have different shapes: {a.shape} vs {b.shape}\")\n",
    "    tstat, pval = ttest_rel(a, b)\n",
    "    mean_a, mean_b = a.mean(), b.mean()\n",
    "    diff = a - b\n",
    "    md = diff.mean()\n",
    "    sd_diff = diff.std(ddof=1)\n",
    "    cohen_d = md / sd_diff if sd_diff > 0 else np.nan\n",
    "    results[m] = {\n",
    "        'knn_mean': mean_a,\n",
    "        'dt_mean': mean_b,\n",
    "        'mean_diff (knn - dt)': md,\n",
    "        't_stat': tstat,\n",
    "        'p_value': pval,\n",
    "        \"cohen_d\": cohen_d\n",
    "    }\n",
    "\n",
    "for metric, r in results.items():\n",
    "    print(f\"Metric: {metric}\")\n",
    "    print(f\"  KNN mean = {r['knn_mean']:.6f}, DT mean = {r['dt_mean']:.6f}\")\n",
    "    print(f\"  Mean diff (KNN - DT) = {r['mean_diff (knn - dt)']:.6f}\")\n",
    "    print(f\"  Paired t-stat = {r['t_stat']:.4f}, p-value = {r['p_value']:.4e}\")\n",
    "    print(f\"  Cohen's d (paired) = {r['cohen_d']:.4f}\")\n",
    "    print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
